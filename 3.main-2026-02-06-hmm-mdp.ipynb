{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1928c93",
   "metadata": {},
   "source": [
    "### Section 1 — Chargement & normalisation (L1 + L3) + contrôles de cohérence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e890c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Data-Mahefa\\thesis\\PhD\\Articles-Mahefa\\articles-elysa\n",
      "L1 — Onglets détectés: ['Résultatsélèves', 'Activités']\n",
      "\n",
      "✅ L1 (résultats) choisi: Résultatsélèves | shape: (813, 8)\n",
      "✅ L1 (activités) choisi: Activités | shape: (10, 54)\n",
      "✅ L3 chargé: | shape: (813, 265)\n",
      "\n",
      "Blocs détectés (préfixes) dans L1/Activités:\n",
      " - Deuxchiffres: 13 colonnes\n",
      " - TroisA: 11 colonnes\n",
      " - Deb: 10 colonnes\n",
      " - Unchiffe: 10 colonnes\n",
      " - TroisB: 8 colonnes\n",
      "\n",
      "Contrôle: nb d'activités exécutées par jour (attendu = 4):\n",
      "count    10.000000\n",
      "mean     20.200000\n",
      "std       0.632456\n",
      "min      20.000000\n",
      "25%      20.000000\n",
      "50%      20.000000\n",
      "75%      20.000000\n",
      "max      22.000000\n",
      "dtype: float64\n",
      "Jours anormaux (ExecutedCount != 4): 10\n",
      " Week  Day  ExecutedCount\n",
      "    1    1             20\n",
      "    1    2             20\n",
      "    1    3             20\n",
      "    1    4             20\n",
      "    1    5             20\n",
      "    2    6             20\n",
      "    2    7             20\n",
      "    2    8             22\n",
      "    2    9             20\n",
      "    2   10             20\n",
      "\n",
      "Contrôle: positions manquantes dans la journée (attendu = 0):\n",
      "0    10\n",
      "\n",
      "L1 Résultats — aperçu colonnes: ['StudentID', 'Age', 'Classe', 'HoursTotal_L1', 'Genre', 'Zone', 'Pretest', 'Final', 'Pretest_i', 'Final_i']\n",
      "\n",
      "Distribution Pretest_i (L1):\n",
      "Pretest_i\n",
      "1      4\n",
      "2     66\n",
      "3    166\n",
      "4    230\n",
      "5    347\n",
      "\n",
      "Distribution Final_i (L1):\n",
      "Final_i\n",
      "1      8\n",
      "2     43\n",
      "3    171\n",
      "4    118\n",
      "5    473\n",
      "\n",
      "HoursTotal_L1 — stats:\n",
      "count    813.000000\n",
      "mean      45.190652\n",
      "std       16.935674\n",
      "min       20.000000\n",
      "25%       20.000000\n",
      "50%       60.000000\n",
      "75%       60.000000\n",
      "max       60.000000\n",
      "\n",
      "L3 — colonnes importantes présentes ?\n",
      " - HoursTotal: OK\n",
      " - Pretest: OK\n",
      " - Final: OK\n",
      " - Delta: OK\n",
      " - Mastery_ge4: OK\n",
      " - Age: OK\n",
      " - Genre: OK\n",
      " - Zone: OK\n",
      " - LevelTag: OK\n",
      "\n",
      "HoursTotal (L3) — stats:\n",
      "count    813.000000\n",
      "mean      45.190652\n",
      "std       16.935674\n",
      "min       20.000000\n",
      "25%       20.000000\n",
      "50%       60.000000\n",
      "75%       60.000000\n",
      "max       60.000000\n",
      "\n",
      "Merge L1-L3 via StudentID: (813, 276)\n",
      "\n",
      "Cohérence HoursTotal: (L3 - L1) stats\n",
      "count    813.0\n",
      "mean       0.0\n",
      "std        0.0\n",
      "min        0.0\n",
      "25%        0.0\n",
      "50%        0.0\n",
      "75%        0.0\n",
      "max        0.0\n",
      "\n",
      "✅ PAGE 1 terminée.\n",
      "Fichiers produits:\n",
      " - ./out/202602/out_pomdp\\L0-L1_results_clean.csv\n",
      " - ./out/202602/out_pomdp\\L0-L1_activities_clean.csv\n",
      " - ./out/202602/out_pomdp\\L0-L1_activity_column_blocks.csv\n",
      " - ./out/202602/out_pomdp\\L0-PAGE1_summary.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Section 1 — Chargement & normalisation (L1 + L3) + contrôles de cohérence\n",
    "\n",
    "Objectifs (Section 1)\n",
    "1) Lire L1 (2 onglets) + L3\n",
    "2) Normaliser les champs (IDs, niveaux 1..5, heures totales)\n",
    "3) Contrôles qualité: valeurs manquantes, distributions, cohérences simples\n",
    "4) Produire des artefacts \"clean\" (CSV) pour les pages suivantes\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# 0) CONFIG — adapte si besoin\n",
    "# ============================================================\n",
    "L1_PATH = \"./data/2025-08/L1.20250818-DataMathsElysa.xlsx\"\n",
    "print(os.getcwd())\n",
    "# L1_PATH = \"../../\" + L1_PATH\n",
    "L3_PATH = \"./data/2025-08/L3.features_by_student.byPretest.csv\"\n",
    "\n",
    "OUT_DIR = \"./out/202602/out_pomdp\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def out_path(fname: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"L0-{fname}\")\n",
    "\n",
    "# Hypothèses  (données fournies par toi)\n",
    "HOURS_PER_ACTIVITY = 2.0   # 1 activité ≈ 2h (vérité absolue dans l'analyse)\n",
    "ACTIVITIES_PER_DAY = 4     # 4 activités / jour (positions 1..4)\n",
    "HOURS_PER_DAY = HOURS_PER_ACTIVITY * ACTIVITIES_PER_DAY  # 8h / jour\n",
    "\n",
    "assert os.path.exists(L1_PATH), f\"Introuvable: {L1_PATH}\"\n",
    "assert os.path.exists(L3_PATH), f\"Introuvable: {L3_PATH}\"\n",
    "\n",
    "xls = pd.ExcelFile(L1_PATH)\n",
    "print(\"L1 — Onglets détectés:\", xls.sheet_names)\n",
    "\n",
    "# Lire les deux onglets (on n'impose pas le nom exact, on mappe par heuristique)\n",
    "df_sheets = {name: pd.read_excel(L1_PATH, sheet_name=name) for name in xls.sheet_names}\n",
    "\n",
    "# Heuristique: onglet \"Résultatsélèves\" = contient Pretest/Final + IDélèves\n",
    "# onglet \"Activités\" = contient Semaine/Jour + colonnes activités 0..4\n",
    "def _score_results_sheet(d: pd.DataFrame) -> int:\n",
    "    cols = {c.strip(): c for c in d.columns}\n",
    "    score = 0\n",
    "    for k in [\"IDélèves\",\"Pretest\",\"Final\",\"Age\",\"Genre\",\"Zone\"]:\n",
    "        if k in cols: score += 2\n",
    "    if \"Nombre d'heures de remédiation fait au total\" in cols: score += 2\n",
    "    return score\n",
    "\n",
    "def _score_activities_sheet(d: pd.DataFrame) -> int:\n",
    "    cols = [str(c) for c in d.columns]\n",
    "    score = 0\n",
    "    if any(c.lower().strip() == \"semaine\" for c in cols): score += 2\n",
    "    if any(c.lower().strip() == \"jour\" for c in cols): score += 2\n",
    "    # beaucoup de colonnes après les 2 premières\n",
    "    if len(cols) > 20: score += 2\n",
    "    return score\n",
    "\n",
    "sheet_scores = []\n",
    "for name, d in df_sheets.items():\n",
    "    sheet_scores.append((name, _score_results_sheet(d), _score_activities_sheet(d)))\n",
    "\n",
    "# Choix\n",
    "results_sheet = sorted(sheet_scores, key=lambda t: t[1], reverse=True)[0][0]\n",
    "activities_sheet = sorted(sheet_scores, key=lambda t: t[2], reverse=True)[0][0]\n",
    "\n",
    "df_results = df_sheets[results_sheet].copy()\n",
    "df_acts = df_sheets[activities_sheet].copy()\n",
    "df_l3 = pd.read_csv(L3_PATH)\n",
    "\n",
    "print(\"\\n L1 (résultats) choisi:\", results_sheet, \"| shape:\", df_results.shape)\n",
    "print(\" L1 (activités) choisi:\", activities_sheet, \"| shape:\", df_acts.shape)\n",
    "print(\" L3 chargé:\", \"| shape:\", df_l3.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 2) NORMALISATION DES COLONNES (L1 Résultats)\n",
    "# ============================================================\n",
    "def _norm_colname(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "df_results.columns = [_norm_colname(c) for c in df_results.columns]\n",
    "df_acts.columns = [_norm_colname(c) for c in df_acts.columns]\n",
    "df_l3.columns = [_norm_colname(c) for c in df_l3.columns]\n",
    "\n",
    "# Renommage standard\n",
    "rename_results = {\n",
    "    \"IDélèves\": \"StudentID\",\n",
    "    \"Nombre d'heures de remédiation fait au total\": \"HoursTotal_L1\",\n",
    "    \"Pretest\": \"Pretest\",\n",
    "    \"Final\": \"Final\",\n",
    "    \"Age\": \"Age\",\n",
    "    \"Classe\": \"Classe\",\n",
    "    \"Genre\": \"Genre\",\n",
    "    \"Zone\": \"Zone\",\n",
    "}\n",
    "for k, v in rename_results.items():\n",
    "    if k in df_results.columns:\n",
    "        df_results.rename(columns={k: v}, inplace=True)\n",
    "\n",
    "# Coercitions types\n",
    "if \"StudentID\" in df_results.columns:\n",
    "    df_results[\"StudentID\"] = df_results[\"StudentID\"].astype(str).str.strip()\n",
    "\n",
    "for c in [\"Age\", \"Pretest\", \"Final\", \"HoursTotal_L1\"]:\n",
    "    if c in df_results.columns:\n",
    "        df_results[c] = pd.to_numeric(df_results[c], errors=\"coerce\")\n",
    "\n",
    "# bornage niveaux 1..5\n",
    "def cap_level(x):\n",
    "    try:\n",
    "        v = int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return min(5, max(1, v))\n",
    "\n",
    "if \"Pretest\" in df_results.columns:\n",
    "    df_results[\"Pretest_i\"] = df_results[\"Pretest\"].apply(cap_level)\n",
    "if \"Final\" in df_results.columns:\n",
    "    df_results[\"Final_i\"] = df_results[\"Final\"].apply(cap_level)\n",
    "\n",
    "# ============================================================\n",
    "# 3) NORMALISATION (L1 Activités)\n",
    "# ============================================================\n",
    "# Attendu: colonnes \"Semaine\", \"Jour\", puis activités groupées par préfixes (Deb, Unchiffe, Deuxchiffres, TroisA, TroisB)\n",
    "# Standardiser \"Semaine\" et \"Jour\"\n",
    "col_week = None\n",
    "col_day = None\n",
    "for c in df_acts.columns:\n",
    "    cl = c.lower().strip()\n",
    "    if cl == \"semaine\": col_week = c\n",
    "    if cl == \"jour\": col_day = c\n",
    "\n",
    "if col_week is None or col_day is None:\n",
    "    raise ValueError(\"Onglet Activités: colonnes 'Semaine' et/ou 'Jour' introuvables après normalisation.\")\n",
    "\n",
    "df_acts.rename(columns={col_week: \"Week\", col_day: \"Day\"}, inplace=True)\n",
    "df_acts[\"Week\"] = pd.to_numeric(df_acts[\"Week\"], errors=\"coerce\")\n",
    "df_acts[\"Day\"] = pd.to_numeric(df_acts[\"Day\"], errors=\"coerce\")\n",
    "\n",
    "# Colonnes d'activités = tout sauf Week/Day\n",
    "activity_cols = [c for c in df_acts.columns if c not in [\"Week\",\"Day\"]]\n",
    "\n",
    "# Forcer numériques (0..4)\n",
    "for c in activity_cols:\n",
    "    df_acts[c] = pd.to_numeric(df_acts[c], errors=\"coerce\")\n",
    "\n",
    "# Détecter les préfixes bloc (avant le premier espace)\n",
    "def block_prefix(col: str) -> str:\n",
    "    s = str(col).strip()\n",
    "    # ex: \"Deb Lecture de la Table d’addition\"\n",
    "    return s.split(\" \", 1)[0] if \" \" in s else s\n",
    "\n",
    "df_blocks = pd.Series([block_prefix(c) for c in activity_cols], name=\"Block\")\n",
    "block_counts = df_blocks.value_counts().to_dict()\n",
    "\n",
    "print(\"\\nBlocs détectés (préfixes) dans L1/Activités:\")\n",
    "for k, v in sorted(block_counts.items(), key=lambda kv: (-kv[1], kv[0])):\n",
    "    print(f\" - {k}: {v} colonnes\")\n",
    "\n",
    "# Marquer bloc pour chaque colonne activité\n",
    "col_to_block = {c: block_prefix(c) for c in activity_cols}\n",
    "\n",
    "# ============================================================\n",
    "# 4) CONTROLES METIER (L1 Activités)\n",
    "# ============================================================\n",
    "# 4.1 Vérifier que chaque jour contient exactement 4 activités exécutées (valeurs 1..4)\n",
    "def count_executed_per_day(row: pd.Series) -> int:\n",
    "    vals = row[activity_cols].values\n",
    "    return int(np.sum(np.isin(vals, [1,2,3,4])))\n",
    "\n",
    "exec_counts = df_acts.apply(count_executed_per_day, axis=1)\n",
    "print(\"\\nContrôle: nb d'activités exécutées par jour (attendu = 4):\")\n",
    "print(exec_counts.describe())\n",
    "\n",
    "bad_days = df_acts.loc[exec_counts != 4, [\"Week\",\"Day\"]].copy()\n",
    "bad_days[\"ExecutedCount\"] = exec_counts[exec_counts != 4].values\n",
    "print(f\"Jours anormaux (ExecutedCount != 4): {len(bad_days)}\")\n",
    "if len(bad_days) > 0:\n",
    "    print(bad_days.head(20).to_string(index=False))\n",
    "\n",
    "# 4.2 Vérifier présence des positions 1..4 dans chaque ligne (si 4 activités, on veut idéalement 1,2,3,4 chacune une fois)\n",
    "def missing_positions(row: pd.Series):\n",
    "    vals = row[activity_cols].values\n",
    "    pos = set([int(v) for v in vals if v in [1,2,3,4]])\n",
    "    missing = [p for p in [1,2,3,4] if p not in pos]\n",
    "    return missing\n",
    "\n",
    "miss_pos = df_acts.apply(missing_positions, axis=1)\n",
    "n_miss = miss_pos.apply(len)\n",
    "print(\"\\nContrôle: positions manquantes dans la journée (attendu = 0):\")\n",
    "print(n_miss.value_counts().sort_index().to_string())\n",
    "\n",
    "# ============================================================\n",
    "# 5) CONTROLES (L1 Résultats) + (L3)\n",
    "# ============================================================\n",
    "print(\"\\nL1 Résultats — aperçu colonnes:\", df_results.columns.tolist())\n",
    "\n",
    "# Checks simples niveaux\n",
    "if \"Pretest_i\" in df_results.columns:\n",
    "    print(\"\\nDistribution Pretest_i (L1):\")\n",
    "    print(df_results[\"Pretest_i\"].value_counts(dropna=False).sort_index().to_string())\n",
    "\n",
    "if \"Final_i\" in df_results.columns:\n",
    "    print(\"\\nDistribution Final_i (L1):\")\n",
    "    print(df_results[\"Final_i\"].value_counts(dropna=False).sort_index().to_string())\n",
    "\n",
    "if \"HoursTotal_L1\" in df_results.columns:\n",
    "    print(\"\\nHoursTotal_L1 — stats:\")\n",
    "    print(df_results[\"HoursTotal_L1\"].describe().to_string())\n",
    "\n",
    "# L3 checks\n",
    "print(\"\\nL3 — colonnes importantes présentes ?\")\n",
    "for c in [\"HoursTotal\",\"Pretest\",\"Final\",\"Delta\",\"Mastery_ge4\",\"Age\",\"Genre\",\"Zone\",\"LevelTag\"]:\n",
    "    print(f\" - {c}: {'OK' if c in df_l3.columns else 'ABSENT'}\")\n",
    "\n",
    "if \"HoursTotal\" in df_l3.columns:\n",
    "    df_l3[\"HoursTotal\"] = pd.to_numeric(df_l3[\"HoursTotal\"], errors=\"coerce\")\n",
    "    print(\"\\nHoursTotal (L3) — stats:\")\n",
    "    print(df_l3[\"HoursTotal\"].describe().to_string())\n",
    "\n",
    "# ============================================================\n",
    "# 6) COHERENCE L1 vs L3 : heures totales\n",
    "# ============================================================\n",
    "# Si L3 a StudentID => merge direct. Sinon: on garde juste des comparaisons de distributions.\n",
    "has_id_l3 = any(c.lower() in [\"studentid\",\"idélèves\",\"ideleves\",\"id_eleve\",\"id\"] for c in df_l3.columns)\n",
    "\n",
    "if has_id_l3:\n",
    "    # repérer la colonne id la plus plausible\n",
    "    cand = None\n",
    "    for c in df_l3.columns:\n",
    "        if c.lower() in [\"studentid\",\"idélèves\",\"ideleves\",\"id_eleve\",\"id\"]:\n",
    "            cand = c\n",
    "            break\n",
    "    if cand and \"StudentID\" in df_results.columns:\n",
    "        df_l3[\"_StudentID\"] = df_l3[cand].astype(str).str.strip()\n",
    "        df_merge = df_results.merge(df_l3, left_on=\"StudentID\", right_on=\"_StudentID\", how=\"inner\", suffixes=(\"_L1\",\"_L3\"))\n",
    "        print(\"\\nMerge L1-L3 via StudentID:\", df_merge.shape)\n",
    "\n",
    "        if \"HoursTotal_L1\" in df_merge.columns and \"HoursTotal\" in df_merge.columns:\n",
    "            diff = (df_merge[\"HoursTotal\"] - df_merge[\"HoursTotal_L1\"])\n",
    "            print(\"\\nCohérence HoursTotal: (L3 - L1) stats\")\n",
    "            print(diff.describe().to_string())\n",
    "else:\n",
    "    print(\"\\nInfo: L3 ne contient pas d'ID explicite (StudentID). On ne merge pas L1 et L3 à cette étape.\")\n",
    "\n",
    "df_results.to_csv(out_path(\"L1_results_clean.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "df_acts.to_csv(out_path(\"L1_activities_clean.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "df_colblock = pd.DataFrame({\n",
    "    \"ActivityColumn\": activity_cols,\n",
    "    \"Block\": [col_to_block[c] for c in activity_cols]\n",
    "})\n",
    "df_colblock.to_csv(out_path(\"L1_activity_column_blocks.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "summary = {\n",
    "    \"L1_path\": L1_PATH,\n",
    "    \"L3_path\": L3_PATH,\n",
    "    \"L1_results_sheet\": results_sheet,\n",
    "    \"L1_activities_sheet\": activities_sheet,\n",
    "    \"L1_results_shape\": df_results.shape,\n",
    "    \"L1_activities_shape\": df_acts.shape,\n",
    "    \"L3_shape\": df_l3.shape,\n",
    "    \"HOURS_PER_ACTIVITY\": HOURS_PER_ACTIVITY,\n",
    "    \"ACTIVITIES_PER_DAY\": ACTIVITIES_PER_DAY,\n",
    "    \"HOURS_PER_DAY\": HOURS_PER_DAY,\n",
    "    \"blocks_detected\": block_counts,\n",
    "    \"n_bad_days_executedcount_ne_4\": int((exec_counts != 4).sum()),\n",
    "    \"n_days_missing_positions\": int((n_miss > 0).sum()),\n",
    "    \"outputs\": {\n",
    "        \"L1_results_clean\": out_path(\"L1_results_clean.csv\"),\n",
    "        \"L1_activities_clean\": out_path(\"L1_activities_clean.csv\"),\n",
    "        \"L1_activity_column_blocks\": out_path(\"L1_activity_column_blocks.csv\"),\n",
    "    }\n",
    "}\n",
    "with open(out_path(\"SECTION1_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    import json\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n Section 1 terminée.\")\n",
    "print(\"Fichiers :\")\n",
    "print(\" -\", out_path(\"L1_results_clean.csv\"))\n",
    "print(\" -\", out_path(\"L1_activities_clean.csv\"))\n",
    "print(\" -\", out_path(\"L1_activity_column_blocks.csv\"))\n",
    "print(\" -\", out_path(\"SECTION1_summary.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c7b72",
   "metadata": {},
   "source": [
    "### Section 1 — Reconstruction des séquences journalières depuis L1 (onglet Activités)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64986362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocs détectés: ['Deb', 'Deuxchiffres', 'TroisA', 'TroisB', 'Unchiffe']\n",
      "\n",
      "Qualité par bloc (résumé):\n",
      " - Deb: total=10 ok=10 bad=0\n",
      " - Deuxchiffres: total=10 ok=9 bad=1\n",
      " - TroisA: total=10 ok=9 bad=1\n",
      " - TroisB: total=10 ok=10 bad=0\n",
      " - Unchiffe: total=10 ok=10 bad=0\n",
      "\n",
      "✅ PAGE 2 terminée.\n",
      "Fichiers produits:\n",
      " - ./out/202602/out_pomdp\\L0-PAGE2_L1_sequences_by_block_long.csv\n",
      " - ./out/202602/out_pomdp\\L0-PAGE2_L1_sequences_by_block_list.csv\n",
      " - ./out/202602/out_pomdp\\L0-PAGE2_L1_sequences_by_block_vector.csv\n",
      " - ./out/202602/out_pomdp\\L0-PAGE2_block_to_activities.json\n",
      " - ./out/202602/out_pomdp\\L0-PAGE2_block_quality.json\n",
      "\n",
      "Aperçu métier (3 premiers jours par bloc) :\n",
      "\n",
      "--- Bloc Deb ---\n",
      "Week 1 Day 1: pos1=Lecture du Tableau de nombres | pos2=Activité avec bâtonnets et paquet | pos3=Gymn aux nombres | pos4=Exercices d’opérations\n",
      "Week 1 Day 2: pos1=Lecture du Tableau de nombres | pos2=Lecture de la Table d’addition | pos3=Opération de base avec problèmes d'addition | pos4=Exercices d’opérations\n",
      "Week 1 Day 3: pos1=Lecture du Tableau de nombres | pos2=Lecture de la Table d’addition | pos3=Opération de base avec problèmes d'addition | pos4=Exercices d’opérations\n",
      "\n",
      "--- Bloc Deuxchiffres ---\n",
      "Week 1 Day 1: pos1=Lecture du Tableau de nombres | pos2=Activité avec bâtonnets et paquet | pos3=Gymn aux nombres | pos4=Exercices d’opérations\n",
      "Week 1 Day 2: pos1=Lecture du Tableau de nombres | pos2=Lecture de la Table d’addition | pos3=Opération de base avec problèmes d'addition | pos4=Exercices d’opérations\n",
      "Week 1 Day 3: pos1=Lecture de la Table d’addition | pos2=Opération de base avec problèmes d'addition | pos3=Mind map Addition | pos4=Exercices d’opérations\n",
      "\n",
      "--- Bloc TroisA ---\n",
      "Week 1 Day 1: pos1=Lecture du Tableau de nombres | pos2=Activité avec bâtonnets et paquet | pos3=Saut aux nombres | pos4=Exercices d’opérations\n",
      "Week 1 Day 2: pos1=Lecture du Tableau de nombres | pos2=Lecture de la Table d’addition | pos3=Opération de base avec problèmes d'addition | pos4=Exercices d’opérations\n",
      "Week 1 Day 3: pos1=Lecture de la Table d’addition | pos2=Opération de base avec problèmes d'addition | pos3=Mind map Addition | pos4=Exercices d’opérations\n",
      "\n",
      "--- Bloc TroisB ---\n",
      "Week 1 Day 1: pos1=Concept de la Multiplication | pos2=Multiplication par échelle | pos3=Lecture de la Table de multiplication | pos4=Exercices d'opérations\n",
      "Week 1 Day 2: pos1=Lecture de la Table de multiplication | pos2=Opération de base avec problèmes de multiplication | pos3=Roue des nombres | pos4=Exercices d'opérations\n",
      "Week 1 Day 3: pos1=Lecture de la Table de multiplication | pos2=Multiplication par échelle | pos3=Opération de base avec problèmes de multiplication | pos4=Exercices d'opérations\n",
      "\n",
      "--- Bloc Unchiffe ---\n",
      "Week 1 Day 1: pos1=Lecture du Tableau de nombres | pos2=Activité avec bâtonnets et paquet | pos3=Gymn aux nombres | pos4=Exercices d’opérations\n",
      "Week 1 Day 2: pos1=Lecture du Tableau de nombres | pos2=Lecture de la Table d’addition | pos3=Opération de base avec problèmes d'addition | pos4=Exercices d’opérations\n",
      "Week 1 Day 3: pos1=Lecture du Tableau de nombres | pos2=Lecture de la Table d’addition | pos3=Opération de base avec problèmes d'addition | pos4=Exercices d’opérations\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Section 1 — Reconstruction des séquences journalières depuis L1 (onglet Activités)\n",
    "+ regroupement par “blocs/niveaux” (préfixes de colonnes)\n",
    "+ extraction d’une séquence ordonnée (pos1..pos4) par jour et par bloc\n",
    "+ production d’artefacts réutilisables pour l’apprentissage (section suivantes)\n",
    "\n",
    "Objectif\n",
    "- Dans L1/Activités : chaque ligne = un jour (Week, Day)\n",
    "- Les colonnes “activités” contiennent des valeurs 0..4 :\n",
    "  0 = non exécutée ce jour\n",
    "  1..4 = position d’exécution (4 activités exécutées/jour)\n",
    "- Les activités sont regroupées par “bloc” via le préfixe (ex: Deb, Unchiffre, Deuxchiffres, TroisA, TroisB, …)\n",
    "- On reconstruit pour chaque (Week,Day,Bloc) la séquence journalière :\n",
    "    pos1 = activité dont la valeur=1\n",
    "    pos2 = activité dont la valeur=2\n",
    "    pos3 = activité dont la valeur=3\n",
    "    pos4 = activité dont la valeur=4\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# 0) CONFIG — reprend la sortie de la Section 1\n",
    "# ============================================================\n",
    "OUT_DIR = \"./out/202602/out_pomdp\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def out_path(fname: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"L0-{fname}\")\n",
    "\n",
    "L1_ACTS_CLEAN = out_path(\"L1_activities_clean.csv\")\n",
    "L1_COLBLOCK   = out_path(\"L1_activity_column_blocks.csv\")\n",
    "\n",
    "assert os.path.exists(L1_ACTS_CLEAN), \"Exécute d’abord la Section 1 (L1_activities_clean.csv introuvable).\"\n",
    "assert os.path.exists(L1_COLBLOCK),   \"Exécute d’abord la Section 1 (L1_activity_column_blocks.csv introuvable).\"\n",
    "\n",
    "df_acts = pd.read_csv(L1_ACTS_CLEAN)\n",
    "df_colblock = pd.read_csv(L1_COLBLOCK)\n",
    "\n",
    "# Attendu: colonnes Week/Day\n",
    "assert \"Week\" in df_acts.columns and \"Day\" in df_acts.columns, \"Week/Day introuvables dans L1_activities_clean.csv\"\n",
    "\n",
    "# ============================================================\n",
    "# 1) IDENTIFIER BLOCS + ACTIVITÉS\n",
    "# ============================================================\n",
    "activity_cols = df_colblock[\"ActivityColumn\"].astype(str).tolist()\n",
    "col_to_block = dict(zip(df_colblock[\"ActivityColumn\"].astype(str), df_colblock[\"Block\"].astype(str)))\n",
    "\n",
    "blocks = sorted(df_colblock[\"Block\"].unique().tolist())\n",
    "print(\"Blocs détectés:\", blocks)\n",
    "\n",
    "# Activités par bloc\n",
    "block_to_cols = {b: [c for c in activity_cols if col_to_block.get(c) == b] for b in blocks}\n",
    "\n",
    "# Fonction: extraire “nom d’activité” (sans préfixe bloc)\n",
    "def activity_name(col: str) -> str:\n",
    "    s = str(col).strip()\n",
    "    # retire le préfixe \"Bloc \"\n",
    "    # ex: \"Deb Lecture du Tableau de nombres\" -> \"Lecture du Tableau de nombres\"\n",
    "    parts = s.split(\" \", 1)\n",
    "    return parts[1].strip() if len(parts) > 1 else s\n",
    "\n",
    "block_to_activities = {b: [activity_name(c) for c in block_to_cols[b]] for b in blocks}\n",
    "\n",
    "# ============================================================\n",
    "# 2) RECONSTRUIRE SEQUENCE (pos1..pos4) POUR UN JOUR ET UN BLOC\n",
    "# ============================================================\n",
    "def extract_day_sequence(row: pd.Series, cols_for_block: list):\n",
    "    \"\"\"\n",
    "    Retourne:\n",
    "      - seq = {pos1:act, pos2:act, pos3:act, pos4:act}\n",
    "      - quality flags (missing positions, duplicate positions, etc.)\n",
    "    \"\"\"\n",
    "    # valeurs dans les colonnes du bloc\n",
    "    vals = row[cols_for_block].values\n",
    "    # map position -> indices d’activités (peut être 0, 1 ou plusieurs si data anormale)\n",
    "    pos_to_idx = {p: np.where(vals == p)[0].tolist() for p in [1,2,3,4]}\n",
    "\n",
    "    seq = {}\n",
    "    issues = []\n",
    "    for p in [1,2,3,4]:\n",
    "        idxs = pos_to_idx[p]\n",
    "        if len(idxs) == 0:\n",
    "            seq[f\"pos{p}\"] = None\n",
    "            issues.append(f\"missing_pos{p}\")\n",
    "        elif len(idxs) > 1:\n",
    "            # anomalie: plusieurs activités marquées même position\n",
    "            seq[f\"pos{p}\"] = activity_name(cols_for_block[idxs[0]])\n",
    "            issues.append(f\"multi_pos{p}\")\n",
    "        else:\n",
    "            seq[f\"pos{p}\"] = activity_name(cols_for_block[idxs[0]])\n",
    "\n",
    "    # contrôle: nb activités exécutées (= count of vals in {1,2,3,4})\n",
    "    executed = int(np.sum(np.isin(vals, [1,2,3,4])))\n",
    "    if executed != 4:\n",
    "        issues.append(f\"executed_count={executed}\")\n",
    "\n",
    "    return seq, issues\n",
    "\n",
    "# ============================================================\n",
    "# 3) CONSTRUIRE TABLES LONGUES (par jour, par bloc)\n",
    "# ============================================================\n",
    "rows_long = []\n",
    "rows_list = []\n",
    "rows_vector = []\n",
    "\n",
    "for _, r in df_acts.iterrows():\n",
    "    wk = int(r[\"Week\"]) if pd.notna(r[\"Week\"]) else None\n",
    "    dy = int(r[\"Day\"]) if pd.notna(r[\"Day\"]) else None\n",
    "\n",
    "    for b in blocks:\n",
    "        cols_b = block_to_cols[b]\n",
    "        if not cols_b:\n",
    "            continue\n",
    "\n",
    "        seq, issues = extract_day_sequence(r, cols_b)\n",
    "\n",
    "        # représentation liste ordonnée [pos1,pos2,pos3,pos4]\n",
    "        seq_list = [seq[\"pos1\"], seq[\"pos2\"], seq[\"pos3\"], seq[\"pos4\"]]\n",
    "\n",
    "        # représentation “vecteur 0..4” sur toutes les activités du bloc\n",
    "        # (même logique que ton format Day i: [0,4,0,2,...])\n",
    "        # chaque entrée = 0 si non exécutée, sinon position 1..4\n",
    "        v = []\n",
    "        for c in cols_b:\n",
    "            x = r[c]\n",
    "            if pd.isna(x):\n",
    "                v.append(0)\n",
    "            else:\n",
    "                xi = int(x)\n",
    "                v.append(xi if xi in [1,2,3,4] else 0)\n",
    "\n",
    "        rows_long.append({\n",
    "            \"Week\": wk, \"Day\": dy, \"Block\": b,\n",
    "            \"pos1\": seq[\"pos1\"], \"pos2\": seq[\"pos2\"], \"pos3\": seq[\"pos3\"], \"pos4\": seq[\"pos4\"],\n",
    "            \"Issues\": \"|\".join(issues) if issues else \"\"\n",
    "        })\n",
    "\n",
    "        rows_list.append({\n",
    "            \"Week\": wk, \"Day\": dy, \"Block\": b,\n",
    "            \"Sequence\": json.dumps(seq_list, ensure_ascii=False),\n",
    "            \"Issues\": \"|\".join(issues) if issues else \"\"\n",
    "        })\n",
    "\n",
    "        # vector row (wide)\n",
    "        row_vec = {\"Week\": wk, \"Day\": dy, \"Block\": b, \"Issues\": \"|\".join(issues) if issues else \"\"}\n",
    "        # noms de colonnes lisibles\n",
    "        for i, c in enumerate(cols_b, start=1):\n",
    "            row_vec[f\"act_{i}:{activity_name(c)}\"] = v[i-1]\n",
    "        rows_vector.append(row_vec)\n",
    "\n",
    "df_long = pd.DataFrame(rows_long)\n",
    "df_list = pd.DataFrame(rows_list)\n",
    "df_vec  = pd.DataFrame(rows_vector)\n",
    "\n",
    "# ============================================================\n",
    "# 4) STATS QUALITE PAR BLOC\n",
    "# ============================================================\n",
    "def issues_stats(df_long_block: pd.DataFrame):\n",
    "    issues = df_long_block[\"Issues\"].fillna(\"\").astype(str)\n",
    "    total = len(df_long_block)\n",
    "    n_ok = int((issues == \"\").sum())\n",
    "    n_bad = total - n_ok\n",
    "\n",
    "    # compter chaque type d’issue\n",
    "    counter = {}\n",
    "    for s in issues:\n",
    "        if not s:\n",
    "            continue\n",
    "        parts = s.split(\"|\")\n",
    "        for p in parts:\n",
    "            counter[p] = counter.get(p, 0) + 1\n",
    "\n",
    "    return {\"total_days\": total, \"ok\": n_ok, \"bad\": n_bad, \"issue_counts\": counter}\n",
    "\n",
    "block_quality = {b: issues_stats(df_long[df_long[\"Block\"] == b]) for b in blocks}\n",
    "\n",
    "print(\"\\nQualité par bloc (résumé):\")\n",
    "for b in blocks:\n",
    "    q = block_quality[b]\n",
    "    print(f\" - {b}: total={q['total_days']} ok={q['ok']} bad={q['bad']}\")\n",
    "\n",
    "df_long.to_csv(out_path(\"SECTION1_L1_sequences_by_block_long.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "df_list.to_csv(out_path(\"SECTION2_L1_sequences_by_block_list.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "df_vec.to_csv(out_path(\"SECTION2_L1_sequences_by_block_vector.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "with open(out_path(\"SECTION2_block_to_activities.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(block_to_activities, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(out_path(\"SECTION2_block_quality.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(block_quality, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n Section 1 terminée.\")\n",
    "print(\"Fichiers :\")\n",
    "print(\" -\", out_path(\"SECTION2_L1_sequences_by_block_long.csv\"))\n",
    "print(\" -\", out_path(\"SECTION2_L1_sequences_by_block_list.csv\"))\n",
    "print(\" -\", out_path(\"SECTION2_L1_sequences_by_block_vector.csv\"))\n",
    "print(\" -\", out_path(\"SECTION2_block_to_activities.json\"))\n",
    "print(\" -\", out_path(\"SECTION2_block_quality.json\"))\n",
    "\n",
    "# ============================================================\n",
    "# 6) Afficher 3 jours pour chaque bloc\n",
    "# ============================================================\n",
    "print(\"\\nAperçu  (3 premiers jours par bloc) :\")\n",
    "for b in blocks:\n",
    "    sub = df_long[df_long[\"Block\"] == b].head(3)\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    print(f\"\\n--- Bloc {b} ---\")\n",
    "    for _, rr in sub.iterrows():\n",
    "        print(f\"Week {rr['Week']} Day {rr['Day']}: pos1={rr['pos1']} | pos2={rr['pos2']} | pos3={rr['pos3']} | pos4={rr['pos4']}\"\n",
    "              + (f\"  [Issues: {rr['Issues']}]\" if rr['Issues'] else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b6f88",
   "metadata": {},
   "source": [
    "### Section 1 — Mapper automatiquement les “blocs” (préfixes L1) vers les NIVEAUX 1..5 (S = {1..5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c3b746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Diag] LevelTag -> Level (à partir de L3 / Pretest_i):\n",
      "LevelTag  DominantLevel  DominantShare     Counts\n",
      "      L1              1            1.0   {\"1\": 4}\n",
      "      L2              2            1.0  {\"2\": 66}\n",
      "      L3              3            1.0 {\"3\": 166}\n",
      "      L4              4            1.0 {\"4\": 230}\n",
      "      L5              5            1.0 {\"5\": 347}\n",
      "\n",
      "✅ Mapping Block -> Level produit :\n",
      "       Block  Level MatchedLevelTag  Similarity                            Method\n",
      "         Deb      1              L1         0.0 text_similarity_block_vs_leveltag\n",
      "Deuxchiffres      1              L1         0.0 text_similarity_block_vs_leveltag\n",
      "      TroisA      1              L1         0.0 text_similarity_block_vs_leveltag\n",
      "      TroisB      1              L1         0.0 text_similarity_block_vs_leveltag\n",
      "    Unchiffe      1              L1         0.0 text_similarity_block_vs_leveltag\n",
      "\n",
      "[Check] TroisA -> Level 1 (selon mapping)\n",
      "\n",
      "Fichiers produits:\n",
      " - ./out/202602/out_pomdp\\L0-PAGE3_block_to_level.csv\n",
      " - ./out/202602/out_pomdp\\L0-PAGE3_block_to_level.json\n",
      " - ./out/202602/out_pomdp\\L0-PAGE3_leveltag_to_level.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Section 1 — Mapper automatiquement les “blocs” (préfixes L1) vers les NIVEAUX 1..5 (S = {1..5})\n",
    "Objectif \n",
    "- Dans L1 (onglet Activités), les colonnes d’activités sont regroupées par blocs (préfixes).\n",
    "- Dans L3 (features_by_student), tu as déjà une notion de niveau via Pretest_i ∈ {1..5} et (souvent) LevelTag.\n",
    "- Cette page construit un mapping robuste:\n",
    "    Block (L1)  ---> Level (1..5)\n",
    "  en utilisant L3 comme référence (distribution Pretest par LevelTag),\n",
    "  puis en “matchant” les noms (Block vs LevelTag) par similarité de texte.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# ============================================================\n",
    "# 0) CONFIG / INPUTS\n",
    "# ============================================================\n",
    "OUT_DIR = \"./out/202602/out_pomdp\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def out_path(fname: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"L0-{fname}\")\n",
    "\n",
    "# Outputs SECTION1\n",
    "SEQ_LONG = out_path(\"SECTION2_L1_sequences_by_block_long.csv\")\n",
    "COLBLOCK = out_path(\"L1_activity_column_blocks.csv\")          # from SECTION2\n",
    "BLOCK_ACTS_JSON = out_path(\"SECTION2_block_to_activities.json\")  # from SECTION2\n",
    "\n",
    "# L3\n",
    "L3_PATH = \"./data/2025-08/L3.features_by_student.byPretest.csv\"  # adapte si besoin\n",
    "assert os.path.exists(L3_PATH), f\"Introuvable L3 : {L3_PATH}\"\n",
    "\n",
    "assert os.path.exists(SEQ_LONG), \"Exécute Section 1 d’abord (SECTION2_L1_sequences_by_block_long.csv introuvable).\"\n",
    "assert os.path.exists(COLBLOCK), \"Exécute Section 1 d’abord (L1_activity_column_blocks.csv introuvable).\"\n",
    "assert os.path.exists(BLOCK_ACTS_JSON), \"Exécute Section 1 d’abord (SECTION2_block_to_activities.json introuvable).\"\n",
    "\n",
    "df_seq_long = pd.read_csv(SEQ_LONG)\n",
    "df_colblock = pd.read_csv(COLBLOCK)\n",
    "df_l3 = pd.read_csv(L3_PATH)\n",
    "\n",
    "# ============================================================\n",
    "# 1) HELPERS\n",
    "# ============================================================\n",
    "def norm_text(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"[’'`]\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def sim(a: str, b: str) -> float:\n",
    "    return SequenceMatcher(None, norm_text(a), norm_text(b)).ratio()\n",
    "\n",
    "def cap_level(x):\n",
    "    try:\n",
    "        v = int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return min(5, max(1, v))\n",
    "\n",
    "# Assure Pretest_i dans L3\n",
    "if \"Pretest_i\" not in df_l3.columns:\n",
    "    if \"Pretest\" in df_l3.columns:\n",
    "        df_l3[\"Pretest_i\"] = df_l3[\"Pretest\"].apply(cap_level)\n",
    "    else:\n",
    "        raise ValueError(\"L3 doit contenir Pretest (ou Pretest_i).\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) (A) Déduire LevelTag -> Level via distribution Pretest\n",
    "# ============================================================\n",
    "leveltag_to_level = {}\n",
    "leveltag_diag_rows = []\n",
    "\n",
    "if \"LevelTag\" in df_l3.columns:\n",
    "    for tag, g in df_l3.dropna(subset=[\"LevelTag\",\"Pretest_i\"]).groupby(\"LevelTag\"):\n",
    "        # niveau dominant (mode) dans ce tag\n",
    "        counts = g[\"Pretest_i\"].astype(int).value_counts().sort_index()\n",
    "        if len(counts) == 0:\n",
    "            continue\n",
    "        dominant_level = int(counts.idxmax())\n",
    "        share = float(counts.max() / counts.sum())\n",
    "        leveltag_to_level[str(tag)] = dominant_level\n",
    "        leveltag_diag_rows.append({\n",
    "            \"LevelTag\": str(tag),\n",
    "            \"DominantLevel\": dominant_level,\n",
    "            \"DominantShare\": round(share, 4),\n",
    "            \"Counts\": json.dumps({int(k): int(v) for k,v in counts.to_dict().items()})\n",
    "        })\n",
    "\n",
    "df_leveltag_diag = pd.DataFrame(leveltag_diag_rows).sort_values(\n",
    "    [\"DominantLevel\",\"DominantShare\"], ascending=[True, False]\n",
    ")\n",
    "\n",
    "df_leveltag_diag.to_csv(out_path(\"SECTION2_leveltag_to_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[Diag] LevelTag -> Level (à partir de L3 / Pretest_i):\")\n",
    "if len(df_leveltag_diag) == 0:\n",
    "    print(\" - Aucun LevelTag exploitable trouvé dans L3 (colonne LevelTag absente ou vide).\")\n",
    "else:\n",
    "    print(df_leveltag_diag.head(30).to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# 3) (B) Mapper Block (L1) -> Level via matching texte Block <-> LevelTag\n",
    "# ============================================================\n",
    "blocks = sorted(df_colblock[\"Block\"].astype(str).unique().tolist())\n",
    "\n",
    "# si pas de LevelTag dans L3, fallback: mapping ordinal par fréquence d’apparition\n",
    "# (moins fiable, mais au moins automatique)\n",
    "has_leveltag = (\"LevelTag\" in df_l3.columns) and (len(df_leveltag_diag) > 0)\n",
    "\n",
    "mapping_rows = []\n",
    "block_to_level = {}\n",
    "\n",
    "# Option: override manuel si tu veux forcer certains cas après inspection\n",
    "MANUAL_OVERRIDE_BLOCK_TO_LEVEL = {\n",
    "    # Exemple (à compléter si besoin):\n",
    "    # \"TroisA\": 4,\n",
    "    # \"TroisB\": 5,\n",
    "}\n",
    "\n",
    "if has_leveltag:\n",
    "    tags = list(leveltag_to_level.keys())\n",
    "\n",
    "    for b in blocks:\n",
    "        if b in MANUAL_OVERRIDE_BLOCK_TO_LEVEL:\n",
    "            lvl = int(MANUAL_OVERRIDE_BLOCK_TO_LEVEL[b])\n",
    "            block_to_level[b] = lvl\n",
    "            mapping_rows.append({\n",
    "                \"Block\": b,\n",
    "                \"MatchedLevelTag\": \"__MANUAL_OVERRIDE__\",\n",
    "                \"Level\": lvl,\n",
    "                \"Similarity\": 1.0,\n",
    "                \"Method\": \"manual_override\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # meilleur match par similarité texte\n",
    "        scored = [(t, sim(b, t)) for t in tags]\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        best_tag, best_score = scored[0]\n",
    "        lvl = int(leveltag_to_level[best_tag])\n",
    "\n",
    "        # garde aussi les 3 meilleurs pour audit\n",
    "        top3 = scored[:3]\n",
    "        mapping_rows.append({\n",
    "            \"Block\": b,\n",
    "            \"MatchedLevelTag\": best_tag,\n",
    "            \"Level\": lvl,\n",
    "            \"Similarity\": round(float(best_score), 4),\n",
    "            \"Top3\": json.dumps([(t, round(float(sc),4)) for t,sc in top3], ensure_ascii=False),\n",
    "            \"Method\": \"text_similarity_block_vs_leveltag\"\n",
    "        })\n",
    "        block_to_level[b] = lvl\n",
    "\n",
    "else:\n",
    "    # Fallback: ordonner les blocs par “activité dans le temps”:\n",
    "    # hypothèse: les blocs “débutants” apparaissent plus tôt (Week/Day petits)\n",
    "    # => on calcule pour chaque block la médiane du (Week,Day) quand il est réellement utilisé (>=1)\n",
    "    # puis on mappe les 5 premiers rangs -> niveaux 1..5\n",
    "    print(\"\\n[WARN] Fallback sans LevelTag: mapping ordinal basé sur la temporalité (moins fiable).\")\n",
    "\n",
    "    # construit un score “timing” par block\n",
    "    timing = []\n",
    "    # utilisation d’un jour si au moins une des pos1..pos4 est non nulle\n",
    "    used = df_seq_long.copy()\n",
    "    used[\"is_used\"] = used[[\"pos1\",\"pos2\",\"pos3\",\"pos4\"]].notna().any(axis=1).astype(int)\n",
    "\n",
    "    for b in blocks:\n",
    "        gb = used[(used[\"Block\"] == b) & (used[\"is_used\"] == 1)]\n",
    "        if len(gb) == 0:\n",
    "            # si jamais bloc jamais utilisé (rare), on met un grand score\n",
    "            med = 10**9\n",
    "        else:\n",
    "            # score = median(Week*100 + Day)\n",
    "            med = float(np.median(gb[\"Week\"].fillna(0).values * 100 + gb[\"Day\"].fillna(0).values))\n",
    "        timing.append((b, med))\n",
    "\n",
    "    timing.sort(key=lambda x: x[1])\n",
    "    # si >5 blocs, on “compresse” sur 1..5 par quantiles\n",
    "    meds = np.array([t[1] for t in timing], dtype=float)\n",
    "    if len(meds) == 0:\n",
    "        raise ValueError(\"Impossible de construire un mapping (aucun bloc détecté).\")\n",
    "\n",
    "    # quantiles\n",
    "    q = np.quantile(meds, [0.2, 0.4, 0.6, 0.8])\n",
    "    def quantile_to_level(m):\n",
    "        if m <= q[0]: return 1\n",
    "        if m <= q[1]: return 2\n",
    "        if m <= q[2]: return 3\n",
    "        if m <= q[3]: return 4\n",
    "        return 5\n",
    "\n",
    "    for b, med in timing:\n",
    "        if b in MANUAL_OVERRIDE_BLOCK_TO_LEVEL:\n",
    "            lvl = int(MANUAL_OVERRIDE_BLOCK_TO_LEVEL[b])\n",
    "            method = \"manual_override\"\n",
    "        else:\n",
    "            lvl = quantile_to_level(med)\n",
    "            method = \"timing_quantile\"\n",
    "        block_to_level[b] = lvl\n",
    "        mapping_rows.append({\n",
    "            \"Block\": b,\n",
    "            \"MatchedLevelTag\": \"\",\n",
    "            \"Level\": lvl,\n",
    "            \"Similarity\": \"\",\n",
    "            \"Top3\": \"\",\n",
    "            \"Method\": method,\n",
    "            \"MedianWeekDayScore\": med\n",
    "        })\n",
    "\n",
    "df_map = pd.DataFrame(mapping_rows)\n",
    "\n",
    "df_map.to_csv(out_path(\"SECTION2_block_to_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "with open(out_path(\"SECTION2_block_to_level.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(block_to_level, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n Mapping Block -> Level produit :\")\n",
    "print(df_map[[\"Block\",\"Level\",\"MatchedLevelTag\",\"Similarity\",\"Method\"]].to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# 4) (C) Vérif  : cohérence TroisA/TroisB\n",
    "# ============================================================\n",
    "for key in [\"TroisA\", \"TroisB\", \"troisA\", \"troisB\"]:\n",
    "    # on normalise par exact match “Block”\n",
    "    if key in block_to_level:\n",
    "        print(f\"\\n[Check] {key} -> Level {block_to_level[key]} (selon mapping)\")\n",
    "        break\n",
    "\n",
    "print(\"\\nFichiers :\")\n",
    "print(\" -\", out_path(\"SECTION2_block_to_level.csv\"))\n",
    "print(\" -\", out_path(\"SECTION2_block_to_level.json\"))\n",
    "print(\" -\", out_path(\"SECTION2_leveltag_to_level.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f758cf",
   "metadata": {},
   "source": [
    "### Section 1 — (1) Définir Action observée A/B via durée réelle (HoursTotal) \n",
    "### (2) Apprendre une politique empirique π_ML(a|s,features) (action réellement observée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a6ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== PAGE 4 — Action label (A/B) via HoursTotal ====================\n",
      "HOURS_PER_DAY = 8.0h, STANDARD_HOURS_A≈80.0h (10 jours)\n",
      "Règle de labellisation observée: Action=A si DaysTotal >= 9.0 jours, sinon B\n",
      "\n",
      "Aperçu HoursTotal/DaysTotal/Action_obs:\n",
      " HoursTotal  DaysTotal Action_obs\n",
      "         60        7.5          B\n",
      "         60        7.5          B\n",
      "         60        7.5          B\n",
      "         60        7.5          B\n",
      "         60        7.5          B\n",
      "         60        7.5          B\n",
      "         60        7.5          B\n",
      "         60        7.5          B\n",
      "         60        7.5          B\n",
      "         60        7.5          B\n",
      "\n",
      "Statistiques DaysTotal:\n",
      "count    813.000000\n",
      "mean       5.648831\n",
      "std        2.116959\n",
      "min        2.500000\n",
      "25%        2.500000\n",
      "50%        7.500000\n",
      "75%        7.500000\n",
      "max        7.500000\n",
      "\n",
      "Répartition Action_obs:\n",
      "Action_obs\n",
      "B    813\n",
      "\n",
      "[Hypothèse Elysa] Répartition observée (Action_obs) par niveau Pretest:\n",
      " Level   N  P(A)  P(B)\n",
      "     1   4   0.0   1.0\n",
      "     2  66   0.0   1.0\n",
      "     3 166   0.0   1.0\n",
      "     4 230   0.0   1.0\n",
      "     5 347   0.0   1.0\n",
      "\n",
      "[DIAG] Répartition y (Action_obs==A):\n",
      "Action_obs\n",
      "0    813\n",
      "PosRate(y=1) = 0.0\n",
      "\n",
      "==================== PAGE 4 — Model selection for π_ML(a|s,features) ====================\n",
      "                     Model  AUC  F1  ACC  PosRate(y=1)  PredPosRate\n",
      "    RandomForestClassifier  NaN 0.0  1.0           0.0          0.0\n",
      "GradientBoostingClassifier  NaN 0.0  1.0           0.0          0.0\n",
      "        LogisticRegression  NaN 0.0  1.0           0.0          0.0\n",
      "                       SVC  NaN 0.0  1.0           0.0          0.0\n",
      "         XGBoostClassifier  NaN 0.0  1.0           0.0          0.0\n",
      "\n",
      "✅ Best π_ML model = RandomForestClassifier\n",
      "\n",
      "[Hypothèse Elysa] Décision π_ML (Action_hat) par niveau Pretest:\n",
      " Level   N  P(A)  P(B)\n",
      "     1   4   0.0   1.0\n",
      "     2  66   0.0   1.0\n",
      "     3 166   0.0   1.0\n",
      "     4 230   0.0   1.0\n",
      "     5 347   0.0   1.0\n",
      "\n",
      "[Hypothèse Elysa] Synthèse groupée:\n",
      "     Group   N  Obs P(A)  π_ML P(A_hat) Expected\n",
      " Low (1-2)  70       0.0            0.0     High\n",
      "High (3-5) 743       0.0            0.0      Low\n",
      "\n",
      "Fichiers produits:\n",
      " - ./out/202602/out_pomdp\\L0-PAGE4_L3_with_action_labels.csv\n",
      " - ./out/202602/out_pomdp\\L0-PAGE4_policyML_model_selection.csv\n",
      " - ./out/202602/out_pomdp\\L0-PAGE4_policyML_predictions.csv\n",
      " - ./out/202602/out_pomdp\\L0-PAGE4_hypothesis_obs_by_level.csv\n",
      " - ./out/202602/out_pomdp\\L0-PAGE4_hypothesis_piML_by_level.csv\n",
      " - ./out/202602/out_pomdp\\L0-PAGE4_hypothesis_support_summary.csv\n",
      " - ./out/202602/out_pomdp\\L0-PAGE4_best_policyML_model.joblib\n",
      " - ./out/202602/out_pomdp\\L0-PAGE4_best_policyML_model_meta.json\n",
      "\n",
      "XGBoost disponible ? True\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Section 1 — (1) Définir Action observée A/B via durée réelle (HoursTotal)\n",
    "         (2) Apprendre une politique empirique π_ML(a|s,features) (action réellement observée)\n",
    "\n",
    "Rappel  (règles) :\n",
    "- 1 activité = 2h (fixe)\n",
    "- 4 activités / jour  => 8h / jour\n",
    "- Action A = “standard” (≈ 10 jours) => ≈ 10 * 8h = 80h\n",
    "- Action B = “intensive” (X jours < 10, X inconnu) => HoursTotal < 80h\n",
    "\n",
    "Donc on étiquette l’action OBSERVÉE par élève (dans L3) via HoursTotal :\n",
    "- A si DaysTotal >= seuil proche de 10\n",
    "- B sinon\n",
    "\n",
    "Puis on entraîne un modèle ML pour apprendre π_ML(a|s,features) :\n",
    "- Entrées = (state s = Pretest_i) + features (Age/Genre/Zone/LevelTag + freq_pos* + freq_all*)\n",
    "- Sortie = P(Action=A|x), P(Action=B|x)\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# optional: xgboost\n",
    "xgb_ok = True\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception:\n",
    "    xgb_ok = False\n",
    "\n",
    "# persist model\n",
    "try:\n",
    "    import joblib\n",
    "    joblib_ok = True\n",
    "except Exception:\n",
    "    joblib_ok = False\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) CONFIG / INPUTS\n",
    "# ============================================================\n",
    "OUT_DIR = \"./out/202602/out_pomdp\"  # adapte si besoin\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def out_path(fname: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"L0-{fname}\")\n",
    "\n",
    "L3_PATH = \"./data/2025-08/L3.features_by_student.byPretest.csv\"  # adapte si besoin\n",
    "assert os.path.exists(L3_PATH), f\"Introuvable : {L3_PATH}\"\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# \n",
    "HOURS_PER_ACTIVITY = 2.0\n",
    "ACTIVITIES_PER_DAY = 4.0\n",
    "HOURS_PER_DAY = HOURS_PER_ACTIVITY * ACTIVITIES_PER_DAY   # 8h\n",
    "STANDARD_DAYS_A = 10.0\n",
    "STANDARD_HOURS_A = STANDARD_DAYS_A * HOURS_PER_DAY         # 80h\n",
    "\n",
    "# seuil pour décider A vs B (ajustable)\n",
    "# -> proche de 10 jours, on tolère un peu d’écart (ex : 9 jours et + => A)\n",
    "THRESHOLD_DAYS_FOR_A = 9.0\n",
    "\n",
    "A, B = \"A\", \"B\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) LOAD L3 + FEATURES\n",
    "# ============================================================\n",
    "df = pd.read_csv(L3_PATH)\n",
    "\n",
    "def cap_level(x):\n",
    "    try:\n",
    "        v = int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return min(5, max(1, v))\n",
    "\n",
    "if \"Pretest_i\" not in df.columns:\n",
    "    if \"Pretest\" in df.columns:\n",
    "        df[\"Pretest_i\"] = df[\"Pretest\"].apply(cap_level)\n",
    "    else:\n",
    "        raise ValueError(\"L3 doit contenir Pretest ou Pretest_i\")\n",
    "\n",
    "if \"HoursTotal\" not in df.columns:\n",
    "    raise ValueError(\"L3 doit contenir la colonne HoursTotal (tu as demandé de l'utiliser).\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) LABEL ACTION OBSERVÉE via HoursTotal -> DaysTotal\n",
    "# ============================================================\n",
    "df[\"DaysTotal\"] = df[\"HoursTotal\"].astype(float) / float(HOURS_PER_DAY)\n",
    "\n",
    "# action observée\n",
    "df[\"Action_obs\"] = np.where(df[\"DaysTotal\"] >= THRESHOLD_DAYS_FOR_A, A, B)\n",
    "\n",
    "print(\"\\n==================== Section 1 — Action label (A/B) via HoursTotal ====================\")\n",
    "print(f\"HOURS_PER_DAY = {HOURS_PER_DAY:.1f}h, STANDARD_HOURS_A≈{STANDARD_HOURS_A:.1f}h (10 jours)\")\n",
    "print(f\"Règle de labellisation observée: Action=A si DaysTotal >= {THRESHOLD_DAYS_FOR_A:.1f} jours, sinon B\")\n",
    "print(\"\\nAperçu HoursTotal/DaysTotal/Action_obs:\")\n",
    "print(df[[\"HoursTotal\", \"DaysTotal\", \"Action_obs\"]].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nStatistiques DaysTotal:\")\n",
    "print(df[\"DaysTotal\"].describe().to_string())\n",
    "\n",
    "print(\"\\nRépartition Action_obs:\")\n",
    "print(df[\"Action_obs\"].value_counts(dropna=False).to_string())\n",
    "\n",
    "df.to_csv(out_path(\"SECTION1_L3_with_action_labels.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) TEST HYPOTHÈSE (Elysa) sur OBSERVÉ: niveaux 1-2 => A, niveaux 3-4-5 => B\n",
    "# ============================================================\n",
    "def summarize_action_by_level(df0: pd.DataFrame, label_col=\"Action_obs\"):\n",
    "    rows = []\n",
    "    tmp = df0.dropna(subset=[\"Pretest_i\"]).copy()\n",
    "    for lvl, g in tmp.groupby(\"Pretest_i\"):\n",
    "        lvl = int(lvl)\n",
    "        n = len(g)\n",
    "        pA = float((g[label_col] == A).mean()) if n > 0 else np.nan\n",
    "        pB = float((g[label_col] == B).mean()) if n > 0 else np.nan\n",
    "        rows.append({\"Level\": lvl, \"N\": n, \"P(A)\": round(pA, 4), \"P(B)\": round(pB, 4)})\n",
    "    return pd.DataFrame(rows).sort_values(\"Level\")\n",
    "\n",
    "df_hyp_obs = summarize_action_by_level(df, label_col=\"Action_obs\")\n",
    "print(\"\\n[Hypothèse Elysa] Répartition observée (Action_obs) par niveau Pretest:\")\n",
    "print(df_hyp_obs.to_string(index=False))\n",
    "df_hyp_obs.to_csv(out_path(\"SECTION2_hypothesis_obs_by_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) APPRENDRE π_ML(a|s,features)  (classification binaire A vs B)\n",
    "# ============================================================\n",
    "demographic_cols = [c for c in [\"Age\", \"Genre\", \"Zone\", \"LevelTag\"] if c in df.columns]\n",
    "state_col = \"Pretest_i\"\n",
    "\n",
    "freq_cols = [c for c in df.columns if c.startswith(\"freq_pos\") or c.startswith(\"freq_all\")]\n",
    "feature_cols = [state_col] + demographic_cols + freq_cols\n",
    "\n",
    "missing = [c for c in feature_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Colonnes manquantes dans L3 pour apprendre π_ML : {missing}\")\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = (df[\"Action_obs\"] == A).astype(int)  # 1=A, 0=B\n",
    "\n",
    "print(\"\\n[DIAG] Répartition y (Action_obs==A):\")\n",
    "print(y.value_counts(dropna=False).to_string())\n",
    "print(\"PosRate(y=1) =\", float(y.mean()))\n",
    "\n",
    "# preprocessing\n",
    "num_cols, cat_cols = [], []\n",
    "for c in X.columns:\n",
    "    if c in [\"Genre\", \"Zone\", \"LevelTag\"]:\n",
    "        cat_cols.append(c)\n",
    "    else:\n",
    "        num_cols.append(c)\n",
    "\n",
    "preproc = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                      (\"sc\", StandardScaler())]),\n",
    "     num_cols),\n",
    "    (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                      (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))]),\n",
    "     cat_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# candidats (avec class_weight pour stabilité quand déséquilibré)\n",
    "candidates = {\n",
    "    \"RandomForestClassifier\": Pipeline([(\"prep\", preproc),\n",
    "                                        (\"m\", RandomForestClassifier(\n",
    "                                            n_estimators=250, random_state=RANDOM_STATE,\n",
    "                                            max_depth=12, n_jobs=1,\n",
    "                                            class_weight=\"balanced\"\n",
    "                                        ))]),\n",
    "    \"GradientBoostingClassifier\": Pipeline([(\"prep\", preproc),\n",
    "                                           (\"m\", GradientBoostingClassifier(\n",
    "                                               random_state=RANDOM_STATE, n_estimators=250\n",
    "                                           ))]),\n",
    "    \"LogisticRegression\": Pipeline([(\"prep\", preproc),\n",
    "                                    (\"m\", LogisticRegression(\n",
    "                                        max_iter=5000,\n",
    "                                        class_weight=\"balanced\"\n",
    "                                    ))]),\n",
    "    \"SVC\": Pipeline([(\"prep\", preproc),\n",
    "                     (\"m\", SVC(\n",
    "                         probability=True,\n",
    "                         random_state=RANDOM_STATE,\n",
    "                         class_weight=\"balanced\"\n",
    "                     ))]),\n",
    "    (\"XGBoostClassifier\" if xgb_ok else \"HistGradientBoostingClassifier\"): Pipeline([\n",
    "        (\"prep\", preproc),\n",
    "        (\"m\", xgb.XGBClassifier(\n",
    "            n_estimators=300, learning_rate=0.07, max_depth=5, subsample=0.9,\n",
    "            colsample_bytree=0.9, random_state=RANDOM_STATE, n_jobs=1,\n",
    "            eval_metric=\"logloss\"\n",
    "        ) if xgb_ok else HistGradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# ---------- Robust proba helpers ----------\n",
    "def _sigmoid(z):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def _proba_class1(fitted_pipe, X_batch):\n",
    "    \"\"\"\n",
    "    Retourne P(y=1|x) robuste :\n",
    "    - si predict_proba dispo et a 2 colonnes => ok (en respectant classes_)\n",
    "    - si une seule classe vue à l'entraînement => renvoie 0 ou 1 constant selon la classe\n",
    "    - sinon fallback decision_function -> sigmoid\n",
    "    \"\"\"\n",
    "    m = fitted_pipe\n",
    "\n",
    "    if hasattr(m, \"predict_proba\"):\n",
    "        proba = m.predict_proba(X_batch)\n",
    "\n",
    "        # cas normal: 2 colonnes\n",
    "        if proba.shape[1] >= 2:\n",
    "            cls = getattr(m, \"classes_\", None)\n",
    "            if cls is None:\n",
    "                return proba[:, 1]\n",
    "            cls = list(cls)\n",
    "            if 1 in cls:\n",
    "                return proba[:, cls.index(1)]\n",
    "            return proba[:, 1]\n",
    "\n",
    "        # cas 1 colonne => une seule classe dans ce fold\n",
    "        cls = getattr(m, \"classes_\", None)\n",
    "        if cls is None:\n",
    "            return np.zeros(len(X_batch), dtype=float)\n",
    "        only = int(list(cls)[0])\n",
    "        return np.full(len(X_batch), float(only), dtype=float)\n",
    "\n",
    "    if hasattr(m, \"decision_function\"):\n",
    "        scores = m.decision_function(X_batch)\n",
    "        scores = np.asarray(scores).ravel()\n",
    "        return _sigmoid(scores)\n",
    "\n",
    "    # dernier fallback\n",
    "    pred = m.predict(X_batch)\n",
    "    return np.asarray(pred, dtype=float)\n",
    "\n",
    "\n",
    "def eval_policy_ml_cv(X, y, pipe, n_splits=5):\n",
    "    \"\"\"\n",
    "    Évaluation CV robuste pour π_ML.\n",
    "    - StratifiedKFold pour préserver le ratio A/B.\n",
    "    - Gère les folds à une seule classe (sinon predict_proba[:,1] plante).\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    probs_all, preds_all, y_all = [], [], []\n",
    "\n",
    "    for tr, te in skf.split(X, y):\n",
    "        m = clone(pipe)\n",
    "        y_tr = y.iloc[tr]\n",
    "\n",
    "        # Si le fold train contient une seule classe => proba constante\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            only = int(np.unique(y_tr)[0])\n",
    "            pr = np.full(len(te), float(only), dtype=float)\n",
    "        else:\n",
    "            m.fit(X.iloc[tr], y_tr)\n",
    "            pr = _proba_class1(m, X.iloc[te])\n",
    "\n",
    "        pd_ = (pr >= 0.5).astype(int)\n",
    "\n",
    "        probs_all.extend(pr.tolist())\n",
    "        preds_all.extend(pd_.tolist())\n",
    "        y_all.extend(y.iloc[te].tolist())\n",
    "\n",
    "    probs_all = np.array(probs_all, dtype=float)\n",
    "    preds_all = np.array(preds_all, dtype=int)\n",
    "    y_all = np.array(y_all, dtype=int)\n",
    "\n",
    "    auc = np.nan\n",
    "    if len(np.unique(y_all)) > 1:\n",
    "        auc = float(roc_auc_score(y_all, probs_all))\n",
    "\n",
    "    return {\n",
    "        \"AUC\": auc,\n",
    "        \"F1\": float(f1_score(y_all, preds_all)),\n",
    "        \"ACC\": float(accuracy_score(y_all, preds_all)),\n",
    "        \"PosRate(y=1)\": float(y.mean()),\n",
    "        \"PredPosRate\": float(preds_all.mean())\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\n==================== Section 2 — Model selection for π_ML(a|s,features) ====================\")\n",
    "rows = []\n",
    "scores = {}\n",
    "\n",
    "for name, pipe in candidates.items():\n",
    "    sc = eval_policy_ml_cv(X, y, pipe, n_splits=5)\n",
    "    scores[name] = sc\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        \"AUC\": (round(sc[\"AUC\"], 4) if isinstance(sc[\"AUC\"], float) and not np.isnan(sc[\"AUC\"]) else sc[\"AUC\"]),\n",
    "        \"F1\": round(sc[\"F1\"], 4),\n",
    "        \"ACC\": round(sc[\"ACC\"], 4),\n",
    "        \"PosRate(y=1)\": round(sc[\"PosRate(y=1)\"], 4),\n",
    "        \"PredPosRate\": round(sc[\"PredPosRate\"], 4),\n",
    "    })\n",
    "\n",
    "df_sel = pd.DataFrame(rows)\n",
    "\n",
    "# tri robuste : AUC d'abord (NaN -> -1), puis F1 puis ACC\n",
    "def _auc_for_sort(v):\n",
    "    try:\n",
    "        if v is None or (isinstance(v, float) and np.isnan(v)):\n",
    "            return -1.0\n",
    "        return float(v)\n",
    "    except Exception:\n",
    "        return -1.0\n",
    "\n",
    "df_sel[\"_AUCsort\"] = df_sel[\"AUC\"].apply(_auc_for_sort)\n",
    "df_sel = df_sel.sort_values([\"_AUCsort\", \"F1\", \"ACC\"], ascending=False).drop(columns=[\"_AUCsort\"])\n",
    "\n",
    "print(df_sel.to_string(index=False))\n",
    "df_sel.to_csv(out_path(\"SECTION2_policyML_model_selection.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# choisir le meilleur\n",
    "best_model_name = df_sel.iloc[0][\"Model\"]\n",
    "best_pipe = candidates[best_model_name]\n",
    "best_pipe.fit(X, y)\n",
    "print(f\"\\n Best π_ML model = {best_model_name}\")\n",
    "\n",
    "# persister (si joblib dispo)\n",
    "if joblib_ok:\n",
    "    joblib.dump(best_pipe, out_path(\"SECTION2_best_policyML_model.joblib\"))\n",
    "    with open(out_path(\"SECTION2_best_policyML_model_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"best_model_name\": best_model_name,\n",
    "            \"threshold_days_for_A\": THRESHOLD_DAYS_FOR_A,\n",
    "            \"hours_per_day\": HOURS_PER_DAY,\n",
    "            \"standard_days_A\": STANDARD_DAYS_A,\n",
    "            \"standard_hours_A\": STANDARD_HOURS_A\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "else:\n",
    "    print(\"[WARN] joblib indisponible: le modèle n'est pas sérialisé.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) PRODUITS: π_ML prédictions + analyse hypothèse sur π_ML\n",
    "# ============================================================\n",
    "# prédire P(A|x) de façon robuste (même si classes_ bizarre)\n",
    "pA = _proba_class1(best_pipe, X)\n",
    "\n",
    "df[\"piML_P(A)\"] = pA\n",
    "df[\"piML_Action_hat\"] = np.where(df[\"piML_P(A)\"] >= 0.5, A, B)\n",
    "\n",
    "pred_cols = [\"Pretest_i\", \"HoursTotal\", \"DaysTotal\", \"Action_obs\", \"piML_P(A)\", \"piML_Action_hat\"]\n",
    "for c in [\"Age\", \"Genre\", \"Zone\", \"LevelTag\"]:\n",
    "    if c in df.columns:\n",
    "        pred_cols.insert(1, c)\n",
    "\n",
    "df[pred_cols].to_csv(out_path(\"SECTION2_policyML_predictions.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# hypothèse Elysa sur π_ML\n",
    "df_hyp_ml = summarize_action_by_level(df, label_col=\"piML_Action_hat\")\n",
    "print(\"\\n[Hypothèse Elysa] Décision π_ML (Action_hat) par niveau Pretest:\")\n",
    "print(df_hyp_ml.to_string(index=False))\n",
    "df_hyp_ml.to_csv(out_path(\"SECTION2_hypothesis_piML_by_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# synthèse groupée “support hypothèse”\n",
    "support_rows = []\n",
    "for group_name, levels in [(\"Low (1-2)\", [1, 2]), (\"High (3-5)\", [3, 4, 5])]:\n",
    "    g = df[df[\"Pretest_i\"].isin(levels)]\n",
    "    if len(g) == 0:\n",
    "        continue\n",
    "    pA_obs = float((g[\"Action_obs\"] == A).mean())\n",
    "    pA_ml  = float((g[\"piML_Action_hat\"] == A).mean())\n",
    "    support_rows.append({\n",
    "        \"Group\": group_name,\n",
    "        \"N\": int(len(g)),\n",
    "        \"Obs P(A)\": round(pA_obs, 4),\n",
    "        \"π_ML P(A_hat)\": round(pA_ml, 4),\n",
    "        \"Expected\": (\"High\" if group_name.startswith(\"Low\") else \"Low\")\n",
    "    })\n",
    "\n",
    "df_support = pd.DataFrame(support_rows)\n",
    "print(\"\\n[Hypothèse Elysa] Synthèse groupée:\")\n",
    "print(df_support.to_string(index=False))\n",
    "df_support.to_csv(out_path(\"SECTION2_hypothesis_support_summary.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\nFichiers :\")\n",
    "print(\" -\", out_path(\"SECTION2_L3_with_action_labels.csv\"))\n",
    "print(\" -\", out_path(\"SECTION2_policyML_model_selection.csv\"))\n",
    "print(\" -\", out_path(\"SECTION2_policyML_predictions.csv\"))\n",
    "print(\" -\", out_path(\"SECTION2_hypothesis_obs_by_level.csv\"))\n",
    "print(\" -\", out_path(\"SECTION2_hypothesis_piML_by_level.csv\"))\n",
    "print(\" -\", out_path(\"SECTION1_hypothesis_support_summary.csv\"))\n",
    "if joblib_ok:\n",
    "    print(\" -\", out_path(\"SECTION2_best_policyML_model.joblib\"))\n",
    "    print(\" -\", out_path(\"SECTION2_best_policyML_model_meta.json\"))\n",
    "\n",
    "print(\"\\nXGBoost disponible ?\", xgb_ok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39808711",
   "metadata": {},
   "source": [
    "### Section 1 — Optimisation du temps réel (heures / jours) pour atteindre l’état absorbant (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580eacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] CSV L4 introuvables, reconstruction de T_A/T_B depuis df (L3).\n",
      "\n",
      "==================== PAGE 5 — Expected time-to-absorption (hours/days) ====================\n",
      "HOURS_PER_DAY = 8.0h (4 activités * 2h)\n",
      "\n",
      "--- Coûts c(s) en heures (par étape) ---\n",
      "Option FIXE (métier): {1: 8.0, 2: 8.0, 3: 8.0, 4: 8.0, 5: 0.0}\n",
      "Option CALIBRÉE (data): {1: 12.0, 2: 12.0, 3: 12.0, 4: 12.0, 5: 0.0}\n",
      "\n",
      "Heures observées (HoursTotal) par niveau initial (L3):\n",
      " Level   N  HoursMean  HoursMedian\n",
      "     1   4  55.000000         60.0\n",
      "     2  66  50.909091         60.0\n",
      "     3 166  51.445783         60.0\n",
      "     4 230  35.565217         20.0\n",
      "     5 347  47.377522         60.0\n",
      "\n",
      "[π_A (A partout) + coût FIXE] Temps attendu jusqu’à maîtrise (état 5) — par niveau initial\n",
      " StartLevel  ExpectedHours  ExpectedDays  CostHoursPerStep\n",
      "          1          29.88          3.73               8.0\n",
      "          2          27.76          3.47               8.0\n",
      "          3          19.34          2.42               8.0\n",
      "          4          19.53          2.44               8.0\n",
      "\n",
      "[π_A (A partout) + coût CALIBRÉ] Temps attendu jusqu’à maîtrise (état 5) — par niveau initial\n",
      " StartLevel  ExpectedHours  ExpectedDays  CostHoursPerStep\n",
      "          1          44.82          5.60              12.0\n",
      "          2          41.63          5.20              12.0\n",
      "          3          29.02          3.63              12.0\n",
      "          4          29.29          3.66              12.0\n",
      "\n",
      "--- Politique optimale π*_hours (Bellman en heures) ---\n",
      "π*_hours (state->A/B): {1: 'B', 2: 'B', 3: 'B', 4: 'B'}\n",
      "V*_hours (états 1..5): [34.68, 31.83, 22.962, 22.552, 0.0]\n",
      "\n",
      "[π*_hours (optimal temps) + coût CALIBRÉ] Temps attendu jusqu’à maîtrise (état 5) — par niveau initial\n",
      " StartLevel  ExpectedHours  ExpectedDays  CostHoursPerStep\n",
      "          1          34.68          4.34              12.0\n",
      "          2          31.83          3.98              12.0\n",
      "          3          22.96          2.87              12.0\n",
      "          4          22.55          2.82              12.0\n",
      "\n",
      "[INFO] df ne contient pas piML_P(A). Exécute Page 4 avant si tu veux π_ML_state.\n",
      "\n",
      "✅ Exports (Page 5):\n",
      " - ./out/202602/out_pomdp\\L5-expected_time_to_absorption_hours_days.json\n",
      " - ./out/202602/out_pomdp\\L5-expected_piA_fixed.csv\n",
      " - ./out/202602/out_pomdp\\L5-expected_piA_calib.csv\n",
      " - ./out/202602/out_pomdp\\L5-expected_pi_star_hours.csv\n",
      "\n",
      "==================== INTERPRÉTATION MÉTIER (à copier-coller) ====================\n",
      "1) ExpectedHours = temps moyen (en heures) attendu pour atteindre le niveau 5 (maîtrise),\n",
      "   en partant du niveau initial, sous une politique donnée (π_A, π*_hours, π_ML_state).\n",
      "2) ExpectedDays = ExpectedHours / 8h, car 4 activités/jour et 2h/activité => 8h/jour.\n",
      "3) π*_hours est la politique optimale trouvée par Bellman quand on minimise le TEMPS (heures) et non le nombre d’étapes.\n",
      "4) La version 'coût calibré' utilise HoursTotal observé pour approximer un coût par niveau (heures/étape).\n",
      "5) Si π*_hours réduit ExpectedHours vs π_A pour certains niveaux, alors l’action B (intensive) est utile sur ces niveaux.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Section 1 — Optimisation du temps réel (heures / jours) pour atteindre l’état absorbant (5)\n",
    "\n",
    "Objectifs :\n",
    "- Calculer le temps attendu (heures/jours) pour atteindre l’état 5\n",
    "  sous :\n",
    "  - π_A (A partout)\n",
    "  - π*_hours (Bellman en heures)\n",
    "  - π_ML_state (politique apprise par ML, agrégée par niveau)\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "EPS = 1e-12\n",
    "A, B = \"A\", \"B\"\n",
    "LEVELS = [1, 2, 3, 4, 5]\n",
    "TRANSIENT = [1, 2, 3, 4]\n",
    "ABSORBING = 5\n",
    "N_LEVELS = 5\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIG TEMPS (règles )\n",
    "# ------------------------------------------------------------\n",
    "HOURS_PER_ACTIVITY = 2.0\n",
    "ACTIVITIES_PER_DAY = 4.0\n",
    "HOURS_PER_DAY = HOURS_PER_ACTIVITY * ACTIVITIES_PER_DAY   # 8h/jour\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PATHS\n",
    "# ------------------------------------------------------------\n",
    "OUT_DIR = \"./out/202602/out_pomdp\"   # adapte si besoin\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def out_path(fname: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"L5-{fname}\")\n",
    "\n",
    "L3_PATH = \"./data/2025-08/L3.features_by_student.byPretest.csv\"  # adapte si besoin\n",
    "assert os.path.exists(L3_PATH), f\"Introuvable : {L3_PATH}\"\n",
    "\n",
    "# Si tu as déjà exporté T_A/T_B dans ton code principal, tu peux les charger ici\n",
    "TA_CSV = os.path.join(OUT_DIR, \"L4-AMC_TA_standard.csv\")   # from principal\n",
    "TB_CSV = os.path.join(OUT_DIR, \"L4-AMC_TB_intensive.csv\")  # from principal\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# UTILITAIRES\n",
    "# ------------------------------------------------------------\n",
    "def cap_level(x):\n",
    "    try:\n",
    "        v = int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return min(5, max(1, v))\n",
    "\n",
    "def canonical_QR(T: np.ndarray):\n",
    "    Q = T[:len(TRANSIENT), :len(TRANSIENT)]\n",
    "    R = T[:len(TRANSIENT), len(TRANSIENT):]\n",
    "    return Q, R\n",
    "\n",
    "def fundamental_matrix_N(Q: np.ndarray):\n",
    "    I = np.eye(Q.shape[0], dtype=float)\n",
    "    M = I - Q\n",
    "    try:\n",
    "        return np.linalg.inv(M)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.linalg.pinv(M)\n",
    "\n",
    "def expected_time_with_costs(T: np.ndarray, cost_vec_transient: np.ndarray):\n",
    "    \"\"\"\n",
    "    Expected cumulative cost until absorption:\n",
    "      m_cost = N * c\n",
    "    cost_vec_transient: c_i (heures) pour i=1..4\n",
    "    \"\"\"\n",
    "    Q, _ = canonical_QR(T)\n",
    "    N = fundamental_matrix_N(Q)\n",
    "    m = N.dot(cost_vec_transient.reshape(-1, 1)).flatten()\n",
    "    return {\"Q\": Q, \"N\": N, \"m\": m, \"J\": float(m.sum())}\n",
    "\n",
    "def build_T_pi(policy: dict, TA: np.ndarray, TB: np.ndarray) -> np.ndarray:\n",
    "    T = np.zeros((N_LEVELS, N_LEVELS), dtype=float)\n",
    "    for s in LEVELS:\n",
    "        if s == ABSORBING:\n",
    "            T[s-1, :] = 0.0\n",
    "            T[s-1, ABSORBING-1] = 1.0\n",
    "        else:\n",
    "            act = policy.get(s, A)\n",
    "            T[s-1, :] = TA[s-1, :] if act == A else TB[s-1, :]\n",
    "    return T\n",
    "\n",
    "def policy_all_A():\n",
    "    return {s: A for s in TRANSIENT}\n",
    "\n",
    "def bellman_value_iteration_time_cost(TA: np.ndarray, TB: np.ndarray, cost_hours: np.ndarray,\n",
    "                                      max_iter=20000, tol=1e-12):\n",
    "    \"\"\"\n",
    "    Bellman en HEURES :\n",
    "      V(5)=0\n",
    "      V(s)=c(s) + min_a sum_{s'} P(s'|s,a) V(s')\n",
    "    cost_hours: length 5, cost_hours[4]=0\n",
    "    \"\"\"\n",
    "    V = np.zeros(N_LEVELS, dtype=float)\n",
    "    V[:4] = float(np.max(cost_hours[:4])) * 5.0\n",
    "    pol = {s: A for s in TRANSIENT}\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        V_old = V.copy()\n",
    "        for s in TRANSIENT:\n",
    "            c = float(cost_hours[s-1])\n",
    "            qA = c + float(np.dot(TA[s-1, :], V_old))\n",
    "            qB = c + float(np.dot(TB[s-1, :], V_old))\n",
    "            if qB < qA:\n",
    "                V[s-1] = qB\n",
    "                pol[s] = B\n",
    "            else:\n",
    "                V[s-1] = qA\n",
    "                pol[s] = A\n",
    "        V[ABSORBING-1] = 0.0\n",
    "        if np.max(np.abs(V - V_old)) < tol:\n",
    "            break\n",
    "    return V, pol\n",
    "\n",
    "def policy_from_piML_predictions(df_with_piML: pd.DataFrame,\n",
    "                                level_col=\"Pretest_i\",\n",
    "                                probaA_col=\"piML_P(A)\",\n",
    "                                threshold=0.5):\n",
    "    \"\"\"\n",
    "    π_ML_state(s) = A si mean(P(A|x)) >= threshold sur les élèves du niveau s, sinon B.\n",
    "    \"\"\"\n",
    "    pol = {}\n",
    "    for s in TRANSIENT:\n",
    "        g = df_with_piML[df_with_piML[level_col] == s]\n",
    "        if len(g) == 0:\n",
    "            pol[s] = A\n",
    "            continue\n",
    "        pA = float(g[probaA_col].mean())\n",
    "        pol[s] = A if pA >= threshold else B\n",
    "    return pol\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (0) LOAD DF\n",
    "# ------------------------------------------------------------\n",
    "df = pd.read_csv(L3_PATH)\n",
    "\n",
    "if \"Pretest_i\" not in df.columns:\n",
    "    if \"Pretest\" in df.columns:\n",
    "        df[\"Pretest_i\"] = df[\"Pretest\"].apply(cap_level)\n",
    "    else:\n",
    "        raise ValueError(\"L3 doit contenir Pretest ou Pretest_i\")\n",
    "\n",
    "if \"HoursTotal\" not in df.columns:\n",
    "    raise ValueError(\"L3 doit contenir HoursTotal\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (1) DEFINIR / CHARGER T_A, T_B\n",
    "# ------------------------------------------------------------\n",
    "def empirical_transition_matrix_from_pretest_final(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimation empirique de P(Final=j | Pretest=i) (Action A — Standard).\n",
    "    \"\"\"\n",
    "    if \"Final_i\" not in df.columns:\n",
    "        if \"Final\" in df.columns:\n",
    "            df = df.copy()\n",
    "            df[\"Final_i\"] = df[\"Final\"].apply(cap_level)\n",
    "        else:\n",
    "            raise ValueError(\"L3 doit contenir Final ou Final_i pour estimer T_A empirique.\")\n",
    "\n",
    "    mat_counts = np.zeros((N_LEVELS, N_LEVELS), dtype=float)\n",
    "    dfx = df.dropna(subset=[\"Pretest_i\", \"Final_i\"]).copy()\n",
    "\n",
    "    for _, r in dfx.iterrows():\n",
    "        i = int(r[\"Pretest_i\"]) - 1\n",
    "        j = int(r[\"Final_i\"]) - 1\n",
    "        mat_counts[i, j] += 1.0\n",
    "\n",
    "    T = np.zeros_like(mat_counts)\n",
    "    for i in range(N_LEVELS):\n",
    "        s = mat_counts[i, :].sum()\n",
    "        if s > 0:\n",
    "            T[i, :] = mat_counts[i, :] / s\n",
    "        else:\n",
    "            T[i, :] = 1.0 / N_LEVELS\n",
    "\n",
    "    # force absorbing state 5\n",
    "    T[ABSORBING-1, :] = 0.0\n",
    "    T[ABSORBING-1, ABSORBING-1] = 1.0\n",
    "    return T\n",
    "\n",
    "def make_intensive_matrix_from_standard(TA: np.ndarray,\n",
    "                                       diag_shrink: float = 0.80,\n",
    "                                       regress_shrink: float = 0.70,\n",
    "                                       boost_to_absorb: float = 1.25) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Action B — Intensive : contre-factuel normatif (non causal)\n",
    "    \"\"\"\n",
    "    TB = TA.copy().astype(float)\n",
    "    for i in range(N_LEVELS):\n",
    "        if i == ABSORBING-1:\n",
    "            continue\n",
    "        for j in range(N_LEVELS):\n",
    "            if j == i:\n",
    "                TB[i, j] *= diag_shrink\n",
    "            elif j < i:\n",
    "                TB[i, j] *= regress_shrink\n",
    "        TB[i, ABSORBING-1] *= boost_to_absorb\n",
    "        s = TB[i, :].sum()\n",
    "        TB[i, :] = (TB[i, :] / s) if s > 0 else (1.0 / N_LEVELS)\n",
    "\n",
    "    TB[ABSORBING-1, :] = 0.0\n",
    "    TB[ABSORBING-1, ABSORBING-1] = 1.0\n",
    "    return TB\n",
    "\n",
    "def load_T_from_csv(path: str) -> np.ndarray:\n",
    "    T = pd.read_csv(path, index_col=0)\n",
    "    # assure shape 5x5\n",
    "    arr = T.values.astype(float)\n",
    "    if arr.shape != (5, 5):\n",
    "        raise ValueError(f\"Matrice {path} doit être 5x5, reçu {arr.shape}\")\n",
    "    return arr\n",
    "\n",
    "# Priorité: charger depuis CSV L4 si dispo, sinon reconstruire depuis df\n",
    "if os.path.exists(TA_CSV) and os.path.exists(TB_CSV):\n",
    "    T_A = load_T_from_csv(TA_CSV)\n",
    "    T_B = load_T_from_csv(TB_CSV)\n",
    "    print(\"\\n[INFO] T_A/T_B chargées depuis CSV L4:\", TA_CSV, TB_CSV)\n",
    "else:\n",
    "    print(\"\\n[INFO] CSV L4 introuvables, reconstruction de T_A/T_B depuis df (L3).\")\n",
    "    T_A = empirical_transition_matrix_from_pretest_final(df)\n",
    "    T_B = make_intensive_matrix_from_standard(T_A)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (2) CALIBRER COÛTS EN HEURES PAR ÉTAT (option 1 fixe, option 2 data)\n",
    "# ------------------------------------------------------------\n",
    "cost_hours_fixed = np.array([HOURS_PER_DAY, HOURS_PER_DAY, HOURS_PER_DAY, HOURS_PER_DAY, 0.0], dtype=float)\n",
    "\n",
    "def calibrate_hours_total_by_level(df: pd.DataFrame):\n",
    "    stats = (\n",
    "        df.dropna(subset=[\"Pretest_i\", \"HoursTotal\"])\n",
    "          .groupby(\"Pretest_i\")[\"HoursTotal\"]\n",
    "          .agg([\"count\", \"mean\", \"median\"])\n",
    "          .reset_index()\n",
    "          .rename(columns={\"Pretest_i\": \"Level\", \"count\": \"N\", \"mean\": \"HoursMean\", \"median\": \"HoursMedian\"})\n",
    "    )\n",
    "    return stats.sort_values(\"Level\")\n",
    "\n",
    "def derive_cost_hours_by_state_from_data(df: pd.DataFrame, T_under_policyA: np.ndarray):\n",
    "    \"\"\"\n",
    "    c_i ≈ mean(HoursTotal | Pretest=i) / t_i(π_A)  (heures par 'étape')\n",
    "    où t_i(π_A) vient de N=(I-Q)^-1 sous π_A, en unités “étapes”.\n",
    "    \"\"\"\n",
    "    Q, _ = canonical_QR(T_under_policyA)\n",
    "    N = fundamental_matrix_N(Q)\n",
    "    t_steps = N.sum(axis=1)  # length 4\n",
    "\n",
    "    stats = calibrate_hours_total_by_level(df)\n",
    "    c = np.array([HOURS_PER_DAY]*4, dtype=float)\n",
    "\n",
    "    for idx, s in enumerate(TRANSIENT):\n",
    "        row = stats[stats[\"Level\"] == s]\n",
    "        if len(row) == 0:\n",
    "            continue\n",
    "        mean_hours = float(row.iloc[0][\"HoursMean\"])\n",
    "        denom = float(t_steps[idx])\n",
    "        if denom > EPS and mean_hours > EPS:\n",
    "            c[idx] = mean_hours / denom\n",
    "\n",
    "    c = np.clip(c, 2.0, 12.0)  # garde-fou\n",
    "    return np.array([c[0], c[1], c[2], c[3], 0.0], dtype=float), stats, t_steps\n",
    "\n",
    "# Policy π_A\n",
    "piA = policy_all_A()\n",
    "T_piA = build_T_pi(piA, T_A, T_B)\n",
    "\n",
    "cost_hours_calib, stats_hours, t_steps_piA = derive_cost_hours_by_state_from_data(df, T_piA)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (3) CALCULS : π_A, π*_hours, π_ML_state\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n==================== Section 1 — Expected time-to-absorption (hours/days) ====================\")\n",
    "print(f\"HOURS_PER_DAY = {HOURS_PER_DAY:.1f}h (4 activités * 2h)\")\n",
    "print(\"\\n--- Coûts c(s) en heures (par étape) ---\")\n",
    "print(\"Option FIXE ():\", {s: float(cost_hours_fixed[s-1]) for s in LEVELS})\n",
    "print(\"Option CALIBRÉE (data):\", {s: float(cost_hours_calib[s-1]) for s in LEVELS})\n",
    "\n",
    "print(\"\\nHeures observées (HoursTotal) par niveau initial (L3):\")\n",
    "print(stats_hours.to_string(index=False))\n",
    "\n",
    "res_piA_fixed = expected_time_with_costs(T_piA, cost_hours_fixed[:4])\n",
    "res_piA_calib = expected_time_with_costs(T_piA, cost_hours_calib[:4])\n",
    "\n",
    "def _print_expected(res, label, cost_hours_vec):\n",
    "    print(f\"\\n[{label}] Temps attendu jusqu’à maîtrise (état 5) — par niveau initial\")\n",
    "    rows = []\n",
    "    for i, s in enumerate(TRANSIENT):\n",
    "        hours = float(res[\"m\"][i])\n",
    "        days = hours / HOURS_PER_DAY\n",
    "        rows.append({\n",
    "            \"StartLevel\": s,\n",
    "            \"ExpectedHours\": round(hours, 2),\n",
    "            \"ExpectedDays\": round(days, 2),\n",
    "            \"CostHoursPerStep\": round(float(cost_hours_vec[s-1]), 2)\n",
    "        })\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    print(df_out.to_string(index=False))\n",
    "    return df_out\n",
    "\n",
    "df_piA_fixed = _print_expected(res_piA_fixed, \"π_A (A partout) + coût FIXE\", cost_hours_fixed)\n",
    "df_piA_calib = _print_expected(res_piA_calib, \"π_A (A partout) + coût CALIBRÉ\", cost_hours_calib)\n",
    "\n",
    "# Bellman optimal en HEURES => π*_hours\n",
    "V_hours_star, pi_hours_star = bellman_value_iteration_time_cost(T_A, T_B, cost_hours_calib)\n",
    "T_pi_hours_star = build_T_pi(pi_hours_star, T_A, T_B)\n",
    "res_pi_star_hours = expected_time_with_costs(T_pi_hours_star, cost_hours_calib[:4])\n",
    "\n",
    "print(\"\\n--- Politique optimale π*_hours (Bellman en heures) ---\")\n",
    "print(\"π*_hours (state->A/B):\", pi_hours_star)\n",
    "print(\"V*_hours (états 1..5):\", [round(float(x), 3) for x in V_hours_star])\n",
    "\n",
    "df_pi_star_hours = _print_expected(res_pi_star_hours, \"π*_hours (optimal temps) + coût CALIBRÉ\", cost_hours_calib)\n",
    "\n",
    "# π_ML_state : agrégée par niveau si Section 1 a été exécutée et a produit piML_P(A)\n",
    "pi_ml_state, df_pi_ml = None, None\n",
    "if \"piML_P(A)\" in df.columns:\n",
    "    pi_ml_state = policy_from_piML_predictions(df, level_col=\"Pretest_i\", probaA_col=\"piML_P(A)\", threshold=0.5)\n",
    "    T_pi_ml_state = build_T_pi(pi_ml_state, T_A, T_B)\n",
    "    res_pi_ml = expected_time_with_costs(T_pi_ml_state, cost_hours_calib[:4])\n",
    "\n",
    "    print(\"\\n--- Politique π_ML_state (agrégée par niveau) ---\")\n",
    "    print(\"π_ML_state (state->A/B):\", pi_ml_state)\n",
    "    df_pi_ml = _print_expected(res_pi_ml, \"π_ML_state + coût CALIBRÉ\", cost_hours_calib)\n",
    "else:\n",
    "    print(\"\\n[INFO] df ne contient pas piML_P(A). Exécute Section 1 avant si tu veux π_ML_state.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (4) EXPORTS\n",
    "# ------------------------------------------------------------\n",
    "export = {\n",
    "    \"HOURS_PER_DAY\": HOURS_PER_DAY,\n",
    "    \"cost_hours_fixed\": cost_hours_fixed.tolist(),\n",
    "    \"cost_hours_calibrated\": cost_hours_calib.tolist(),\n",
    "    \"piA\": piA,\n",
    "    \"pi_hours_star\": pi_hours_star,\n",
    "    \"V_hours_star\": V_hours_star.tolist(),\n",
    "    \"pi_ml_state\": (pi_ml_state if pi_ml_state is not None else None),\n",
    "    \"expected_piA_fixed_hours\": df_piA_fixed.to_dict(orient=\"records\"),\n",
    "    \"expected_piA_calib_hours\": df_piA_calib.to_dict(orient=\"records\"),\n",
    "    \"expected_pi_star_hours\": df_pi_star_hours.to_dict(orient=\"records\"),\n",
    "    \"expected_pi_ml_hours\": (df_pi_ml.to_dict(orient=\"records\") if df_pi_ml is not None else None)\n",
    "}\n",
    "\n",
    "with open(out_path(\"expected_time_to_absorption_hours_days.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "pd.DataFrame(export[\"expected_piA_fixed_hours\"]).to_csv(out_path(\"expected_piA_fixed.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "pd.DataFrame(export[\"expected_piA_calib_hours\"]).to_csv(out_path(\"expected_piA_calib.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "pd.DataFrame(export[\"expected_pi_star_hours\"]).to_csv(out_path(\"expected_pi_star_hours.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "if export[\"expected_pi_ml_hours\"] is not None:\n",
    "    pd.DataFrame(export[\"expected_pi_ml_hours\"]).to_csv(out_path(\"expected_pi_ml_hours.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n Exports (Section 1):\")\n",
    "print(\" -\", out_path(\"expected_time_to_absorption_hours_days.json\"))\n",
    "print(\" -\", out_path(\"expected_piA_fixed.csv\"))\n",
    "print(\" -\", out_path(\"expected_piA_calib.csv\"))\n",
    "print(\" -\", out_path(\"expected_pi_star_hours.csv\"))\n",
    "if export[\"expected_pi_ml_hours\"] is not None:\n",
    "    print(\" -\", out_path(\"expected_pi_ml_hours.csv\"))\n",
    "\n",
    "print(\"\\n==================== INTERPRÉTATION  ====================\")\n",
    "print(\"1) ExpectedHours = temps moyen (en heures) attendu pour atteindre le niveau 5 (maîtrise),\")\n",
    "print(\"   en partant du niveau initial, sous une politique donnée (π_A, π*_hours, π_ML_state).\")\n",
    "print(\"2) ExpectedDays = ExpectedHours / 8h, car 4 activités/jour et 2h/activité => 8h/jour.\")\n",
    "print(\"3) π*_hours est la politique optimale trouvée par Bellman quand on minimise le TEMPS (heures) et non le nombre d’étapes.\")\n",
    "print(\"4) La version 'coût calibré' utilise HoursTotal observé pour approximer un coût par niveau (heures/étape).\")\n",
    "print(\"5) Si π*_hours réduit ExpectedHours vs π_A pour certains niveaux, alors l’action B (intensive) est utile sur ces niveaux.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfbfc7c",
   "metadata": {},
   "source": [
    "### Section 1 — Planification “jour par jour” avec durée VARIABLE (X jours) jusqu’à absorption (niveau 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152cd7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fallback activités/séquences construit depuis L1: ./data/2025-08/L1.20250818-DataMathsElysa.xlsx | sheet: Activités\n",
      "[INFO] activities_for_level(level) défini (fallback).\n",
      "[INFO] action_to_sequence(level, action) défini (fallback).\n",
      "\n",
      "==================== PAGE 6 — AUTO-STOP PLANNING ====================\n",
      "OUT_DIR = ./out/202602/out_pomdp\n",
      "START_LEVEL=1, TARGET_LEVEL=5, MAX_DAYS_CAP=60\n",
      "[INFO] L4 CSV introuvables -> reconstruction T_A/T_B depuis L3.\n",
      "[INFO] pi_hours_star chargée depuis Page 5 JSON.\n",
      "\n",
      "--- Politiques disponibles ---\n",
      "π*_hours: {1: 'B', 2: 'B', 3: 'B', 4: 'B'}\n",
      "π_ML_state: None\n",
      "\n",
      "Baseline MDP (fully observed) — POLICY=π*_hours (auto-stop) :\n",
      "\n",
      "Best sequences of activities selected for 60 days (auto-stop) (Start 1, Target 5) :  Baseline MDP (fully observed) |  byModel=PolicyTimeOptimal |  mode=most_likely\n",
      "Day 1: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 2: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 3: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 4: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 5: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 6: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 7: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 8: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 9: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 10: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 11: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 12: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 13: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 14: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 15: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 16: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 17: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 18: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 19: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 20: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 21: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 22: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 23: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 24: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 25: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 26: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 27: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 28: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 29: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 30: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 31: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 32: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 33: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 34: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 35: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 36: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 37: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 38: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 39: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 40: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 41: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 42: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 43: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 44: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 45: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 46: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 47: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 48: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 49: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 50: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 51: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 52: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 53: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 54: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 55: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 56: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 57: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 58: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 59: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "Day 60: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "\n",
      "[INFO] Reached absorption? False | final_state=1 | X_days=60 | approx_total_hours=480.0h\n",
      "\n",
      "Baseline MDP (fully observed) — POLICY=π*_hours (auto-stop, sample) :\n",
      "\n",
      "Best sequences of activities selected for 1 days (auto-stop) (Start 1, Target 5) :  Baseline MDP (fully observed) |  byModel=PolicyTimeOptimal |  mode=sample_seed42\n",
      "Day 1: [0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0]\n",
      "\n",
      "[INFO] (sample) Reached absorption? True | final_state=5 | X_days=1 | approx_total_hours=8.0h\n",
      "\n",
      "[INFO] pi_ml_state non disponible (exécute Page 4+5 pour l’obtenir).\n",
      "\n",
      "[INFO] POMDP non exécuté : variables manquantes (O_emit/T_hidden_A/T_hidden_B/pi_hidden_star).\n",
      "\n",
      "✅ Export Page 6: ./out/202602/out_pomdp\\L6-page6_autostop_summary.json\n",
      "\n",
      "==================== INTERPRÉTATION MÉTIER (à copier-coller) ====================\n",
      "1) Le plan 'auto-stop' s’arrête automatiquement quand le niveau 5 est atteint.\n",
      "2) X_days est donc le nombre de jours (séquences journalières) nécessaires selon la politique.\n",
      "3) Total_hours ≈ X_days * 8h (car 4 activités/jour * 2h/activité).\n",
      "4) 'most_likely' donne un chemin typique (argmax). 'sample' donne un scénario plausible (stochastique).\n",
      "5) Si π*_hours produit un X_days plus petit que π_A (standard), l’action B (intensive) apporte un gain de temps.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Section 1 — Planification “jour par jour” avec durée VARIABLE (X jours) jusqu’à absorption (niveau 5)\n",
    "\n",
    "But  :\n",
    "- Ne plus forcer \"10 jours\".\n",
    "- Déterminer automatiquement X (nombre de jours) nécessaires pour atteindre l’état absorbant 5,\n",
    "  sous une politique π (π*_hours optimisée en temps, ou π_ML_state).\n",
    "\n",
    "Sorties :\n",
    "- Plans MDP auto-stop (most_likely + sample)\n",
    "- Plan POMDP auto-stop si les variables HMM/POMDP existent déjà dans le namespace\n",
    "- Export JSON + CSV + TXT dans OUT_DIR\n",
    "\n",
    "Pré-requis  (doivent exister quelque part dans ton code/projet) :\n",
    "- activities_for_level(level)        -> retourne la liste des activités d’un niveau\n",
    "- action_to_sequence(level, action)  -> retourne {pos1..pos4} pour (niveau, action A/B)\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "EPS = 1e-12\n",
    "A, B = \"A\", \"B\"\n",
    "LEVELS = [1, 2, 3, 4, 5]\n",
    "TRANSIENT = [1, 2, 3, 4]\n",
    "ABSORBING = 5\n",
    "N_LEVELS = 5\n",
    "\n",
    "# Durée max de sécurité (évite boucle infinie si transitions ne mènent pas à 5)\n",
    "MAX_DAYS_CAP = 60\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIG TEMPS (règles )\n",
    "# ------------------------------------------------------------\n",
    "HOURS_PER_ACTIVITY = 2.0\n",
    "ACTIVITIES_PER_DAY = 4.0\n",
    "HOURS_PER_DAY = HOURS_PER_ACTIVITY * ACTIVITIES_PER_DAY  # 8h/jour\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PATHS ( demandé : utiliser ./out/202602/out_pomdp)\n",
    "# ------------------------------------------------------------\n",
    "OUT_DIR = \"./out/202602/out_pomdp\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def out_path(fname: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"L6-{fname}\")\n",
    "\n",
    "# Données\n",
    "L3_PATH = \"./data/2025-08/L3.features_by_student.byPretest.csv\"  # adapte si besoin\n",
    "assert os.path.exists(L3_PATH), f\"Introuvable : {L3_PATH}\"\n",
    "\n",
    "# Matrices L4 (si ton code principal les a déjà exportées)\n",
    "TA_CSV = os.path.join(OUT_DIR, \"L4-AMC_TA_standard.csv\")\n",
    "TB_CSV = os.path.join(OUT_DIR, \"L4-AMC_TB_intensive.csv\")\n",
    "\n",
    "# Résultats Section 1 (si disponibles)\n",
    "SECTION1_JSON = os.path.join(OUT_DIR, \"L5-expected_time_to_absorption_hours_days.json\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DEFAULTS si variables non définies dans ton notebook\n",
    "# ------------------------------------------------------------\n",
    "#  Correction de ton erreur : START_LEVEL absent\n",
    "START_LEVEL = int(globals().get(\"START_LEVEL\", 1))\n",
    "TARGET_LEVEL = int(globals().get(\"TARGET_LEVEL\", 5))\n",
    "\n",
    "BY_MODEL_NAME = globals().get(\"BY_MODEL_NAME\", \"PolicyTimeOptimal\")\n",
    "TECHNIQUE_MDP = globals().get(\"TECHNIQUE_MDP\", \"Baseline MDP (fully observed)\")\n",
    "TECHNIQUE_POMDP = globals().get(\"TECHNIQUE_POMDP\", \"POMDP approx (HMM belief + projection)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OUTILS GÉNÉRIQUES\n",
    "# ------------------------------------------------------------\n",
    "def cap_level(x):\n",
    "    try:\n",
    "        v = int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return min(5, max(1, v))\n",
    "\n",
    "def _safe_tag(s: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"-_.\" else \"_\" for ch in str(s))\n",
    "\n",
    "def load_T_from_csv(path: str) -> np.ndarray:\n",
    "    T = pd.read_csv(path, index_col=0)\n",
    "    arr = T.values.astype(float)\n",
    "    if arr.shape != (5, 5):\n",
    "        raise ValueError(f\"Matrice {path} doit être 5x5, reçu {arr.shape}\")\n",
    "    return arr\n",
    "\n",
    "def empirical_transition_matrix_from_pretest_final(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimation empirique de P(Final=j | Pretest=i) (Action A — Standard).\n",
    "    \"\"\"\n",
    "    if \"Final_i\" not in df.columns:\n",
    "        if \"Final\" in df.columns:\n",
    "            df = df.copy()\n",
    "            df[\"Final_i\"] = df[\"Final\"].apply(cap_level)\n",
    "        else:\n",
    "            raise ValueError(\"L3 doit contenir Final ou Final_i pour estimer T_A empirique.\")\n",
    "\n",
    "    mat_counts = np.zeros((N_LEVELS, N_LEVELS), dtype=float)\n",
    "    dfx = df.dropna(subset=[\"Pretest_i\", \"Final_i\"]).copy()\n",
    "\n",
    "    for _, r in dfx.iterrows():\n",
    "        i = int(r[\"Pretest_i\"]) - 1\n",
    "        j = int(r[\"Final_i\"]) - 1\n",
    "        mat_counts[i, j] += 1.0\n",
    "\n",
    "    T = np.zeros_like(mat_counts)\n",
    "    for i in range(N_LEVELS):\n",
    "        s = mat_counts[i, :].sum()\n",
    "        if s > 0:\n",
    "            T[i, :] = mat_counts[i, :] / s\n",
    "        else:\n",
    "            T[i, :] = 1.0 / N_LEVELS\n",
    "\n",
    "    T[ABSORBING-1, :] = 0.0\n",
    "    T[ABSORBING-1, ABSORBING-1] = 1.0\n",
    "    return T\n",
    "\n",
    "def make_intensive_matrix_from_standard(TA: np.ndarray,\n",
    "                                       diag_shrink: float = 0.80,\n",
    "                                       regress_shrink: float = 0.70,\n",
    "                                       boost_to_absorb: float = 1.25) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Action B — Intensive : contre-factuel normatif (non causal).\n",
    "    \"\"\"\n",
    "    TB = TA.copy().astype(float)\n",
    "    for i in range(N_LEVELS):\n",
    "        if i == ABSORBING-1:\n",
    "            continue\n",
    "        for j in range(N_LEVELS):\n",
    "            if j == i:\n",
    "                TB[i, j] *= diag_shrink\n",
    "            elif j < i:\n",
    "                TB[i, j] *= regress_shrink\n",
    "        TB[i, ABSORBING-1] *= boost_to_absorb\n",
    "        s = TB[i, :].sum()\n",
    "        TB[i, :] = (TB[i, :] / s) if s > 0 else (1.0 / N_LEVELS)\n",
    "\n",
    "    TB[ABSORBING-1, :] = 0.0\n",
    "    TB[ABSORBING-1, ABSORBING-1] = 1.0\n",
    "    return TB\n",
    "\n",
    "def bellman_value_iteration_time_cost(TA: np.ndarray, TB: np.ndarray, cost_hours: np.ndarray,\n",
    "                                      max_iter=20000, tol=1e-12):\n",
    "    \"\"\"\n",
    "    Bellman en heures :\n",
    "      V(5)=0\n",
    "      V(s)=c(s) + min_a sum_{s'} P(s'|s,a) V(s')\n",
    "    cost_hours: length 5, cost_hours[4]=0\n",
    "    \"\"\"\n",
    "    V = np.zeros(N_LEVELS, dtype=float)\n",
    "    V[:4] = float(np.max(cost_hours[:4])) * 5.0\n",
    "    pol = {s: A for s in TRANSIENT}\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        V_old = V.copy()\n",
    "        for s in TRANSIENT:\n",
    "            c = float(cost_hours[s-1])\n",
    "            qA = c + float(np.dot(TA[s-1, :], V_old))\n",
    "            qB = c + float(np.dot(TB[s-1, :], V_old))\n",
    "            if qB < qA:\n",
    "                V[s-1] = qB\n",
    "                pol[s] = B\n",
    "            else:\n",
    "                V[s-1] = qA\n",
    "                pol[s] = A\n",
    "        V[ABSORBING-1] = 0.0\n",
    "        if np.max(np.abs(V - V_old)) < tol:\n",
    "            break\n",
    "    return V, pol\n",
    "\n",
    "def choose_action(level: int, policy: dict) -> str:\n",
    "    return policy.get(int(level), A)\n",
    "\n",
    "def next_state_sample(level: int, action: str, TA: np.ndarray, TB: np.ndarray, rng: np.random.Generator) -> int:\n",
    "    row = TA[level-1, :] if action == A else TB[level-1, :]\n",
    "    row = np.clip(row, 0.0, 1.0)\n",
    "    s = float(row.sum())\n",
    "    row = (row / s) if s > EPS else (np.ones_like(row) / len(row))\n",
    "    return int(rng.choice(np.arange(1, N_LEVELS+1), p=row))\n",
    "\n",
    "def next_state_most_likely(level: int, action: str, TA: np.ndarray, TB: np.ndarray) -> int:\n",
    "    row = TA[level-1, :] if action == A else TB[level-1, :]\n",
    "    return int(np.argmax(row) + 1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FALLBACK (AUTO) — activities_for_level / action_to_sequence\n",
    "#   Priorité: L1 (séquences journalières pos1..pos4)\n",
    "# ------------------------------------------------------------\n",
    "L1_PATH = globals().get(\"L1_PATH\", \"./data/2025-08/L1.20250818-DataMathsElysa.xlsx\")\n",
    "\n",
    "def _detect_sequence_sheet(xlsx_path: str) -> str:\n",
    "    xl = pd.ExcelFile(xlsx_path)\n",
    "    for sh in xl.sheet_names:\n",
    "        tmp = pd.read_excel(xlsx_path, sheet_name=sh, nrows=3)\n",
    "        cols = [str(c).strip().lower() for c in tmp.columns]\n",
    "        if any(\"semaine\" in c for c in cols) and any(\"jour\" in c for c in cols):\n",
    "            return sh\n",
    "    return xl.sheet_names[0]\n",
    "\n",
    "def _build_seq_type_from_rows(df_seq: pd.DataFrame, activity_cols):\n",
    "    # pour chaque position 1..4: activité la plus fréquente\n",
    "    pos_buckets = {1: [], 2: [], 3: [], 4: []}\n",
    "    for _, r in df_seq.iterrows():\n",
    "        for col in activity_cols:\n",
    "            v = r.get(col, np.nan)\n",
    "            if pd.isna(v):\n",
    "                continue\n",
    "            try:\n",
    "                iv = int(v)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if 1 <= iv <= 4:\n",
    "                pos_buckets[iv].append(col)\n",
    "\n",
    "    seq = {}\n",
    "    for pos in [1, 2, 3, 4]:\n",
    "        if len(pos_buckets[pos]) == 0:\n",
    "            seq[f\"pos{pos}\"] = None\n",
    "        else:\n",
    "            seq[f\"pos{pos}\"] = pd.Series(pos_buckets[pos]).mode().iloc[0]\n",
    "    return seq\n",
    "\n",
    "LEVEL_SEQS = None\n",
    "\n",
    "if (\"activities_for_level\" not in globals()) or (\"action_to_sequence\" not in globals()):\n",
    "    if os.path.exists(L1_PATH):\n",
    "        try:\n",
    "            sh = _detect_sequence_sheet(L1_PATH)\n",
    "            d = pd.read_excel(L1_PATH, sheet_name=sh)\n",
    "\n",
    "            # colonnes meta\n",
    "            lower = {c: str(c).strip().lower() for c in d.columns}\n",
    "            meta_cols = [c for c, lc in lower.items() if (\"semaine\" in lc or \"jour\" in lc)]\n",
    "            activity_cols = [c for c in d.columns if c not in meta_cols]\n",
    "\n",
    "            # si une colonne \"Level/Niveau/Pretest\" existe, on fait par niveau, sinon global\n",
    "            level_col = None\n",
    "            for c in d.columns:\n",
    "                lc = str(c).strip().lower()\n",
    "                if lc in [\"level\", \"niveau\", \"pretest\", \"pretest_i\", \"niveau_pretest\"]:\n",
    "                    level_col = c\n",
    "                    break\n",
    "\n",
    "            LEVEL_SEQS = {}\n",
    "\n",
    "            if level_col is None:\n",
    "                seq_global = _build_seq_type_from_rows(d, activity_cols)\n",
    "                for lvl in LEVELS:\n",
    "                    LEVEL_SEQS[int(lvl)] = {\n",
    "                        \"activities\": list(activity_cols),\n",
    "                        \"seqA\": dict(seq_global),\n",
    "                        \"seqB\": dict(seq_global),  # A/B = vitesse, même séquence par défaut\n",
    "                    }\n",
    "            else:\n",
    "                for lvl, g in d.groupby(level_col):\n",
    "                    try:\n",
    "                        lv = int(lvl)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    lv = max(1, min(5, lv))\n",
    "                    seq_lv = _build_seq_type_from_rows(g, activity_cols)\n",
    "                    LEVEL_SEQS[lv] = {\n",
    "                        \"activities\": list(activity_cols),\n",
    "                        \"seqA\": dict(seq_lv),\n",
    "                        \"seqB\": dict(seq_lv),\n",
    "                    }\n",
    "                # compléter niveaux manquants avec global si besoin\n",
    "                if len(LEVEL_SEQS) < 5:\n",
    "                    seq_global = _build_seq_type_from_rows(d, activity_cols)\n",
    "                    for lvl in LEVELS:\n",
    "                        if lvl not in LEVEL_SEQS:\n",
    "                            LEVEL_SEQS[int(lvl)] = {\n",
    "                                \"activities\": list(activity_cols),\n",
    "                                \"seqA\": dict(seq_global),\n",
    "                                \"seqB\": dict(seq_global),\n",
    "                            }\n",
    "\n",
    "            print(\"[INFO] Fallback activités/séquences construit depuis L1:\", L1_PATH, \"| sheet:\", sh)\n",
    "\n",
    "        except Exception as e:\n",
    "            LEVEL_SEQS = None\n",
    "            print(\"[WARN] Fallback L1 impossible:\", str(e))\n",
    "    else:\n",
    "        print(\"[WARN] L1 introuvable -> fallback minimal (pas de vraie séquence).\")\n",
    "\n",
    "    # définir les fonctions si elles n'existent pas\n",
    "    if \"activities_for_level\" not in globals():\n",
    "        def activities_for_level(level: int):\n",
    "            if LEVEL_SEQS is None:\n",
    "                raise NameError(\"activities_for_level(level) indisponible: L1 non exploitable.\")\n",
    "            return list(LEVEL_SEQS[int(level)][\"activities\"])\n",
    "        print(\"[INFO] activities_for_level(level) défini (fallback).\")\n",
    "\n",
    "    if \"action_to_sequence\" not in globals():\n",
    "        def action_to_sequence(level: int, action: str):\n",
    "            if LEVEL_SEQS is None:\n",
    "                raise NameError(\"action_to_sequence(level, action) indisponible: L1 non exploitable.\")\n",
    "            pack = LEVEL_SEQS[int(level)]\n",
    "            if str(action).upper() == \"B\":\n",
    "                return dict(pack[\"seqB\"])\n",
    "            return dict(pack[\"seqA\"])\n",
    "        print(\"[INFO] action_to_sequence(level, action) défini (fallback).\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (1) PLAN MDP fully observed — durée variable\n",
    "# ------------------------------------------------------------\n",
    "def build_plan_mdp_auto_stop(start_level: int,\n",
    "                            policy: dict,\n",
    "                            TA: np.ndarray, TB: np.ndarray,\n",
    "                            mode: str = \"most_likely\",   # \"most_likely\" or \"sample\"\n",
    "                            seed: int = 42,\n",
    "                            stop_at: int = 5,\n",
    "                            max_days: int = MAX_DAYS_CAP):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    current = int(start_level)\n",
    "    plan = []\n",
    "\n",
    "    for day in range(1, max_days+1):\n",
    "        if current >= stop_at:\n",
    "            break\n",
    "\n",
    "        action = choose_action(current, policy)\n",
    "\n",
    "        # ---- dépendances  (doivent exister) ----\n",
    "        if \"action_to_sequence\" not in globals():\n",
    "            raise NameError(\"action_to_sequence(level, action) n'existe pas dans le namespace. \"\n",
    "                            \"Exécute d'abord ton code principal (partie activités).\")\n",
    "        seq = action_to_sequence(current, action)\n",
    "\n",
    "        plan.append({\n",
    "            \"Day\": day,\n",
    "            \"Level\": current,\n",
    "            \"Action\": action,\n",
    "            \"pos1\": seq.get(\"pos1\"),\n",
    "            \"pos2\": seq.get(\"pos2\"),\n",
    "            \"pos3\": seq.get(\"pos3\"),\n",
    "            \"pos4\": seq.get(\"pos4\"),\n",
    "        })\n",
    "\n",
    "        if mode == \"sample\":\n",
    "            current = next_state_sample(current, action, TA, TB, rng)\n",
    "        else:\n",
    "            current = next_state_most_likely(current, action, TA, TB)\n",
    "\n",
    "    reached = (current >= stop_at)\n",
    "    return plan, reached, current\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (2) PRINT + SAVE\n",
    "# ------------------------------------------------------------\n",
    "def print_and_save_plan_variable_days(plan_rows,\n",
    "                                      technique: str,\n",
    "                                      by_model: str,\n",
    "                                      start_level: int,\n",
    "                                      target_level: int,\n",
    "                                      filename_prefix: str,\n",
    "                                      mode_label: str):\n",
    "    X = len(plan_rows)\n",
    "    title = (f\"Best sequences of activities selected for {X} days (auto-stop) \"\n",
    "             f\"(Start {start_level}, Target {target_level}) :  {technique} |  byModel={by_model} |  mode={mode_label}\")\n",
    "    print(title)\n",
    "\n",
    "    if \"activities_for_level\" not in globals():\n",
    "        raise NameError(\"activities_for_level(level) n'existe pas dans le namespace. \"\n",
    "                        \"Exécute d'abord ton code principal (partie activités).\")\n",
    "\n",
    "    base_acts = activities_for_level(start_level)\n",
    "\n",
    "    lines = [title]\n",
    "    out_rows = []\n",
    "\n",
    "    for i, r in enumerate(plan_rows, start=1):\n",
    "        seq = {\"pos1\": r.get(\"pos1\"), \"pos2\": r.get(\"pos2\"), \"pos3\": r.get(\"pos3\"), \"pos4\": r.get(\"pos4\")}\n",
    "        v = [0]*len(base_acts)\n",
    "        for pos in [1,2,3,4]:\n",
    "            a = seq.get(f\"pos{pos}\")\n",
    "            if a in base_acts:\n",
    "                v[base_acts.index(a)] = pos\n",
    "\n",
    "        print(f\"Day {i}: {v}\")\n",
    "        lines.append(f\"Day {i}: {v}\")\n",
    "\n",
    "        out_rows.append({\n",
    "            \"Day\": i,\n",
    "            \"StartLevel\": start_level,\n",
    "            \"TargetLevel\": target_level,\n",
    "            \"Technique\": technique,\n",
    "            \"ByModel\": by_model,\n",
    "            \"Mode\": mode_label,\n",
    "            \"LevelUsedForDecision\": r.get(\"Level\", \"\"),\n",
    "            \"Action\": r.get(\"Action\", \"\"),\n",
    "            \"pos1\": seq.get(\"pos1\",\"\"),\n",
    "            \"pos2\": seq.get(\"pos2\",\"\"),\n",
    "            \"pos3\": seq.get(\"pos3\",\"\"),\n",
    "            \"pos4\": seq.get(\"pos4\",\"\"),\n",
    "            \"HoursPerDay\": HOURS_PER_DAY,\n",
    "            \"CumHoursIfCompleted\": round(i * HOURS_PER_DAY, 2),\n",
    "        })\n",
    "\n",
    "    safe_model = _safe_tag(by_model)\n",
    "    out_txt = out_path(f\"{filename_prefix}_autoStop_Start{start_level}_Target{target_level}__byModel-{safe_model}__{mode_label}.txt\")\n",
    "    out_csv = out_path(f\"{filename_prefix}_autoStop_Start{start_level}_Target{target_level}__byModel-{safe_model}__{mode_label}.csv\")\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "        f.write(\"\\n(0 = non sélectionnée ce jour ; 1..4 = position de l’activité)\\n\")\n",
    "\n",
    "    pd.DataFrame(out_rows).to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    return out_txt, out_csv, X\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (3) POMDP auto-stop (si variables présentes)\n",
    "# ------------------------------------------------------------\n",
    "def normalize(p: np.ndarray) -> np.ndarray:\n",
    "    s = float(p.sum())\n",
    "    if s <= 0:\n",
    "        return np.ones_like(p) / len(p)\n",
    "    return p / s\n",
    "\n",
    "def belief_init_from_observation(obs_level: int, O_emit: np.ndarray, prior=None) -> np.ndarray:\n",
    "    if prior is None:\n",
    "        prior = np.ones(O_emit.shape[1], dtype=float) / O_emit.shape[1]\n",
    "    b = prior * O_emit[obs_level-1, :]\n",
    "    return normalize(b)\n",
    "\n",
    "def belief_predict(b: np.ndarray, Th: np.ndarray) -> np.ndarray:\n",
    "    return normalize(b.dot(Th))\n",
    "\n",
    "def belief_update(b_pred: np.ndarray, obs_level: int, O_emit: np.ndarray) -> np.ndarray:\n",
    "    b_new = b_pred * O_emit[obs_level-1, :]\n",
    "    return normalize(b_new)\n",
    "\n",
    "def most_likely_observation_from_belief(b: np.ndarray, O_emit: np.ndarray) -> int:\n",
    "    p_obs = O_emit.dot(b)\n",
    "    return int(np.argmax(p_obs) + 1)\n",
    "\n",
    "def build_plan_pomdp_auto_stop(start_obs_level: int,\n",
    "                               O_emit: np.ndarray,\n",
    "                               T_hidden_A: np.ndarray,\n",
    "                               T_hidden_B: np.ndarray,\n",
    "                               pi_hidden_star: dict,   # keys: 0/1\n",
    "                               hidden_names=(\"Faible\",\"Maitrise\"),\n",
    "                               mode_obs: str = \"most_likely\",\n",
    "                               seed: int = 42,\n",
    "                               max_days: int = MAX_DAYS_CAP):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    H0, H1 = 0, 1\n",
    "\n",
    "    b = belief_init_from_observation(int(start_obs_level), O_emit)\n",
    "    plan = []\n",
    "    obs_level = int(start_obs_level)\n",
    "\n",
    "    for day in range(1, max_days+1):\n",
    "        z_hat = int(np.argmax(b))\n",
    "        if z_hat == H1:\n",
    "            break\n",
    "\n",
    "        action = pi_hidden_star.get(z_hat, A)\n",
    "\n",
    "        if \"action_to_sequence\" not in globals():\n",
    "            raise NameError(\"action_to_sequence(level, action) n'existe pas (requis pour POMDP aussi).\")\n",
    "        seq = action_to_sequence(obs_level, action)\n",
    "\n",
    "        plan.append({\n",
    "            \"Day\": day,\n",
    "            \"ObsLevel\": obs_level,\n",
    "            \"z_hat\": hidden_names[z_hat],\n",
    "            \"belief_Faible\": float(b[H0]),\n",
    "            \"belief_Maitrise\": float(b[H1]),\n",
    "            \"Action\": action,\n",
    "            \"pos1\": seq.get(\"pos1\"),\n",
    "            \"pos2\": seq.get(\"pos2\"),\n",
    "            \"pos3\": seq.get(\"pos3\"),\n",
    "            \"pos4\": seq.get(\"pos4\"),\n",
    "        })\n",
    "\n",
    "        Th = T_hidden_A if action == A else T_hidden_B\n",
    "        b_pred = belief_predict(b, Th)\n",
    "\n",
    "        if mode_obs == \"sample\":\n",
    "            p_obs = O_emit.dot(b_pred)\n",
    "            p_obs = p_obs / max(EPS, float(p_obs.sum()))\n",
    "            obs_level = int(rng.choice(np.arange(1, N_LEVELS+1), p=p_obs))\n",
    "        else:\n",
    "            obs_level = most_likely_observation_from_belief(b_pred, O_emit)\n",
    "\n",
    "        b = belief_update(b_pred, obs_level, O_emit)\n",
    "\n",
    "    reached = (int(np.argmax(b)) == H1)\n",
    "    return plan, reached\n",
    "\n",
    "def print_and_save_plan_pomdp_variable(plan_rows,\n",
    "                                       technique: str,\n",
    "                                       by_model: str,\n",
    "                                       start_obs_level: int,\n",
    "                                       target_level: int,\n",
    "                                       filename_prefix: str,\n",
    "                                       mode_label: str):\n",
    "    X = len(plan_rows)\n",
    "    title = (f\"Best sequences of activities selected for {X} days (auto-stop) \"\n",
    "             f\"(Start {start_obs_level}, Target {target_level}) :  {technique} |  byModel={by_model} |  mode={mode_label}\")\n",
    "    print(title)\n",
    "\n",
    "    if \"activities_for_level\" not in globals():\n",
    "        raise NameError(\"activities_for_level(level) n'existe pas (requis pour l'affichage vecteur).\")\n",
    "\n",
    "    base_acts = activities_for_level(start_obs_level)\n",
    "\n",
    "    lines = [title]\n",
    "    out_rows = []\n",
    "\n",
    "    for i, r in enumerate(plan_rows, start=1):\n",
    "        seq = {\"pos1\": r.get(\"pos1\"), \"pos2\": r.get(\"pos2\"), \"pos3\": r.get(\"pos3\"), \"pos4\": r.get(\"pos4\")}\n",
    "        v = [0]*len(base_acts)\n",
    "        for pos in [1,2,3,4]:\n",
    "            a = seq.get(f\"pos{pos}\")\n",
    "            if a in base_acts:\n",
    "                v[base_acts.index(a)] = pos\n",
    "\n",
    "        print(f\"Day {i}: {v}\")\n",
    "        lines.append(f\"Day {i}: {v}\")\n",
    "\n",
    "        out_rows.append({\n",
    "            \"Day\": i,\n",
    "            \"StartObsLevel\": start_obs_level,\n",
    "            \"TargetLevel\": target_level,\n",
    "            \"Technique\": technique,\n",
    "            \"ByModel\": by_model,\n",
    "            \"Mode\": mode_label,\n",
    "            \"ObsLevel\": r.get(\"ObsLevel\",\"\"),\n",
    "            \"z_hat\": r.get(\"z_hat\",\"\"),\n",
    "            \"belief_Faible\": r.get(\"belief_Faible\",\"\"),\n",
    "            \"belief_Maitrise\": r.get(\"belief_Maitrise\",\"\"),\n",
    "            \"Action\": r.get(\"Action\",\"\"),\n",
    "            \"pos1\": seq.get(\"pos1\",\"\"),\n",
    "            \"pos2\": seq.get(\"pos2\",\"\"),\n",
    "            \"pos3\": seq.get(\"pos3\",\"\"),\n",
    "            \"pos4\": seq.get(\"pos4\",\"\"),\n",
    "            \"HoursPerDay\": HOURS_PER_DAY,\n",
    "            \"CumHoursIfCompleted\": round(i * HOURS_PER_DAY, 2),\n",
    "        })\n",
    "\n",
    "    safe_model = _safe_tag(by_model)\n",
    "    out_txt = out_path(f\"{filename_prefix}_autoStop_Start{start_obs_level}_Target{target_level}__byModel-{safe_model}__{mode_label}.txt\")\n",
    "    out_csv = out_path(f\"{filename_prefix}_autoStop_Start{start_obs_level}_Target{target_level}__byModel-{safe_model}__{mode_label}.csv\")\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "        f.write(\"\\n(0 = non sélectionnée ce jour ; 1..4 = position de l’activité)\\n\")\n",
    "\n",
    "    pd.DataFrame(out_rows).to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    return out_txt, out_csv, X\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (4) CHARGEMENTS / POLITIQUES (π*_hours / π_ML_state)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n==================== Section 1 — AUTO-STOP PLANNING ====================\")\n",
    "print(f\"OUT_DIR = {OUT_DIR}\")\n",
    "print(f\"START_LEVEL={START_LEVEL}, TARGET_LEVEL={TARGET_LEVEL}, MAX_DAYS_CAP={MAX_DAYS_CAP}\")\n",
    "\n",
    "df = pd.read_csv(L3_PATH)\n",
    "if \"Pretest_i\" not in df.columns and \"Pretest\" in df.columns:\n",
    "    df[\"Pretest_i\"] = df[\"Pretest\"].apply(cap_level)\n",
    "\n",
    "# T_A/T_B\n",
    "if os.path.exists(TA_CSV) and os.path.exists(TB_CSV):\n",
    "    T_A = load_T_from_csv(TA_CSV)\n",
    "    T_B = load_T_from_csv(TB_CSV)\n",
    "    print(\"[INFO] T_A/T_B chargées depuis L4 CSV.\")\n",
    "else:\n",
    "    print(\"[INFO] L4 CSV introuvables -> reconstruction T_A/T_B depuis L3.\")\n",
    "    T_A = empirical_transition_matrix_from_pretest_final(df)\n",
    "    T_B = make_intensive_matrix_from_standard(T_A)\n",
    "\n",
    "# Politique optimisée temps: priorité à SECTION2 JSON si dispo\n",
    "pi_hours_star = None\n",
    "if os.path.exists(SECTION2_JSON):\n",
    "    try:\n",
    "        with open(SECTION2_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "            SECTION2 = json.load(f)\n",
    "        if isinstance(SECTION2.get(\"pi_hours_star\", None), dict):\n",
    "            # clés potentiellement string -> cast int\n",
    "            pi_hours_star = {int(k): v for k, v in SECTION2[\"pi_hours_star\"].items()}\n",
    "            print(\"[INFO] pi_hours_star chargée depuis Section 1 JSON.\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Lecture SECTION2 JSON impossible:\", str(e))\n",
    "\n",
    "# Si pas trouvé, calcule Bellman temps avec coût simple 8h/étape\n",
    "if pi_hours_star is None:\n",
    "    print(\"[INFO] pi_hours_star non trouvée -> calcul Bellman temps avec coût 8h/étape (fallback).\")\n",
    "    cost_hours = np.array([HOURS_PER_DAY]*4 + [0.0], dtype=float)\n",
    "    _, pi_hours_star = bellman_value_iteration_time_cost(T_A, T_B, cost_hours)\n",
    "\n",
    "# π_ML_state si présent dans SECTION2 JSON\n",
    "pi_ml_state = None\n",
    "if os.path.exists(SECTION2_JSON):\n",
    "    try:\n",
    "        if SECTION2.get(\"pi_ml_state\", None) is not None:\n",
    "            pi_ml_state = {int(k): v for k, v in SECTION2[\"pi_ml_state\"].items()}\n",
    "            print(\"[INFO] pi_ml_state chargée depuis Section 1 JSON.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"\\n--- Politiques disponibles ---\")\n",
    "print(\"π*_hours:\", pi_hours_star)\n",
    "print(\"π_ML_state:\", pi_ml_state)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (5) RUN MDP auto-stop\n",
    "# ------------------------------------------------------------\n",
    "policy_opt = pi_hours_star\n",
    "\n",
    "# most_likely\n",
    "plan_opt_ml, reached_opt, final_state_opt = build_plan_mdp_auto_stop(\n",
    "    start_level=START_LEVEL,\n",
    "    policy=policy_opt,\n",
    "    TA=T_A, TB=T_B,\n",
    "    mode=\"most_likely\",\n",
    "    seed=42,\n",
    "    stop_at=ABSORBING,\n",
    "    max_days=MAX_DAYS_CAP\n",
    ")\n",
    "\n",
    "print(\"\\nBaseline MDP (fully observed) — POLICY=π*_hours (auto-stop) :\\n\")\n",
    "opt_txt, opt_csv, opt_X = print_and_save_plan_variable_days(\n",
    "    plan_opt_ml,\n",
    "    technique=TECHNIQUE_MDP,\n",
    "    by_model=BY_MODEL_NAME,\n",
    "    start_level=START_LEVEL,\n",
    "    target_level=ABSORBING,\n",
    "    filename_prefix=\"best_sequences_MDP_OPT\",\n",
    "    mode_label=\"most_likely\"\n",
    ")\n",
    "\n",
    "print(f\"\\n[INFO] Reached absorption? {reached_opt} | final_state={final_state_opt} | \"\n",
    "      f\"X_days={opt_X} | approx_total_hours={opt_X*HOURS_PER_DAY:.1f}h\")\n",
    "\n",
    "# sample\n",
    "plan_opt_sample, reached_opt_s, final_state_opt_s = build_plan_mdp_auto_stop(\n",
    "    start_level=START_LEVEL,\n",
    "    policy=policy_opt,\n",
    "    TA=T_A, TB=T_B,\n",
    "    mode=\"sample\",\n",
    "    seed=42,\n",
    "    stop_at=ABSORBING,\n",
    "    max_days=MAX_DAYS_CAP\n",
    ")\n",
    "\n",
    "print(\"\\nBaseline MDP (fully observed) — POLICY=π*_hours (auto-stop, sample) :\\n\")\n",
    "optS_txt, optS_csv, optS_X = print_and_save_plan_variable_days(\n",
    "    plan_opt_sample,\n",
    "    technique=TECHNIQUE_MDP,\n",
    "    by_model=BY_MODEL_NAME,\n",
    "    start_level=START_LEVEL,\n",
    "    target_level=ABSORBING,\n",
    "    filename_prefix=\"best_sequences_MDP_OPT\",\n",
    "    mode_label=\"sample_seed42\"\n",
    ")\n",
    "\n",
    "print(f\"\\n[INFO] (sample) Reached absorption? {reached_opt_s} | final_state={final_state_opt_s} | \"\n",
    "      f\"X_days={optS_X} | approx_total_hours={optS_X*HOURS_PER_DAY:.1f}h\")\n",
    "\n",
    "# MDP sous π_ML_state (si dispo)\n",
    "ml_txt = ml_csv = None\n",
    "ml_X = None\n",
    "reached_ml = None\n",
    "final_state_ml = None\n",
    "\n",
    "if isinstance(pi_ml_state, dict):\n",
    "    plan_ml, reached_ml, final_state_ml = build_plan_mdp_auto_stop(\n",
    "        start_level=START_LEVEL,\n",
    "        policy=pi_ml_state,\n",
    "        TA=T_A, TB=T_B,\n",
    "        mode=\"most_likely\",\n",
    "        seed=42,\n",
    "        stop_at=ABSORBING,\n",
    "        max_days=MAX_DAYS_CAP\n",
    "    )\n",
    "\n",
    "    print(\"\\nBaseline MDP (fully observed) — POLICY=π_ML_state (auto-stop) :\\n\")\n",
    "    ml_txt, ml_csv, ml_X = print_and_save_plan_variable_days(\n",
    "        plan_ml,\n",
    "        technique=TECHNIQUE_MDP,\n",
    "        by_model=BY_MODEL_NAME,\n",
    "        start_level=START_LEVEL,\n",
    "        target_level=ABSORBING,\n",
    "        filename_prefix=\"best_sequences_MDP_piML\",\n",
    "        mode_label=\"most_likely\"\n",
    "    )\n",
    "    print(f\"\\n[INFO] π_ML_state Reached absorption? {reached_ml} | final_state={final_state_ml} | \"\n",
    "          f\"X_days={ml_X} | approx_total_hours={ml_X*HOURS_PER_DAY:.1f}h\")\n",
    "else:\n",
    "    print(\"\\n[INFO] pi_ml_state non disponible (exécute Section 1+5 pour l’obtenir).\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (6) RUN POMDP auto-stop\n",
    "# ------------------------------------------------------------\n",
    "p_txt = p_csv = None\n",
    "p_X = None\n",
    "reached_p = None\n",
    "\n",
    "if all(k in globals() for k in [\"O_emit\", \"T_hidden_A\", \"T_hidden_B\", \"pi_hidden_star\"]):\n",
    "    # pi_hidden_star peut être dict int->A/B, sinon on tente de le convertir\n",
    "    if isinstance(pi_hidden_star, dict) and len(pi_hidden_star) > 0:\n",
    "        any_key = list(pi_hidden_star.keys())[0]\n",
    "        if not isinstance(any_key, int):\n",
    "            # cas où clés sont \"Faible\"/\"Maitrise\"\n",
    "            pi_hidden_star_int = {0: pi_hidden_star.get(\"Faible\", A), 1: pi_hidden_star.get(\"Maitrise\", A)}\n",
    "        else:\n",
    "            pi_hidden_star_int = pi_hidden_star\n",
    "    else:\n",
    "        pi_hidden_star_int = {0: A, 1: A}\n",
    "\n",
    "    plan_pomdp_ml, reached_p = build_plan_pomdp_auto_stop(\n",
    "        start_obs_level=START_LEVEL,\n",
    "        O_emit=O_emit,\n",
    "        T_hidden_A=T_hidden_A,\n",
    "        T_hidden_B=T_hidden_B,\n",
    "        pi_hidden_star=pi_hidden_star_int,\n",
    "        hidden_names=(\"Faible\", \"Maitrise\"),\n",
    "        mode_obs=\"most_likely\",\n",
    "        seed=42,\n",
    "        max_days=MAX_DAYS_CAP\n",
    "    )\n",
    "\n",
    "    print(\"\\nPOMDP approx (HMM belief + projection) — POLICY=π_hidden* (auto-stop) :\\n\")\n",
    "    p_txt, p_csv, p_X = print_and_save_plan_pomdp_variable(\n",
    "        plan_pomdp_ml,\n",
    "        technique=TECHNIQUE_POMDP,\n",
    "        by_model=BY_MODEL_NAME,\n",
    "        start_obs_level=START_LEVEL,\n",
    "        target_level=ABSORBING,\n",
    "        filename_prefix=\"best_sequences_POMDP_OPT\",\n",
    "        mode_label=\"most_likely\"\n",
    "    )\n",
    "    print(f\"\\n[INFO] POMDP Reached latent mastery? {reached_p} | X_days={p_X} | approx_total_hours={p_X*HOURS_PER_DAY:.1f}h\")\n",
    "else:\n",
    "    print(\"\\n[INFO] POMDP non exécuté : variables manquantes (O_emit/T_hidden_A/T_hidden_B/pi_hidden_star).\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (7) Export résumé\n",
    "# ------------------------------------------------------------\n",
    "summary6 = {\n",
    "    \"OUT_DIR\": OUT_DIR,\n",
    "    \"MAX_DAYS_CAP\": MAX_DAYS_CAP,\n",
    "    \"HOURS_PER_DAY\": HOURS_PER_DAY,\n",
    "    \"START_LEVEL\": START_LEVEL,\n",
    "    \"TARGET_LEVEL\": TARGET_LEVEL,\n",
    "    \"pi_hours_star\": pi_hours_star,\n",
    "    \"pi_ml_state\": (pi_ml_state if isinstance(pi_ml_state, dict) else None),\n",
    "\n",
    "    \"MDP_OPT_most_likely\": {\n",
    "        \"X_days\": opt_X, \"total_hours\": float(opt_X * HOURS_PER_DAY),\n",
    "        \"reached\": bool(reached_opt), \"final_state\": int(final_state_opt),\n",
    "        \"txt\": opt_txt, \"csv\": opt_csv\n",
    "    },\n",
    "    \"MDP_OPT_sample_seed42\": {\n",
    "        \"X_days\": optS_X, \"total_hours\": float(optS_X * HOURS_PER_DAY),\n",
    "        \"reached\": bool(reached_opt_s), \"final_state\": int(final_state_opt_s),\n",
    "        \"txt\": optS_txt, \"csv\": optS_csv\n",
    "    },\n",
    "    \"MDP_piML_most_likely\": (\n",
    "        {\n",
    "            \"X_days\": ml_X, \"total_hours\": float(ml_X * HOURS_PER_DAY),\n",
    "            \"reached\": bool(reached_ml), \"final_state\": int(final_state_ml),\n",
    "            \"txt\": ml_txt, \"csv\": ml_csv\n",
    "        } if ml_X is not None else None\n",
    "    ),\n",
    "    \"POMDP_OPT_most_likely\": (\n",
    "        {\n",
    "            \"X_days\": p_X, \"total_hours\": float(p_X * HOURS_PER_DAY),\n",
    "            \"reached\": bool(reached_p),\n",
    "            \"txt\": p_txt, \"csv\": p_csv\n",
    "        } if p_X is not None else None\n",
    "    )\n",
    "}\n",
    "\n",
    "with open(out_path(\"SECTION2_autostop_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary6, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n Export Section 1:\", out_path(\"SECTION1_autostop_summary.json\"))\n",
    "\n",
    "print(\"\\n==================== INTERPRÉTATION  ====================\")\n",
    "print(\"1) Le plan 'auto-stop' s’arrête automatiquement quand le niveau 5 est atteint.\")\n",
    "print(\"2) X_days est donc le nombre de jours (séquences journalières) nécessaires selon la politique.\")\n",
    "print(\"3) Total_hours ≈ X_days * 8h (car 4 activités/jour * 2h/activité).\")\n",
    "print(\"4) 'most_likely' donne un chemin typique (argmax). 'sample' donne un scénario plausible (stochastique).\")\n",
    "print(\"5) Si π*_hours produit un X_days plus petit que π_A (standard), l’action B (intensive) apporte un gain de temps.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96624574",
   "metadata": {},
   "source": [
    "### Section 1 — Analyse & validation de l’hypothèse (A vs B selon le niveau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39055018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== PAGE 7 — Hypothesis (Bellman/Q in hours) ====================\n",
      "[INFO] Value used: V_hours_star\n",
      " Level BestAction_by_Qhours  Q_A_hours  Q_B_hours  Delta_B_minus_A_hours\n",
      "     1                    B     33.298     30.680                 -2.618\n",
      "     2                    B     30.754     27.830                 -2.924\n",
      "     3                    B     21.363     18.962                 -2.402\n",
      "     4                    B     21.530     18.552                 -2.978\n",
      "\n",
      "Lecture métier rapide :\n",
      "- Delta_B_minus_A_hours < 0  => B réduit l’espérance de temps (meilleur).\n",
      "- Delta_B_minus_A_hours > 0  => A est meilleur.\n",
      "=> Compare niveaux 1–2 vs 3–4 pour confirmer/infirmer l’hypothèse.\n",
      "\n",
      "\n",
      "==================== PAGE 7 — Hypothesis (Monte-Carlo absorption time) ====================\n",
      " Level  MC_N  A_mean_h  B_mean_h  A_median_h  B_median_h  B_minus_A_mean_h  B_minus_A_median_h  A_p10_h  A_p90_h  B_p10_h  B_p90_h\n",
      "     1  2000     29.82     23.45        24.0        16.0             -6.37                -8.0      8.0     64.0      8.0     48.0\n",
      "     2  2000     27.38     20.54        24.0        16.0             -6.83                -8.0      8.0     56.0      8.0     40.0\n",
      "     3  2000     19.17     15.27        16.0         8.0             -3.90                -8.0      8.0     40.0      8.0     32.0\n",
      "     4  2000     19.39     15.55        16.0         8.0             -3.84                -8.0      8.0     40.0      8.0     32.0\n",
      "\n",
      "Lecture métier rapide :\n",
      "- B_minus_A_mean_h < 0  => en moyenne, B mène plus vite à la maîtrise (niveau 5).\n",
      "- Les quantiles p10/p90 donnent l’incertitude (variabilité des trajectoires).\n",
      "\n",
      "\n",
      "==================== PAGE 7 — Réel (HoursTotal) par niveau ====================\n",
      " Level  n_students  HoursTotal_mean  HoursTotal_median  HoursTotal_p10  HoursTotal_p90  HoursTotal_min  HoursTotal_max\n",
      "     1           4            55.00               60.0            46.0            60.0            40.0            60.0\n",
      "     2          66            50.91               60.0            40.0            60.0            40.0            60.0\n",
      "     3         166            51.45               60.0            40.0            60.0            20.0            60.0\n",
      "     4         230            35.57               20.0            20.0            60.0            20.0            60.0\n",
      "\n",
      "Lecture métier :\n",
      "- Ceci décrit le temps de remediation OBSERVÉ (heures) selon le niveau initial.\n",
      "- Ça ne prouve pas A vs B si on n’a pas de label 'action réelle', mais ça contextualise la difficulté par niveau.\n",
      "\n",
      "\n",
      "==================== PAGE 7 — Verdict hypothèse ====================\n",
      "{\n",
      "  \"hypothesis_statement\": \"Levels 1-2 => A, Levels 3-4 => B\",\n",
      "  \"check_Qhours_pass\": false,\n",
      "  \"check_MC_pass\": false,\n",
      "  \"notes\": {\n",
      "    \"Qhours\": \"best action by minimizing one-step Bellman Q in HOURS\",\n",
      "    \"MC\": \"mean absorption time (hours) via Monte-Carlo with N=2000\"\n",
      "  }\n",
      "}\n",
      "\n",
      "✅ Exports Page 7 :\n",
      "- ./out/202602/out_pomdp\\L6-page7_hypothesis_level_summary.csv\n",
      "- ./out/202602/out_pomdp\\L6-page7_mc_summary.csv\n",
      "- ./out/202602/out_pomdp\\L6-page7_real_hours_by_level.csv\n",
      "- ./out/202602/out_pomdp\\L6-page7_hypothesis_summary.json\n",
      "\n",
      "==================== TEXTE MÉTIER (copier-coller après les résultats) ====================\n",
      "• Le tableau 'Bellman/Q in hours' compare A vs B pour chaque niveau en termes de temps attendu (heures).\n",
      "  - Si BestAction_by_Qhours = B pour un niveau, cela signifie que l’intensif réduit l’espérance de temps.\n",
      "• Le tableau 'Monte-Carlo' simule des trajectoires aléatoires jusqu’au niveau 5 : il montre la vitesse moyenne/typique.\n",
      "  - B_minus_A_mean_h < 0 signifie que B est plus rapide en moyenne.\n",
      "• Le tableau 'HoursTotal réel' est un descriptif : il donne la charge horaire observée dans les données par niveau initial.\n",
      "• La synthèse JSON te dit si l’hypothèse (1–2=>A, 3–4=>B) est confirmée selon ces 2 tests (Qhours et Monte-Carlo).\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Section 1 — Analyse & validation de l’hypothèse (A vs B selon le niveau)\n",
    "+ Synthèses  + exports CSV/JSON\n",
    "\n",
    "Hypothèse à tester (non vérité absolue) :\n",
    "- niveaux 1–2 : action A plus adaptée\n",
    "- niveaux 3–4 : action B plus adaptée\n",
    "\n",
    "Ce que cette page fait concrètement :\n",
    "1) Compare A vs B par NIVEAU via 3 angles complémentaires :\n",
    "   (i)  MDP en HEURES : Q_hours(s,a) = hours_per_day + sum_{s'} P(s'|s,a) * V_hours*(s')\n",
    "        -> action “meilleure” = argmin Q_hours\n",
    "   (ii) Monte-Carlo “temps d’absorption” en heures : simulation stochastique jusqu’à 5\n",
    "        -> compare moyenne/mediane/quantiles\n",
    "   (iii) Données réelles (HoursTotal) : compare la durée observée (heures) des élèves\n",
    "        de chaque niveau, en reliant à la “politique observée” si tu as un label action\n",
    "        (sinon, on fait une analyse descriptive globale par niveau)\n",
    "\n",
    "2) Analyse “π_ML” (si disponible) :\n",
    "   - distribution des actions prédites par niveau (combien A vs B)\n",
    "   - si tu as proba, moyenne proba(B) par niveau\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "EPS = 1e-12\n",
    "A, B = \"A\", \"B\"\n",
    "LEVELS = [1,2,3,4,5]\n",
    "TRANSIENT = [1,2,3,4]\n",
    "ABSORBING = 5\n",
    "N_LEVELS = 5\n",
    "\n",
    "#  : 4 activités/jour, 2h/activité\n",
    "HOURS_PER_ACTIVITY = 2.0\n",
    "ACTIVITIES_PER_DAY = 4.0\n",
    "HOURS_PER_DAY = HOURS_PER_ACTIVITY * ACTIVITIES_PER_DAY  # 8h\n",
    "\n",
    "# Monte-Carlo\n",
    "MC_N = 2000\n",
    "MC_MAX_DAYS = 120\n",
    "MC_SEED = 42\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (0) Helpers\n",
    "# ------------------------------------------------------------\n",
    "def _ensure_prob_row(row: np.ndarray) -> np.ndarray:\n",
    "    row = np.array(row, dtype=float)\n",
    "    row = np.clip(row, 0.0, 1.0)\n",
    "    s = float(row.sum())\n",
    "    if s <= EPS:\n",
    "        return np.ones_like(row) / len(row)\n",
    "    return row / s\n",
    "\n",
    "def q_hours(level: int, action: str, V_hours: np.ndarray, TA: np.ndarray, TB: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Q_hours(s,a) = cost(hours_per_day) + sum_{s'} P(s'|s,a) * V_hours(s')\n",
    "    V_hours(5)=0\n",
    "    \"\"\"\n",
    "    row = TA[level-1, :] if action == A else TB[level-1, :]\n",
    "    row = _ensure_prob_row(row)\n",
    "    return float(HOURS_PER_DAY + np.dot(row, V_hours))\n",
    "\n",
    "def pick_best_action_by_q(level: int, V_hours: np.ndarray, TA: np.ndarray, TB: np.ndarray) -> dict:\n",
    "    qA = q_hours(level, A, V_hours, TA, TB)\n",
    "    qB = q_hours(level, B, V_hours, TA, TB)\n",
    "    best = A if qA <= qB else B\n",
    "    return {\"qA_hours\": qA, \"qB_hours\": qB, \"best_action\": best}\n",
    "\n",
    "def mc_absorption_time_hours(start_level: int, action_fixed: str, TA: np.ndarray, TB: np.ndarray,\n",
    "                             n: int = 1000, seed: int = 42, max_days: int = 120) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simule n trajectoires jusqu’à absorption (5), en appliquant action_fixed à chaque étape.\n",
    "    Retourne un vecteur des temps (heures).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    times = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        s = int(start_level)\n",
    "        days = 0\n",
    "        while s != ABSORBING and days < max_days:\n",
    "            row = TA[s-1, :] if action_fixed == A else TB[s-1, :]\n",
    "            row = _ensure_prob_row(row)\n",
    "            s = int(rng.choice(np.arange(1, N_LEVELS+1), p=row))\n",
    "            days += 1\n",
    "\n",
    "        times.append(float(days * HOURS_PER_DAY))\n",
    "\n",
    "    return np.array(times, dtype=float)\n",
    "\n",
    "def summarize_mc(x: np.ndarray) -> dict:\n",
    "    x = np.array(x, dtype=float)\n",
    "    return {\n",
    "        \"mean\": float(np.mean(x)),\n",
    "        \"median\": float(np.median(x)),\n",
    "        \"p10\": float(np.quantile(x, 0.10)),\n",
    "        \"p90\": float(np.quantile(x, 0.90)),\n",
    "        \"min\": float(np.min(x)),\n",
    "        \"max\": float(np.max(x)),\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (1) Choisir V_hours à utiliser\n",
    "# ------------------------------------------------------------\n",
    "if \"V_hours_star\" in globals():\n",
    "    Vh = np.array(V_hours_star, dtype=float)\n",
    "    used_V_name = \"V_hours_star\"\n",
    "elif \"V_star\" in globals():\n",
    "    # fallback : V_star est en “pas” (jours). On approx en heures : pas * 8h\n",
    "    Vh = np.array(V_star, dtype=float) * HOURS_PER_DAY\n",
    "    used_V_name = \"V_star_scaled_to_hours\"\n",
    "else:\n",
    "    # dernier recours\n",
    "    Vh = np.zeros(N_LEVELS, dtype=float)\n",
    "    used_V_name = \"none\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (2) Hypothesis check — via Q_hours (Bellman local)\n",
    "# ------------------------------------------------------------\n",
    "rows = []\n",
    "for s in TRANSIENT:\n",
    "    d = pick_best_action_by_q(s, Vh, T_A, T_B)\n",
    "    rows.append({\n",
    "        \"Level\": s,\n",
    "        \"BestAction_by_Qhours\": d[\"best_action\"],\n",
    "        \"Q_A_hours\": round(d[\"qA_hours\"], 3),\n",
    "        \"Q_B_hours\": round(d[\"qB_hours\"], 3),\n",
    "        \"Delta_B_minus_A_hours\": round(d[\"qB_hours\"] - d[\"qA_hours\"], 3),  # <0 => B better\n",
    "    })\n",
    "\n",
    "df_q = pd.DataFrame(rows).sort_values(\"Level\")\n",
    "df_q.to_csv(out_path(\"SECTION1_hypothesis_level_summary.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n==================== Section 1 — Hypothesis (Bellman/Q in hours) ====================\")\n",
    "print(f\"[INFO] Value used: {used_V_name}\")\n",
    "print(df_q.to_string(index=False))\n",
    "print(\"\\nLecture  rapide :\")\n",
    "print(\"- Delta_B_minus_A_hours < 0  => B réduit l’espérance de temps (meilleur).\")\n",
    "print(\"- Delta_B_minus_A_hours > 0  => A est meilleur.\")\n",
    "print(\"=> Compare niveaux 1–2 vs 3–4 pour confirmer/infirmer l’hypothèse.\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (3) Hypothesis check — Monte-Carlo (A fixe vs B fixe)\n",
    "# ------------------------------------------------------------\n",
    "mc_rows = []\n",
    "for s in TRANSIENT:\n",
    "    tA = mc_absorption_time_hours(s, A, T_A, T_B, n=MC_N, seed=MC_SEED+s, max_days=MC_MAX_DAYS)\n",
    "    tB = mc_absorption_time_hours(s, B, T_A, T_B, n=MC_N, seed=MC_SEED+100+s, max_days=MC_MAX_DAYS)\n",
    "\n",
    "    sA = summarize_mc(tA)\n",
    "    sB = summarize_mc(tB)\n",
    "\n",
    "    mc_rows.append({\n",
    "        \"Level\": s,\n",
    "        \"MC_N\": MC_N,\n",
    "        \"A_mean_h\": round(sA[\"mean\"], 2),\n",
    "        \"B_mean_h\": round(sB[\"mean\"], 2),\n",
    "        \"A_median_h\": round(sA[\"median\"], 2),\n",
    "        \"B_median_h\": round(sB[\"median\"], 2),\n",
    "        \"B_minus_A_mean_h\": round(sB[\"mean\"] - sA[\"mean\"], 2),   # <0 => B better\n",
    "        \"B_minus_A_median_h\": round(sB[\"median\"] - sA[\"median\"], 2),\n",
    "        \"A_p10_h\": round(sA[\"p10\"], 2),\n",
    "        \"A_p90_h\": round(sA[\"p90\"], 2),\n",
    "        \"B_p10_h\": round(sB[\"p10\"], 2),\n",
    "        \"B_p90_h\": round(sB[\"p90\"], 2),\n",
    "    })\n",
    "\n",
    "df_mc = pd.DataFrame(mc_rows).sort_values(\"Level\")\n",
    "df_mc.to_csv(out_path(\"SECTION2_mc_summary.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n==================== Section 2 — Hypothesis (Monte-Carlo absorption time) ====================\")\n",
    "print(df_mc.to_string(index=False))\n",
    "print(\"\\nLecture  rapide :\")\n",
    "print(\"- B_minus_A_mean_h < 0  => en moyenne, B mène plus vite à la maîtrise (niveau 5).\")\n",
    "print(\"- Les quantiles p10/p90 donnent l’incertitude (variabilité des trajectoires).\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (4) Données réelles — HoursTotal par niveau (descriptif)\n",
    "# ------------------------------------------------------------\n",
    "df_real = df.dropna(subset=[\"Pretest_i\", \"HoursTotal\"]).copy()\n",
    "df_real[\"Pretest_i\"] = df_real[\"Pretest_i\"].astype(int)\n",
    "df_real[\"HoursTotal\"] = pd.to_numeric(df_real[\"HoursTotal\"], errors=\"coerce\")\n",
    "\n",
    "real_rows = []\n",
    "for s in TRANSIENT:\n",
    "    dfi = df_real[df_real[\"Pretest_i\"] == s]\n",
    "    if len(dfi) == 0:\n",
    "        continue\n",
    "    x = dfi[\"HoursTotal\"].dropna().values.astype(float)\n",
    "    if len(x) == 0:\n",
    "        continue\n",
    "    real_rows.append({\n",
    "        \"Level\": s,\n",
    "        \"n_students\": int(len(x)),\n",
    "        \"HoursTotal_mean\": round(float(np.mean(x)), 2),\n",
    "        \"HoursTotal_median\": round(float(np.median(x)), 2),\n",
    "        \"HoursTotal_p10\": round(float(np.quantile(x, 0.10)), 2),\n",
    "        \"HoursTotal_p90\": round(float(np.quantile(x, 0.90)), 2),\n",
    "        \"HoursTotal_min\": round(float(np.min(x)), 2),\n",
    "        \"HoursTotal_max\": round(float(np.max(x)), 2),\n",
    "    })\n",
    "\n",
    "df_real_sum = pd.DataFrame(real_rows).sort_values(\"Level\")\n",
    "df_real_sum.to_csv(out_path(\"SECTION2_real_hours_by_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n==================== Section 2 — Réel (HoursTotal) par niveau ====================\")\n",
    "if len(df_real_sum) > 0:\n",
    "    print(df_real_sum.to_string(index=False))\n",
    "    print(\"\\nLecture  :\")\n",
    "    print(\"- Ceci décrit le temps de remediation OBSERVÉ (heures) selon le niveau initial.\")\n",
    "    print(\"- Ça ne prouve pas A vs B si on n’a pas de label 'action réelle', mais ça contextualise la difficulté par niveau.\\n\")\n",
    "else:\n",
    "    print(\"[WARN] Pas de données HoursTotal exploitables.\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (5) π_ML : distribution des actions prédites par niveau (si dispo)\n",
    "# ------------------------------------------------------------\n",
    "ml_exported = False\n",
    "if \"pi_ml_state\" in globals() and isinstance(pi_ml_state, dict):\n",
    "    # pi_ml_state est une “politique stationnaire” (par niveau) => simple tableau\n",
    "    ml_rows = []\n",
    "    for s in TRANSIENT:\n",
    "        ml_rows.append({\n",
    "            \"Level\": s,\n",
    "            \"pi_ml_state_action\": pi_ml_state.get(s, A)\n",
    "        })\n",
    "    df_ml = pd.DataFrame(ml_rows)\n",
    "    df_ml.to_csv(out_path(\"SECTION2_ml_policy_by_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    ml_exported = True\n",
    "\n",
    "    print(\"\\n==================== Section 3 — π_ML_state (stationnaire) ====================\")\n",
    "    print(df_ml.to_string(index=False))\n",
    "    print(\"\\nLecture  :\")\n",
    "    print(\"- π_ML_state dit : 'si un élève est au niveau s, l’action la plus probable observée/prévue est A ou B'.\\n\")\n",
    "\n",
    "# Si tu as une fonction de proba par ligne (Section 1), on peut aussi faire : moyenne P(B|features) par niveau\n",
    "if \"predict_action_proba_row\" in globals() and callable(predict_action_proba_row):\n",
    "    dfx = df.dropna(subset=[\"Pretest_i\"]).copy()\n",
    "    dfx[\"Pretest_i\"] = dfx[\"Pretest_i\"].astype(int)\n",
    "\n",
    "    prows = []\n",
    "    for s in TRANSIENT:\n",
    "        dfi = dfx[dfx[\"Pretest_i\"] == s].head(500).copy()  # cap sécurité\n",
    "        if len(dfi) == 0:\n",
    "            continue\n",
    "        pB_list = []\n",
    "        for _, r in dfi.iterrows():\n",
    "            pr = predict_action_proba_row(r)  # expected dict or tuple\n",
    "            # on accepte plusieurs formats robustes :\n",
    "            if isinstance(pr, dict) and \"pB\" in pr:\n",
    "                pB_list.append(float(pr[\"pB\"]))\n",
    "            elif isinstance(pr, (list, tuple)) and len(pr) >= 1:\n",
    "                pB_list.append(float(pr[0]))\n",
    "        if len(pB_list) == 0:\n",
    "            continue\n",
    "\n",
    "        prows.append({\n",
    "            \"Level\": s,\n",
    "            \"n_rows_used\": int(len(pB_list)),\n",
    "            \"mean_pB\": round(float(np.mean(pB_list)), 4),\n",
    "            \"median_pB\": round(float(np.median(pB_list)), 4),\n",
    "        })\n",
    "\n",
    "    if len(prows) > 0:\n",
    "        df_pB = pd.DataFrame(prows).sort_values(\"Level\")\n",
    "        df_pB.to_csv(out_path(\"SECTION2_ml_probB_by_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(\"\\n==================== Section 3 — π_ML(features): proba(B) par niveau ====================\")\n",
    "        print(df_pB.to_string(index=False))\n",
    "        print(\"\\nLecture  :\")\n",
    "        print(\"- mean_pB proche de 1 => le ML pense que B est souvent adapté pour ce niveau/ces profils.\")\n",
    "        print(\"- mean_pB proche de 0 => plutôt A.\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (6) Verdict automatique sur l’hypothèse (résumé)\n",
    "# ------------------------------------------------------------\n",
    "# Hypothèse : 1-2 => A ; 3-4 => B\n",
    "hyp_ok_q = True\n",
    "for s in [1,2]:\n",
    "    act = df_q[df_q[\"Level\"]==s][\"BestAction_by_Qhours\"].values\n",
    "    if len(act) and act[0] != A:\n",
    "        hyp_ok_q = False\n",
    "for s in [3,4]:\n",
    "    act = df_q[df_q[\"Level\"]==s][\"BestAction_by_Qhours\"].values\n",
    "    if len(act) and act[0] != B:\n",
    "        hyp_ok_q = False\n",
    "\n",
    "hyp_ok_mc = True\n",
    "for s in [1,2]:\n",
    "    v = df_mc[df_mc[\"Level\"]==s][\"B_minus_A_mean_h\"].values\n",
    "    if len(v) and float(v[0]) < 0:  # B faster for level 1/2\n",
    "        hyp_ok_mc = False\n",
    "for s in [3,4]:\n",
    "    v = df_mc[df_mc[\"Level\"]==s][\"B_minus_A_mean_h\"].values\n",
    "    if len(v) and float(v[0]) > 0:  # A faster for level 3/4\n",
    "        hyp_ok_mc = False\n",
    "\n",
    "verdict = {\n",
    "    \"hypothesis_statement\": \"Levels 1-2 => A, Levels 3-4 => B\",\n",
    "    \"check_Qhours_pass\": bool(hyp_ok_q),\n",
    "    \"check_MC_pass\": bool(hyp_ok_mc),\n",
    "    \"notes\": {\n",
    "        \"Qhours\": \"best action by minimizing one-step Bellman Q in HOURS\",\n",
    "        \"MC\": f\"mean absorption time (hours) via Monte-Carlo with N={MC_N}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(out_path(\"SECTION2_hypothesis_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(verdict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(json.dumps(verdict, ensure_ascii=False, indent=2))\n",
    "print(\"\\n Exports Section 1 :\")\n",
    "print(\"-\", out_path(\"SECTION2_hypothesis_level_summary.csv\"))\n",
    "print(\"-\", out_path(\"SECTION2_mc_summary.csv\"))\n",
    "print(\"-\", out_path(\"SECTION2_real_hours_by_level.csv\"))\n",
    "if ml_exported:\n",
    "    print(\"-\", out_path(\"SECTION2_ml_policy_by_level.csv\"))\n",
    "print(\"-\", out_path(\"SECTION2_hypothesis_summary.json\"))\n",
    "\n",
    "print(\"• Le tableau 'Bellman/Q in hours' compare A vs B pour chaque niveau en termes de temps attendu (heures).\")\n",
    "print(\"  - Si BestAction_by_Qhours = B pour un niveau, cela signifie que l’intensif réduit l’espérance de temps.\")\n",
    "print(\"• Le tableau 'Monte-Carlo' simule des trajectoires aléatoires jusqu’au niveau 5 : il montre la vitesse moyenne/typique.\")\n",
    "print(\"  - B_minus_A_mean_h < 0 signifie que B est plus rapide en moyenne.\")\n",
    "print(\"• Le tableau 'HoursTotal réel' est un descriptif : il donne la charge horaire observée dans les données par niveau initial.\")\n",
    "print(\"• La synthèse JSON te dit si l’hypothèse (1–2=>A, 3–4=>B) est confirmée selon ces 2 tests (Qhours et Monte-Carlo).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
