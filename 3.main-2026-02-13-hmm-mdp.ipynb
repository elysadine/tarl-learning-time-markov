{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd78a542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIG SECTION 1 ---\n",
      "HOURS_PER_DAY : 2.0 h (Durée totale de la séquence de 4 activités)\n",
      "ACTIVITIES_PER_DAY : 4\n",
      "L1 — Onglets détectés: ['Résultatsélèves', 'Activités']\n",
      "\n",
      " L1 (résultats) choisi: Résultatsélèves | shape: (813, 8)\n",
      " L1 (activités) choisi: Activités | shape: (10, 54)\n",
      " L3 chargé: | shape: (813, 265)\n",
      "\n",
      "Blocs détectés (préfixes) dans L1/Activités:\n",
      " - Deuxchiffres: 13 colonnes\n",
      " - TroisA: 11 colonnes\n",
      " - Deb: 10 colonnes\n",
      " - Unchiffe: 10 colonnes\n",
      " - TroisB: 8 colonnes\n",
      "\n",
      "Contrôle: nb d'activités exécutées par jour (attendu = 4):\n",
      "count    10.000000\n",
      "mean     20.200000\n",
      "std       0.632456\n",
      "min      20.000000\n",
      "25%      20.000000\n",
      "50%      20.000000\n",
      "75%      20.000000\n",
      "max      22.000000\n",
      "dtype: float64\n",
      "Jours anormaux (ExecutedCount != 4): 10\n",
      " Week  Day  ExecutedCount\n",
      "    1    1             20\n",
      "    1    2             20\n",
      "    1    3             20\n",
      "    1    4             20\n",
      "    1    5             20\n",
      "    2    6             20\n",
      "    2    7             20\n",
      "    2    8             22\n",
      "    2    9             20\n",
      "    2   10             20\n",
      "\n",
      "Contrôle: positions manquantes dans la journée (attendu = 0):\n",
      "0    10\n",
      "\n",
      "L1 Résultats — aperçu colonnes: ['StudentID', 'Age', 'Classe', 'HoursTotal_L1', 'Genre', 'Zone', 'Pretest', 'Final', 'Pretest_i', 'Final_i']\n",
      "\n",
      "Distribution Pretest_i (L1):\n",
      "Pretest_i\n",
      "1      4\n",
      "2     66\n",
      "3    166\n",
      "4    230\n",
      "5    347\n",
      "\n",
      "Distribution Final_i (L1):\n",
      "Final_i\n",
      "1      8\n",
      "2     43\n",
      "3    171\n",
      "4    118\n",
      "5    473\n",
      "\n",
      "HoursTotal_L1 — stats:\n",
      "count    813.000000\n",
      "mean      45.190652\n",
      "std       16.935674\n",
      "min       20.000000\n",
      "25%       20.000000\n",
      "50%       60.000000\n",
      "75%       60.000000\n",
      "max       60.000000\n",
      "\n",
      "L3 — colonnes importantes \n",
      " - HoursTotal: OK\n",
      " - Pretest: OK\n",
      " - Final: OK\n",
      " - Delta: OK\n",
      " - Mastery_ge4: OK\n",
      " - Age: OK\n",
      " - Genre: OK\n",
      " - Zone: OK\n",
      " - LevelTag: OK\n",
      "\n",
      "HoursTotal (L3) — stats:\n",
      "count    813.000000\n",
      "mean      45.190652\n",
      "std       16.935674\n",
      "min       20.000000\n",
      "25%       20.000000\n",
      "50%       60.000000\n",
      "75%       60.000000\n",
      "max       60.000000\n",
      "\n",
      "Merge L1-L3 via StudentID: (813, 276)\n",
      "\n",
      "Cohérence HoursTotal: (L3 - L1) stats\n",
      "count    813.0\n",
      "mean       0.0\n",
      "std        0.0\n",
      "min        0.0\n",
      "25%        0.0\n",
      "50%        0.0\n",
      "75%        0.0\n",
      "max        0.0\n",
      "\n",
      " SECTION 1 terminée.\n",
      "Fichiers produits:\n",
      " - ./out/202602/out_pomdp\\L0-L1_results_clean.csv\n",
      " - ./out/202602/out_pomdp\\L0-L1_activities_clean.csv\n",
      " - ./out/202602/out_pomdp\\L0-L1_activity_column_blocks.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION1_summary.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# ============================================================\n",
    "# 0) CONFIG — CORRIGÉE (1 jour = 2h)\n",
    "# ============================================================\n",
    "L1_PATH = \"./data/2025-08/L1.20250818-DataMathsElysa.xlsx\"\n",
    "L3_PATH = \"./data/2025-08/L3.features_by_student.byPretest.csv\"\n",
    "\n",
    "OUT_DIR = \"./out/202602/out_pomdp\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def out_path(fname: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"L0-{fname}\")\n",
    "\n",
    "# --- CORRECTION  ---\n",
    "# La séquence journalière est composée de 4 activités.\n",
    "# Cette séquence complète dure 2h.\n",
    "HOURS_PER_DAY = 2.0\n",
    "ACTIVITIES_PER_DAY = 4\n",
    "\n",
    "# (Pour info seulement, si besoin un jour)\n",
    "HOURS_PER_ACTIVITY = HOURS_PER_DAY / ACTIVITIES_PER_DAY  # 0.5h = 30 min\n",
    "\n",
    "print(f\"--- CONFIG SECTION 1 ---\")\n",
    "print(f\"HOURS_PER_DAY : {HOURS_PER_DAY} h (Durée totale de la séquence de 4 activités)\")\n",
    "print(f\"ACTIVITIES_PER_DAY : {ACTIVITIES_PER_DAY}\")\n",
    "\n",
    "# ============================================================\n",
    "# 1) LECTURE FICHIERS\n",
    "# ============================================================\n",
    "assert os.path.exists(L1_PATH), f\"Introuvable: {L1_PATH}\"\n",
    "assert os.path.exists(L3_PATH), f\"Introuvable: {L3_PATH}\"\n",
    "\n",
    "xls = pd.ExcelFile(L1_PATH)\n",
    "print(\"L1 — Onglets détectés:\", xls.sheet_names)\n",
    "\n",
    "# Lire les deux onglets (on n'impose pas le nom exact, on mappe par heuristique)\n",
    "df_sheets = {name: pd.read_excel(L1_PATH, sheet_name=name) for name in xls.sheet_names}\n",
    "\n",
    "# Heuristique: onglet \"Résultatsélèves\" = contient Pretest/Final + IDélèves\n",
    "# onglet \"Activités\" = contient Semaine/Jour + colonnes activités 0..4\n",
    "def _score_results_sheet(d: pd.DataFrame) -> int:\n",
    "    cols = {c.strip(): c for c in d.columns}\n",
    "    score = 0\n",
    "    for k in [\"IDélèves\",\"Pretest\",\"Final\",\"Age\",\"Genre\",\"Zone\"]:\n",
    "        if k in cols: score += 2\n",
    "    if \"Nombre d'heures de remédiation fait au total\" in cols: score += 2\n",
    "    return score\n",
    "\n",
    "def _score_activities_sheet(d: pd.DataFrame) -> int:\n",
    "    cols = [str(c) for c in d.columns]\n",
    "    score = 0\n",
    "    if any(c.lower().strip() == \"semaine\" for c in cols): score += 2\n",
    "    if any(c.lower().strip() == \"jour\" for c in cols): score += 2\n",
    "    # beaucoup de colonnes après les 2 premières\n",
    "    if len(cols) > 20: score += 2\n",
    "    return score\n",
    "\n",
    "sheet_scores = []\n",
    "for name, d in df_sheets.items():\n",
    "    sheet_scores.append((name, _score_results_sheet(d), _score_activities_sheet(d)))\n",
    "\n",
    "# Choix\n",
    "results_sheet = sorted(sheet_scores, key=lambda t: t[1], reverse=True)[0][0]\n",
    "activities_sheet = sorted(sheet_scores, key=lambda t: t[2], reverse=True)[0][0]\n",
    "\n",
    "df_results = df_sheets[results_sheet].copy()\n",
    "df_acts = df_sheets[activities_sheet].copy()\n",
    "df_l3 = pd.read_csv(L3_PATH)\n",
    "\n",
    "print(\"\\n L1 (résultats) choisi:\", results_sheet, \"| shape:\", df_results.shape)\n",
    "print(\" L1 (activités) choisi:\", activities_sheet, \"| shape:\", df_acts.shape)\n",
    "print(\" L3 chargé:\", \"| shape:\", df_l3.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 2) NORMALISATION DES COLONNES (L1 Résultats)\n",
    "# ============================================================\n",
    "def _norm_colname(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "df_results.columns = [_norm_colname(c) for c in df_results.columns]\n",
    "df_acts.columns = [_norm_colname(c) for c in df_acts.columns]\n",
    "df_l3.columns = [_norm_colname(c) for c in df_l3.columns]\n",
    "\n",
    "# Renommage standard\n",
    "rename_results = {\n",
    "    \"IDélèves\": \"StudentID\",\n",
    "    \"Nombre d'heures de remédiation fait au total\": \"HoursTotal_L1\",\n",
    "    \"Pretest\": \"Pretest\",\n",
    "    \"Final\": \"Final\",\n",
    "    \"Age\": \"Age\",\n",
    "    \"Classe\": \"Classe\",\n",
    "    \"Genre\": \"Genre\",\n",
    "    \"Zone\": \"Zone\",\n",
    "}\n",
    "for k, v in rename_results.items():\n",
    "    if k in df_results.columns:\n",
    "        df_results.rename(columns={k: v}, inplace=True)\n",
    "\n",
    "# Coercitions types\n",
    "if \"StudentID\" in df_results.columns:\n",
    "    df_results[\"StudentID\"] = df_results[\"StudentID\"].astype(str).str.strip()\n",
    "\n",
    "for c in [\"Age\", \"Pretest\", \"Final\", \"HoursTotal_L1\"]:\n",
    "    if c in df_results.columns:\n",
    "        df_results[c] = pd.to_numeric(df_results[c], errors=\"coerce\")\n",
    "\n",
    "# bornage niveaux 1..5\n",
    "def cap_level(x):\n",
    "    try:\n",
    "        v = int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return min(5, max(1, v))\n",
    "\n",
    "if \"Pretest\" in df_results.columns:\n",
    "    df_results[\"Pretest_i\"] = df_results[\"Pretest\"].apply(cap_level)\n",
    "if \"Final\" in df_results.columns:\n",
    "    df_results[\"Final_i\"] = df_results[\"Final\"].apply(cap_level)\n",
    "\n",
    "# ============================================================\n",
    "# 3) NORMALISATION (L1 Activités)\n",
    "# ============================================================\n",
    "# Attendu: colonnes \"Semaine\", \"Jour\", puis activités groupées par préfixes (Deb, Unchiffe, Deuxchiffres, TroisA, TroisB)\n",
    "# Standardiser \"Semaine\" et \"Jour\"\n",
    "col_week = None\n",
    "col_day = None\n",
    "for c in df_acts.columns:\n",
    "    cl = c.lower().strip()\n",
    "    if cl == \"semaine\": col_week = c\n",
    "    if cl == \"jour\": col_day = c\n",
    "\n",
    "if col_week is None or col_day is None:\n",
    "    raise ValueError(\"Onglet Activités: colonnes 'Semaine' et/ou 'Jour' introuvables après normalisation.\")\n",
    "\n",
    "df_acts.rename(columns={col_week: \"Week\", col_day: \"Day\"}, inplace=True)\n",
    "df_acts[\"Week\"] = pd.to_numeric(df_acts[\"Week\"], errors=\"coerce\")\n",
    "df_acts[\"Day\"] = pd.to_numeric(df_acts[\"Day\"], errors=\"coerce\")\n",
    "\n",
    "# Colonnes d'activités = tout sauf Week/Day\n",
    "activity_cols = [c for c in df_acts.columns if c not in [\"Week\",\"Day\"]]\n",
    "\n",
    "# Forcer numériques (0..4)\n",
    "for c in activity_cols:\n",
    "    df_acts[c] = pd.to_numeric(df_acts[c], errors=\"coerce\")\n",
    "\n",
    "# Détecter les préfixes bloc (avant le premier espace)\n",
    "def block_prefix(col: str) -> str:\n",
    "    s = str(col).strip()\n",
    "    # ex: \"Deb Lecture de la Table d’addition\"\n",
    "    return s.split(\" \", 1)[0] if \" \" in s else s\n",
    "\n",
    "df_blocks = pd.Series([block_prefix(c) for c in activity_cols], name=\"Block\")\n",
    "block_counts = df_blocks.value_counts().to_dict()\n",
    "\n",
    "print(\"\\nBlocs détectés (préfixes) dans L1/Activités:\")\n",
    "for k, v in sorted(block_counts.items(), key=lambda kv: (-kv[1], kv[0])):\n",
    "    print(f\" - {k}: {v} colonnes\")\n",
    "\n",
    "# Marquer bloc pour chaque colonne activité\n",
    "col_to_block = {c: block_prefix(c) for c in activity_cols}\n",
    "\n",
    "# ============================================================\n",
    "# 4) CONTROLES  (L1 Activités)\n",
    "# ============================================================\n",
    "# 4.1 Vérifier que chaque jour contient exactement 4 activités exécutées (valeurs 1..4)\n",
    "def count_executed_per_day(row: pd.Series) -> int:\n",
    "    vals = row[activity_cols].values\n",
    "    return int(np.sum(np.isin(vals, [1,2,3,4])))\n",
    "\n",
    "exec_counts = df_acts.apply(count_executed_per_day, axis=1)\n",
    "print(\"\\nContrôle: nb d'activités exécutées par jour (attendu = 4):\")\n",
    "print(exec_counts.describe())\n",
    "\n",
    "bad_days = df_acts.loc[exec_counts != 4, [\"Week\",\"Day\"]].copy()\n",
    "bad_days[\"ExecutedCount\"] = exec_counts[exec_counts != 4].values\n",
    "print(f\"Jours anormaux (ExecutedCount != 4): {len(bad_days)}\")\n",
    "if len(bad_days) > 0:\n",
    "    print(bad_days.head(20).to_string(index=False))\n",
    "\n",
    "# 4.2 Vérifier présence des positions 1..4 dans chaque ligne (si 4 activités, on veut idéalement 1,2,3,4 chacune une fois)\n",
    "def missing_positions(row: pd.Series):\n",
    "    vals = row[activity_cols].values\n",
    "    pos = set([int(v) for v in vals if v in [1,2,3,4]])\n",
    "    missing = [p for p in [1,2,3,4] if p not in pos]\n",
    "    return missing\n",
    "\n",
    "miss_pos = df_acts.apply(missing_positions, axis=1)\n",
    "n_miss = miss_pos.apply(len)\n",
    "print(\"\\nContrôle: positions manquantes dans la journée (attendu = 0):\")\n",
    "print(n_miss.value_counts().sort_index().to_string())\n",
    "\n",
    "# ============================================================\n",
    "# 5) CONTROLES (L1 Résultats) + (L3)\n",
    "# ============================================================\n",
    "print(\"\\nL1 Résultats — aperçu colonnes:\", df_results.columns.tolist())\n",
    "\n",
    "# Checks simples niveaux\n",
    "if \"Pretest_i\" in df_results.columns:\n",
    "    print(\"\\nDistribution Pretest_i (L1):\")\n",
    "    print(df_results[\"Pretest_i\"].value_counts(dropna=False).sort_index().to_string())\n",
    "\n",
    "if \"Final_i\" in df_results.columns:\n",
    "    print(\"\\nDistribution Final_i (L1):\")\n",
    "    print(df_results[\"Final_i\"].value_counts(dropna=False).sort_index().to_string())\n",
    "\n",
    "if \"HoursTotal_L1\" in df_results.columns:\n",
    "    print(\"\\nHoursTotal_L1 — stats:\")\n",
    "    print(df_results[\"HoursTotal_L1\"].describe().to_string())\n",
    "\n",
    "# L3 checks\n",
    "print(\"\\nL3 — colonnes importantes \")\n",
    "for c in [\"HoursTotal\",\"Pretest\",\"Final\",\"Delta\",\"Mastery_ge4\",\"Age\",\"Genre\",\"Zone\",\"LevelTag\"]:\n",
    "    print(f\" - {c}: {'OK' if c in df_l3.columns else 'ABSENT'}\")\n",
    "\n",
    "if \"HoursTotal\" in df_l3.columns:\n",
    "    df_l3[\"HoursTotal\"] = pd.to_numeric(df_l3[\"HoursTotal\"], errors=\"coerce\")\n",
    "    print(\"\\nHoursTotal (L3) — stats:\")\n",
    "    print(df_l3[\"HoursTotal\"].describe().to_string())\n",
    "\n",
    "# ============================================================\n",
    "# 6) (OPTIONNEL) COHERENCE L1 vs L3 : heures totales\n",
    "# ============================================================\n",
    "has_id_l3 = any(c.lower() in [\"studentid\",\"idélèves\",\"ideleves\",\"id_eleve\",\"id\"] for c in df_l3.columns)\n",
    "\n",
    "if has_id_l3:\n",
    "    # repérer la colonne id la plus plausible\n",
    "    cand = None\n",
    "    for c in df_l3.columns:\n",
    "        if c.lower() in [\"studentid\",\"idélèves\",\"ideleves\",\"id_eleve\",\"id\"]:\n",
    "            cand = c\n",
    "            break\n",
    "    if cand and \"StudentID\" in df_results.columns:\n",
    "        df_l3[\"_StudentID\"] = df_l3[cand].astype(str).str.strip()\n",
    "        df_merge = df_results.merge(df_l3, left_on=\"StudentID\", right_on=\"_StudentID\", how=\"inner\", suffixes=(\"_L1\",\"_L3\"))\n",
    "        print(\"\\nMerge L1-L3 via StudentID:\", df_merge.shape)\n",
    "\n",
    "        if \"HoursTotal_L1\" in df_merge.columns and \"HoursTotal\" in df_merge.columns:\n",
    "            diff = (df_merge[\"HoursTotal\"] - df_merge[\"HoursTotal_L1\"])\n",
    "            print(\"\\nCohérence HoursTotal: (L3 - L1) stats\")\n",
    "            print(diff.describe().to_string())\n",
    "else:\n",
    "    print(\"\\nInfo: L3 ne contient pas d'ID explicite (StudentID). On ne merge pas L1 et L3 à cette étape.\")\n",
    "\n",
    "# ============================================================\n",
    "# 7) SAUVEGARDES “CLEAN”\n",
    "# ============================================================\n",
    "# Sauver L1 clean\n",
    "df_results.to_csv(out_path(\"L1_results_clean.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "df_acts.to_csv(out_path(\"L1_activities_clean.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Sauver un petit dictionnaire colonne->bloc pour réutilisation\n",
    "df_colblock = pd.DataFrame({\n",
    "    \"ActivityColumn\": activity_cols,\n",
    "    \"Block\": [col_to_block[c] for c in activity_cols]\n",
    "})\n",
    "df_colblock.to_csv(out_path(\"L1_activity_column_blocks.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Sauver une synthèse simple avec les NOUVELLES CONSTANTES\n",
    "summary = {\n",
    "    \"L1_path\": L1_PATH,\n",
    "    \"L3_path\": L3_PATH,\n",
    "    \"L1_results_sheet\": results_sheet,\n",
    "    \"L1_activities_sheet\": activities_sheet,\n",
    "    \"L1_results_shape\": df_results.shape,\n",
    "    \"L1_activities_shape\": df_acts.shape,\n",
    "    \"L3_shape\": df_l3.shape,\n",
    "    \"HOURS_PER_DAY\": HOURS_PER_DAY,          # 2.0\n",
    "    \"ACTIVITIES_PER_DAY\": ACTIVITIES_PER_DAY, # 4\n",
    "    \"HOURS_PER_ACTIVITY\": HOURS_PER_ACTIVITY, # 0.5\n",
    "    \"blocks_detected\": block_counts,\n",
    "    \"n_bad_days_executedcount_ne_4\": int((exec_counts != 4).sum()),\n",
    "    \"n_days_missing_positions\": int((n_miss > 0).sum()),\n",
    "    \"outputs\": {\n",
    "        \"L1_results_clean\": out_path(\"L1_results_clean.csv\"),\n",
    "        \"L1_activities_clean\": out_path(\"L1_activities_clean.csv\"),\n",
    "        \"L1_activity_column_blocks\": out_path(\"L1_activity_column_blocks.csv\"),\n",
    "    }\n",
    "}\n",
    "with open(out_path(\"SECTION1_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n SECTION 1 terminée.\")\n",
    "print(\"Fichiers produits:\")\n",
    "print(\" -\", out_path(\"L1_results_clean.csv\"))\n",
    "print(\" -\", out_path(\"L1_activities_clean.csv\"))\n",
    "print(\" -\", out_path(\"L1_activity_column_blocks.csv\"))\n",
    "print(\" -\", out_path(\"SECTION1_summary.json\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "beb44e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocs détectés: ['Deb', 'Deuxchiffres', 'TroisA', 'TroisB', 'Unchiffe']\n",
      "\n",
      "Qualité par bloc (résumé):\n",
      " - Deb: total=10 ok=10 bad=0\n",
      " - Deuxchiffres: total=10 ok=9 bad=1\n",
      " - TroisA: total=10 ok=9 bad=1\n",
      " - TroisB: total=10 ok=10 bad=0\n",
      " - Unchiffe: total=10 ok=10 bad=0\n",
      "\n",
      " SECTION 2 terminée.\n",
      "Fichiers produits:\n",
      " - ./out/202602/out_pomdp\\L0-SECTION2_L1_sequences_by_block_long.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION2_L1_sequences_by_block_list.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION2_L1_sequences_by_block_vector.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION2_block_to_activities.json\n",
      " - ./out/202602/out_pomdp\\L0-SECTION2_block_quality.json\n",
      "\n",
      "Aperçu  (3 premiers jours par bloc) :\n",
      "\n",
      "--- Bloc Deb ---\n",
      "Week 1 Day 1: pos1=Lecture du Tableau de nombres | pos2=Activité avec bâtonnets et paquet | pos3=Gymn aux nombres | pos4=Exercices d’opérations\n",
      "Week 1 Day 2: pos1=Lecture du Tableau de nombres | pos2=Lecture de la Table d’addition | pos3=Opération de base avec problèmes d'addition | pos4=Exercices d’opérations\n",
      "Week 1 Day 3: pos1=Lecture du Tableau de nombres | pos2=Lecture de la Table d’addition | pos3=Opération de base avec problèmes d'addition | pos4=Exercices d’opérations\n",
      "\n",
      "--- Bloc Deuxchiffres ---\n",
      "Week 1 Day 1: pos1=Lecture du Tableau de nombres | pos2=Activité avec bâtonnets et paquet | pos3=Gymn aux nombres | pos4=Exercices d’opérations\n",
      "Week 1 Day 2: pos1=Lecture du Tableau de nombres | pos2=Lecture de la Table d’addition | pos3=Opération de base avec problèmes d'addition | pos4=Exercices d’opérations\n",
      "Week 1 Day 3: pos1=Lecture de la Table d’addition | pos2=Opération de base avec problèmes d'addition | pos3=Mind map Addition | pos4=Exercices d’opérations\n",
      "\n",
      "--- Bloc TroisA ---\n",
      "Week 1 Day 1: pos1=Lecture du Tableau de nombres | pos2=Activité avec bâtonnets et paquet | pos3=Saut aux nombres | pos4=Exercices d’opérations\n",
      "Week 1 Day 2: pos1=Lecture du Tableau de nombres | pos2=Lecture de la Table d’addition | pos3=Opération de base avec problèmes d'addition | pos4=Exercices d’opérations\n",
      "Week 1 Day 3: pos1=Lecture de la Table d’addition | pos2=Opération de base avec problèmes d'addition | pos3=Mind map Addition | pos4=Exercices d’opérations\n",
      "\n",
      "--- Bloc TroisB ---\n",
      "Week 1 Day 1: pos1=Concept de la Multiplication | pos2=Multiplication par échelle | pos3=Lecture de la Table de multiplication | pos4=Exercices d'opérations\n",
      "Week 1 Day 2: pos1=Lecture de la Table de multiplication | pos2=Opération de base avec problèmes de multiplication | pos3=Roue des nombres | pos4=Exercices d'opérations\n",
      "Week 1 Day 3: pos1=Lecture de la Table de multiplication | pos2=Multiplication par échelle | pos3=Opération de base avec problèmes de multiplication | pos4=Exercices d'opérations\n",
      "\n",
      "--- Bloc Unchiffe ---\n",
      "Week 1 Day 1: pos1=Lecture du Tableau de nombres | pos2=Activité avec bâtonnets et paquet | pos3=Gymn aux nombres | pos4=Exercices d’opérations\n",
      "Week 1 Day 2: pos1=Lecture du Tableau de nombres | pos2=Lecture de la Table d’addition | pos3=Opération de base avec problèmes d'addition | pos4=Exercices d’opérations\n",
      "Week 1 Day 3: pos1=Lecture du Tableau de nombres | pos2=Lecture de la Table d’addition | pos3=Opération de base avec problèmes d'addition | pos4=Exercices d’opérations\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "OUT_DIR = \"./out/202602/out_pomdp\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def out_path(fname: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"L0-{fname}\")\n",
    "\n",
    "L1_ACTS_CLEAN = out_path(\"L1_activities_clean.csv\")\n",
    "L1_COLBLOCK   = out_path(\"L1_activity_column_blocks.csv\")\n",
    "\n",
    "assert os.path.exists(L1_ACTS_CLEAN), \"Exécute d’abord la SECTION 1 (L1_activities_clean.csv introuvable).\"\n",
    "assert os.path.exists(L1_COLBLOCK),   \"Exécute d’abord la SECTION 1 (L1_activity_column_blocks.csv introuvable).\"\n",
    "\n",
    "df_acts = pd.read_csv(L1_ACTS_CLEAN)\n",
    "df_colblock = pd.read_csv(L1_COLBLOCK)\n",
    "\n",
    "# Attendu: colonnes Week/Day\n",
    "assert \"Week\" in df_acts.columns and \"Day\" in df_acts.columns, \"Week/Day introuvables dans L1_activities_clean.csv\"\n",
    "\n",
    "# ============================================================\n",
    "# 1) IDENTIFIER BLOCS + ACTIVITÉS\n",
    "# ============================================================\n",
    "activity_cols = df_colblock[\"ActivityColumn\"].astype(str).tolist()\n",
    "col_to_block = dict(zip(df_colblock[\"ActivityColumn\"].astype(str), df_colblock[\"Block\"].astype(str)))\n",
    "\n",
    "blocks = sorted(df_colblock[\"Block\"].unique().tolist())\n",
    "print(\"Blocs détectés:\", blocks)\n",
    "\n",
    "# Activités par bloc\n",
    "block_to_cols = {b: [c for c in activity_cols if col_to_block.get(c) == b] for b in blocks}\n",
    "\n",
    "# Fonction: extraire “nom d’activité” (sans préfixe bloc)\n",
    "def activity_name(col: str) -> str:\n",
    "    s = str(col).strip()\n",
    "    # retire le préfixe \"Bloc \"\n",
    "    # ex: \"Deb Lecture du Tableau de nombres\" -> \"Lecture du Tableau de nombres\"\n",
    "    parts = s.split(\" \", 1)\n",
    "    return parts[1].strip() if len(parts) > 1 else s\n",
    "\n",
    "block_to_activities = {b: [activity_name(c) for c in block_to_cols[b]] for b in blocks}\n",
    "\n",
    "# ============================================================\n",
    "# 2) RECONSTRUIRE SEQUENCE (pos1..pos4) POUR UN JOUR ET UN BLOC\n",
    "# ============================================================\n",
    "def extract_day_sequence(row: pd.Series, cols_for_block: list):\n",
    "    \"\"\"\n",
    "    Retourne:\n",
    "      - seq = {pos1:act, pos2:act, pos3:act, pos4:act}\n",
    "      - quality flags (missing positions, duplicate positions, etc.)\n",
    "    \"\"\"\n",
    "    # valeurs dans les colonnes du bloc\n",
    "    vals = row[cols_for_block].values\n",
    "    # map position -> indices d’activités (peut être 0, 1 ou plusieurs si data anormale)\n",
    "    pos_to_idx = {p: np.where(vals == p)[0].tolist() for p in [1,2,3,4]}\n",
    "\n",
    "    seq = {}\n",
    "    issues = []\n",
    "    for p in [1,2,3,4]:\n",
    "        idxs = pos_to_idx[p]\n",
    "        if len(idxs) == 0:\n",
    "            seq[f\"pos{p}\"] = None\n",
    "            issues.append(f\"missing_pos{p}\")\n",
    "        elif len(idxs) > 1:\n",
    "            # anomalie: plusieurs activités marquées même position\n",
    "            seq[f\"pos{p}\"] = activity_name(cols_for_block[idxs[0]])\n",
    "            issues.append(f\"multi_pos{p}\")\n",
    "        else:\n",
    "            seq[f\"pos{p}\"] = activity_name(cols_for_block[idxs[0]])\n",
    "\n",
    "    # contrôle: nb activités exécutées (= count of vals in {1,2,3,4})\n",
    "    executed = int(np.sum(np.isin(vals, [1,2,3,4])))\n",
    "    if executed != 4:\n",
    "        issues.append(f\"executed_count={executed}\")\n",
    "\n",
    "    return seq, issues\n",
    "\n",
    "# ============================================================\n",
    "# 3) CONSTRUIRE TABLES LONGUES (par jour, par bloc)\n",
    "# ============================================================\n",
    "rows_long = []\n",
    "rows_list = []\n",
    "rows_vector = []\n",
    "\n",
    "for _, r in df_acts.iterrows():\n",
    "    wk = int(r[\"Week\"]) if pd.notna(r[\"Week\"]) else None\n",
    "    dy = int(r[\"Day\"]) if pd.notna(r[\"Day\"]) else None\n",
    "\n",
    "    for b in blocks:\n",
    "        cols_b = block_to_cols[b]\n",
    "        if not cols_b:\n",
    "            continue\n",
    "\n",
    "        seq, issues = extract_day_sequence(r, cols_b)\n",
    "\n",
    "        # représentation liste ordonnée [pos1,pos2,pos3,pos4]\n",
    "        seq_list = [seq[\"pos1\"], seq[\"pos2\"], seq[\"pos3\"], seq[\"pos4\"]]\n",
    "\n",
    "        # représentation “vecteur 0..4” sur toutes les activités du bloc\n",
    "        # (même logique que ton format Day i: [0,4,0,2,...])\n",
    "        # chaque entrée = 0 si non exécutée, sinon position 1..4\n",
    "        v = []\n",
    "        for c in cols_b:\n",
    "            x = r[c]\n",
    "            if pd.isna(x):\n",
    "                v.append(0)\n",
    "            else:\n",
    "                xi = int(x)\n",
    "                v.append(xi if xi in [1,2,3,4] else 0)\n",
    "\n",
    "        rows_long.append({\n",
    "            \"Week\": wk, \"Day\": dy, \"Block\": b,\n",
    "            \"pos1\": seq[\"pos1\"], \"pos2\": seq[\"pos2\"], \"pos3\": seq[\"pos3\"], \"pos4\": seq[\"pos4\"],\n",
    "            \"Issues\": \"|\".join(issues) if issues else \"\"\n",
    "        })\n",
    "\n",
    "        rows_list.append({\n",
    "            \"Week\": wk, \"Day\": dy, \"Block\": b,\n",
    "            \"Sequence\": json.dumps(seq_list, ensure_ascii=False),\n",
    "            \"Issues\": \"|\".join(issues) if issues else \"\"\n",
    "        })\n",
    "\n",
    "        # vector row (wide)\n",
    "        row_vec = {\"Week\": wk, \"Day\": dy, \"Block\": b, \"Issues\": \"|\".join(issues) if issues else \"\"}\n",
    "        # noms de colonnes lisibles\n",
    "        for i, c in enumerate(cols_b, start=1):\n",
    "            row_vec[f\"act_{i}:{activity_name(c)}\"] = v[i-1]\n",
    "        rows_vector.append(row_vec)\n",
    "\n",
    "df_long = pd.DataFrame(rows_long)\n",
    "df_list = pd.DataFrame(rows_list)\n",
    "df_vec  = pd.DataFrame(rows_vector)\n",
    "\n",
    "# ============================================================\n",
    "# 4) STATS QUALITE PAR BLOC\n",
    "# ============================================================\n",
    "def issues_stats(df_long_block: pd.DataFrame):\n",
    "    issues = df_long_block[\"Issues\"].fillna(\"\").astype(str)\n",
    "    total = len(df_long_block)\n",
    "    n_ok = int((issues == \"\").sum())\n",
    "    n_bad = total - n_ok\n",
    "\n",
    "    # compter chaque type d’issue\n",
    "    counter = {}\n",
    "    for s in issues:\n",
    "        if not s:\n",
    "            continue\n",
    "        parts = s.split(\"|\")\n",
    "        for p in parts:\n",
    "            counter[p] = counter.get(p, 0) + 1\n",
    "\n",
    "    return {\"total_days\": total, \"ok\": n_ok, \"bad\": n_bad, \"issue_counts\": counter}\n",
    "\n",
    "block_quality = {b: issues_stats(df_long[df_long[\"Block\"] == b]) for b in blocks}\n",
    "\n",
    "print(\"\\nQualité par bloc (résumé):\")\n",
    "for b in blocks:\n",
    "    q = block_quality[b]\n",
    "    print(f\" - {b}: total={q['total_days']} ok={q['ok']} bad={q['bad']}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) SAUVEGARDES\n",
    "# ============================================================\n",
    "df_long.to_csv(out_path(\"SECTION2_L1_sequences_by_block_long.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "df_list.to_csv(out_path(\"SECTION2_L1_sequences_by_block_list.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "df_vec.to_csv(out_path(\"SECTION2_L1_sequences_by_block_vector.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "with open(out_path(\"SECTION2_block_to_activities.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(block_to_activities, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(out_path(\"SECTION2_block_quality.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(block_quality, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n SECTION 2 terminée.\")\n",
    "print(\"Fichiers produits:\")\n",
    "print(\" -\", out_path(\"SECTION2_L1_sequences_by_block_long.csv\"))\n",
    "print(\" -\", out_path(\"SECTION2_L1_sequences_by_block_list.csv\"))\n",
    "print(\" -\", out_path(\"SECTION2_L1_sequences_by_block_vector.csv\"))\n",
    "print(\" -\", out_path(\"SECTION2_block_to_activities.json\"))\n",
    "print(\" -\", out_path(\"SECTION2_block_quality.json\"))\n",
    "\n",
    "# ============================================================\n",
    "# 6) (OPTION) APERCU  : afficher 3 jours pour chaque bloc\n",
    "# ============================================================\n",
    "print(\"\\nAperçu  (3 premiers jours par bloc) :\")\n",
    "for b in blocks:\n",
    "    sub = df_long[df_long[\"Block\"] == b].head(3)\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    print(f\"\\n--- Bloc {b} ---\")\n",
    "    for _, rr in sub.iterrows():\n",
    "        print(f\"Week {rr['Week']} Day {rr['Day']}: pos1={rr['pos1']} | pos2={rr['pos2']} | pos3={rr['pos3']} | pos4={rr['pos4']}\"\n",
    "              + (f\"  [Issues: {rr['Issues']}]\" if rr['Issues'] else \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d6cea30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Diag] LevelTag -> Level (à partir de L3 / Pretest_i):\n",
      "LevelTag  DominantLevel  DominantShare     Counts\n",
      "      L1              1            1.0   {\"1\": 4}\n",
      "      L2              2            1.0  {\"2\": 66}\n",
      "      L3              3            1.0 {\"3\": 166}\n",
      "      L4              4            1.0 {\"4\": 230}\n",
      "      L5              5            1.0 {\"5\": 347}\n",
      "\n",
      " Mapping Block -> Level produit :\n",
      "       Block  Level MatchedLevelTag  Similarity                            Method\n",
      "         Deb      1              L1         0.0 text_similarity_block_vs_leveltag\n",
      "Deuxchiffres      1              L1         0.0 text_similarity_block_vs_leveltag\n",
      "      TroisA      1              L1         0.0 text_similarity_block_vs_leveltag\n",
      "      TroisB      1              L1         0.0 text_similarity_block_vs_leveltag\n",
      "    Unchiffe      1              L1         0.0 text_similarity_block_vs_leveltag\n",
      "\n",
      "[Check] TroisA -> Level 1 (selon mapping)\n",
      "\n",
      "Fichiers produits:\n",
      " - ./out/202602/out_pomdp\\L0-SECTION3_block_to_level.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION3_block_to_level.json\n",
      " - ./out/202602/out_pomdp\\L0-SECTION3_leveltag_to_level.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "OUT_DIR = \"./out/202602/out_pomdp\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def out_path(fname: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"L0-{fname}\")\n",
    "\n",
    "# Outputs SECTION2\n",
    "SEQ_LONG = out_path(\"SECTION2_L1_sequences_by_block_long.csv\")\n",
    "COLBLOCK = out_path(\"L1_activity_column_blocks.csv\")          # from SECTION1\n",
    "BLOCK_ACTS_JSON = out_path(\"SECTION2_block_to_activities.json\")  # from SECTION2\n",
    "\n",
    "# L3\n",
    "L3_PATH = \"./data/2025-08/L3.features_by_student.byPretest.csv\"  # adapte si besoin\n",
    "assert os.path.exists(L3_PATH), f\"Introuvable L3 : {L3_PATH}\"\n",
    "\n",
    "assert os.path.exists(SEQ_LONG), \"Exécute SECTION 2 d’abord (SECTION2_L1_sequences_by_block_long.csv introuvable).\"\n",
    "assert os.path.exists(COLBLOCK), \"Exécute SECTION 1 d’abord (L1_activity_column_blocks.csv introuvable).\"\n",
    "assert os.path.exists(BLOCK_ACTS_JSON), \"Exécute SECTION 2 d’abord (SECTION2_block_to_activities.json introuvable).\"\n",
    "\n",
    "df_seq_long = pd.read_csv(SEQ_LONG)\n",
    "df_colblock = pd.read_csv(COLBLOCK)\n",
    "df_l3 = pd.read_csv(L3_PATH)\n",
    "\n",
    "# ============================================================\n",
    "# 1) HELPERS\n",
    "# ============================================================\n",
    "def norm_text(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"[’'`]\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def sim(a: str, b: str) -> float:\n",
    "    return SequenceMatcher(None, norm_text(a), norm_text(b)).ratio()\n",
    "\n",
    "def cap_level(x):\n",
    "    try:\n",
    "        v = int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return min(5, max(1, v))\n",
    "\n",
    "# Assure Pretest_i dans L3\n",
    "if \"Pretest_i\" not in df_l3.columns:\n",
    "    if \"Pretest\" in df_l3.columns:\n",
    "        df_l3[\"Pretest_i\"] = df_l3[\"Pretest\"].apply(cap_level)\n",
    "    else:\n",
    "        raise ValueError(\"L3 doit contenir Pretest (ou Pretest_i).\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) (A) Déduire LevelTag -> Level via distribution Pretest\n",
    "# ============================================================\n",
    "leveltag_to_level = {}\n",
    "leveltag_diag_rows = []\n",
    "\n",
    "if \"LevelTag\" in df_l3.columns:\n",
    "    for tag, g in df_l3.dropna(subset=[\"LevelTag\",\"Pretest_i\"]).groupby(\"LevelTag\"):\n",
    "        # niveau dominant (mode) dans ce tag\n",
    "        counts = g[\"Pretest_i\"].astype(int).value_counts().sort_index()\n",
    "        if len(counts) == 0:\n",
    "            continue\n",
    "        dominant_level = int(counts.idxmax())\n",
    "        share = float(counts.max() / counts.sum())\n",
    "        leveltag_to_level[str(tag)] = dominant_level\n",
    "        leveltag_diag_rows.append({\n",
    "            \"LevelTag\": str(tag),\n",
    "            \"DominantLevel\": dominant_level,\n",
    "            \"DominantShare\": round(share, 4),\n",
    "            \"Counts\": json.dumps({int(k): int(v) for k,v in counts.to_dict().items()})\n",
    "        })\n",
    "\n",
    "df_leveltag_diag = pd.DataFrame(leveltag_diag_rows).sort_values(\n",
    "    [\"DominantLevel\",\"DominantShare\"], ascending=[True, False]\n",
    ")\n",
    "\n",
    "df_leveltag_diag.to_csv(out_path(\"SECTION3_leveltag_to_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[Diag] LevelTag -> Level (à partir de L3 / Pretest_i):\")\n",
    "if len(df_leveltag_diag) == 0:\n",
    "    print(\" - Aucun LevelTag exploitable trouvé dans L3 (colonne LevelTag absente ou vide).\")\n",
    "else:\n",
    "    print(df_leveltag_diag.head(30).to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# 3) (B) Mapper Block (L1) -> Level via matching texte Block <-> LevelTag\n",
    "# ============================================================\n",
    "blocks = sorted(df_colblock[\"Block\"].astype(str).unique().tolist())\n",
    "\n",
    "# si pas de LevelTag dans L3, fallback: mapping ordinal par fréquence d’apparition\n",
    "# (moins fiable, mais au moins automatique)\n",
    "has_leveltag = (\"LevelTag\" in df_l3.columns) and (len(df_leveltag_diag) > 0)\n",
    "\n",
    "mapping_rows = []\n",
    "block_to_level = {}\n",
    "\n",
    "# Option: override manuel si tu veux forcer certains cas après inspection\n",
    "MANUAL_OVERRIDE_BLOCK_TO_LEVEL = {\n",
    "    # Exemple (à compléter si besoin):\n",
    "    # \"TroisA\": 4,\n",
    "    # \"TroisB\": 5,\n",
    "}\n",
    "\n",
    "if has_leveltag:\n",
    "    tags = list(leveltag_to_level.keys())\n",
    "\n",
    "    for b in blocks:\n",
    "        if b in MANUAL_OVERRIDE_BLOCK_TO_LEVEL:\n",
    "            lvl = int(MANUAL_OVERRIDE_BLOCK_TO_LEVEL[b])\n",
    "            block_to_level[b] = lvl\n",
    "            mapping_rows.append({\n",
    "                \"Block\": b,\n",
    "                \"MatchedLevelTag\": \"__MANUAL_OVERRIDE__\",\n",
    "                \"Level\": lvl,\n",
    "                \"Similarity\": 1.0,\n",
    "                \"Method\": \"manual_override\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # meilleur match par similarité texte\n",
    "        scored = [(t, sim(b, t)) for t in tags]\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        best_tag, best_score = scored[0]\n",
    "        lvl = int(leveltag_to_level[best_tag])\n",
    "\n",
    "        # garde aussi les 3 meilleurs pour audit\n",
    "        top3 = scored[:3]\n",
    "        mapping_rows.append({\n",
    "            \"Block\": b,\n",
    "            \"MatchedLevelTag\": best_tag,\n",
    "            \"Level\": lvl,\n",
    "            \"Similarity\": round(float(best_score), 4),\n",
    "            \"Top3\": json.dumps([(t, round(float(sc),4)) for t,sc in top3], ensure_ascii=False),\n",
    "            \"Method\": \"text_similarity_block_vs_leveltag\"\n",
    "        })\n",
    "        block_to_level[b] = lvl\n",
    "\n",
    "else:\n",
    "    # Fallback: ordonner les blocs par “activité dans le temps”:\n",
    "    # hypothèse: les blocs “débutants” apparaissent plus tôt (Week/Day petits)\n",
    "    # => on calcule pour chaque block la médiane du (Week,Day) quand il est réellement utilisé (>=1)\n",
    "    # puis on mappe les 5 premiers rangs -> niveaux 1..5\n",
    "    print(\"\\n[WARN] Fallback sans LevelTag: mapping ordinal basé sur la temporalité (moins fiable).\")\n",
    "\n",
    "    # construit un score “timing” par block\n",
    "    timing = []\n",
    "    # utilisation d’un jour si au moins une des pos1..pos4 est non nulle\n",
    "    used = df_seq_long.copy()\n",
    "    used[\"is_used\"] = used[[\"pos1\",\"pos2\",\"pos3\",\"pos4\"]].notna().any(axis=1).astype(int)\n",
    "\n",
    "    for b in blocks:\n",
    "        gb = used[(used[\"Block\"] == b) & (used[\"is_used\"] == 1)]\n",
    "        if len(gb) == 0:\n",
    "            # si jamais bloc jamais utilisé (rare), on met un grand score\n",
    "            med = 10**9\n",
    "        else:\n",
    "            # score = median(Week*100 + Day)\n",
    "            med = float(np.median(gb[\"Week\"].fillna(0).values * 100 + gb[\"Day\"].fillna(0).values))\n",
    "        timing.append((b, med))\n",
    "\n",
    "    timing.sort(key=lambda x: x[1])\n",
    "    # si >5 blocs, on “compresse” sur 1..5 par quantiles\n",
    "    meds = np.array([t[1] for t in timing], dtype=float)\n",
    "    if len(meds) == 0:\n",
    "        raise ValueError(\"Impossible de construire un mapping (aucun bloc détecté).\")\n",
    "\n",
    "    # quantiles\n",
    "    q = np.quantile(meds, [0.2, 0.4, 0.6, 0.8])\n",
    "    def quantile_to_level(m):\n",
    "        if m <= q[0]: return 1\n",
    "        if m <= q[1]: return 2\n",
    "        if m <= q[2]: return 3\n",
    "        if m <= q[3]: return 4\n",
    "        return 5\n",
    "\n",
    "    for b, med in timing:\n",
    "        if b in MANUAL_OVERRIDE_BLOCK_TO_LEVEL:\n",
    "            lvl = int(MANUAL_OVERRIDE_BLOCK_TO_LEVEL[b])\n",
    "            method = \"manual_override\"\n",
    "        else:\n",
    "            lvl = quantile_to_level(med)\n",
    "            method = \"timing_quantile\"\n",
    "        block_to_level[b] = lvl\n",
    "        mapping_rows.append({\n",
    "            \"Block\": b,\n",
    "            \"MatchedLevelTag\": \"\",\n",
    "            \"Level\": lvl,\n",
    "            \"Similarity\": \"\",\n",
    "            \"Top3\": \"\",\n",
    "            \"Method\": method,\n",
    "            \"MedianWeekDayScore\": med\n",
    "        })\n",
    "\n",
    "df_map = pd.DataFrame(mapping_rows)\n",
    "\n",
    "df_map.to_csv(out_path(\"SECTION3_block_to_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "with open(out_path(\"SECTION3_block_to_level.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(block_to_level, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n Mapping Block -> Level produit :\")\n",
    "print(df_map[[\"Block\",\"Level\",\"MatchedLevelTag\",\"Similarity\",\"Method\"]].to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# 4) (C) Vérif  : cohérence TroisA/TroisB (si présents)\n",
    "# ============================================================\n",
    "for key in [\"TroisA\", \"TroisB\", \"troisA\", \"troisB\"]:\n",
    "    # on normalise par exact match “Block”\n",
    "    if key in block_to_level:\n",
    "        print(f\"\\n[Check] {key} -> Level {block_to_level[key]} (selon mapping)\")\n",
    "        break\n",
    "\n",
    "print(\"\\nFichiers produits:\")\n",
    "print(\" -\", out_path(\"SECTION3_block_to_level.csv\"))\n",
    "print(\" -\", out_path(\"SECTION3_block_to_level.json\"))\n",
    "print(\" -\", out_path(\"SECTION3_leveltag_to_level.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "efaf048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== SECTION 4 — Action label (A/B) via HoursTotal (CORRIGÉ) ====================\n",
      "HOURS_PER_DAY = 2.0h (séquence journalière), STANDARD_HOURS_A≈20.0h (10 jours)\n",
      "Règle de labellisation observée: Action=A si DaysTotal >= 9.0 jours (soit >= 18.0h), sinon B\n",
      "\n",
      "Aperçu HoursTotal/DaysTotal/Action_obs (head 15):\n",
      " HoursTotal  DaysTotal Action_obs\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "         60       30.0          A\n",
      "\n",
      "Répartition Action_obs:\n",
      "Action_obs\n",
      "A    813\n",
      "\n",
      "[Hypothèse Elysa] Répartition observée (Action_obs) par niveau Pretest:\n",
      " Level   N  P(A)  P(B)\n",
      "     1   4   1.0   0.0\n",
      "     2  66   1.0   0.0\n",
      "     3 166   1.0   0.0\n",
      "     4 230   1.0   0.0\n",
      "     5 347   1.0   0.0\n",
      "\n",
      "==================== SECTION 4 — Policy ML ====================\n",
      "Impossible d'entraîner π_ML : y ne contient qu'une seule classe (A uniquement ou B uniquement).\n",
      "On sort une politique constante: P(A)= 1.0\n",
      "\n",
      "Fichiers produits (Section 4):\n",
      " - ./out/202602/out_pomdp\\L0-SECTION4_L3_with_action_labels.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION4_policyML_model_selection.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION4_policyML_predictions.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION4_hypothesis_obs_by_level.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION4_hypothesis_piML_by_level.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION4_hypothesis_support_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning) # Ignore uniquement les warnings de fragmentation\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# optional: xgboost\n",
    "xgb_ok = True\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception:\n",
    "    xgb_ok = False\n",
    "\n",
    "# persist model\n",
    "try:\n",
    "    import joblib\n",
    "    joblib_ok = True\n",
    "except Exception:\n",
    "    joblib_ok = False\n",
    "\n",
    "OUT_DIR = \"./out/202602/out_pomdp\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def out_path(fname: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"L0-{fname}\")\n",
    "\n",
    "L3_PATH = \"./data/2025-08/L3.features_by_student.byPretest.csv\"  # adapte si besoin\n",
    "assert os.path.exists(L3_PATH), f\"Introuvable : {L3_PATH}\"\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "EPS = 1e-12\n",
    "\n",
    "# ============================================================\n",
    "#  — TEMPS (CORRIGÉ)\n",
    "# ============================================================\n",
    "# Séquence journalière = 4 activités = 2h TOTAL\n",
    "HOURS_PER_DAY = 2.0\n",
    "\n",
    "STANDARD_DAYS_A = 10.0\n",
    "STANDARD_HOURS_A = STANDARD_DAYS_A * HOURS_PER_DAY  # 20h (et non 80h)\n",
    "\n",
    "# seuil pour décider A vs B\n",
    "# -> proche de 10 jours, on tolère un peu d’écart (ex : 9 jours et + => A)\n",
    "THRESHOLD_DAYS_FOR_A = 9.0  # => 18h\n",
    "\n",
    "A, B = \"A\", \"B\"\n",
    "\n",
    "# ============================================================\n",
    "# 1) LOAD L3 + FEATURES\n",
    "# ============================================================\n",
    "df = pd.read_csv(L3_PATH)\n",
    "\n",
    "def cap_level(x):\n",
    "    try:\n",
    "        v = int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return min(5, max(1, v))\n",
    "\n",
    "if \"Pretest_i\" not in df.columns:\n",
    "    if \"Pretest\" in df.columns:\n",
    "        df[\"Pretest_i\"] = df[\"Pretest\"].apply(cap_level)\n",
    "    else:\n",
    "        raise ValueError(\"L3 doit contenir Pretest ou Pretest_i\")\n",
    "\n",
    "if \"HoursTotal\" not in df.columns:\n",
    "    raise ValueError(\"L3 doit contenir la colonne HoursTotal (tu as demandé de l'utiliser).\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) LABEL ACTION OBSERVÉE via HoursTotal -> DaysTotal\n",
    "# ============================================================\n",
    "# Correction: on divise par 2.0, pas par 8.0\n",
    "df[\"DaysTotal\"] = df[\"HoursTotal\"].astype(float) / float(HOURS_PER_DAY)\n",
    "\n",
    "df[\"Action_obs\"] = np.where(df[\"DaysTotal\"] >= THRESHOLD_DAYS_FOR_A, A, B)\n",
    "\n",
    "print(\"\\n==================== SECTION 4 — Action label (A/B) via HoursTotal (CORRIGÉ) ====================\")\n",
    "print(f\"HOURS_PER_DAY = {HOURS_PER_DAY:.1f}h (séquence journalière), STANDARD_HOURS_A≈{STANDARD_HOURS_A:.1f}h (10 jours)\")\n",
    "print(f\"Règle de labellisation observée: Action=A si DaysTotal >= {THRESHOLD_DAYS_FOR_A:.1f} jours (soit >= {THRESHOLD_DAYS_FOR_A*HOURS_PER_DAY:.1f}h), sinon B\")\n",
    "\n",
    "preview_cols = [c for c in [\"HoursTotal\", \"DaysTotal\", \"Action_obs\"] if c in df.columns]\n",
    "print(\"\\nAperçu HoursTotal/DaysTotal/Action_obs (head 15):\")\n",
    "print(df[preview_cols].head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\nRépartition Action_obs:\")\n",
    "print(df[\"Action_obs\"].value_counts(dropna=False).to_string())\n",
    "\n",
    "df.to_csv(out_path(\"SECTION4_L3_with_action_labels.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) TEST HYPOTHÈSE (Elysa) sur OBSERVÉ: niveaux 1-2 => A, niveaux 3-4-5 => B\n",
    "# ============================================================\n",
    "def summarize_action_by_level(df0: pd.DataFrame, label_col=\"Action_obs\"):\n",
    "    rows = []\n",
    "    dfx = df0.dropna(subset=[\"Pretest_i\"])\n",
    "    for lvl, g in dfx.groupby(\"Pretest_i\"):\n",
    "        lvl = int(lvl)\n",
    "        n = len(g)\n",
    "        pA = float((g[label_col] == A).mean()) if n > 0 else np.nan\n",
    "        pB = float((g[label_col] == B).mean()) if n > 0 else np.nan\n",
    "        rows.append({\"Level\": lvl, \"N\": n, \"P(A)\": round(pA, 4), \"P(B)\": round(pB, 4)})\n",
    "    return pd.DataFrame(rows).sort_values(\"Level\")\n",
    "\n",
    "df_hyp_obs = summarize_action_by_level(df, label_col=\"Action_obs\")\n",
    "print(\"\\n[Hypothèse Elysa] Répartition observée (Action_obs) par niveau Pretest:\")\n",
    "print(df_hyp_obs.to_string(index=False))\n",
    "df_hyp_obs.to_csv(out_path(\"SECTION4_hypothesis_obs_by_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) APPRENDRE π_ML(a|s,features)  (classification binaire A vs B)\n",
    "# ============================================================\n",
    "demographic_cols = [c for c in [\"Age\", \"Genre\", \"Zone\", \"LevelTag\"] if c in df.columns]\n",
    "state_col = \"Pretest_i\"\n",
    "freq_cols = [c for c in df.columns if c.startswith(\"freq_pos\") or c.startswith(\"freq_all\")]\n",
    "\n",
    "feature_cols = [state_col] + demographic_cols + freq_cols\n",
    "missing = [c for c in feature_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Colonnes manquantes dans L3 pour apprendre π_ML : {missing}\")\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = (df[\"Action_obs\"] == A).astype(int)  # 1 = A, 0 = B\n",
    "\n",
    "# Cas dégénéré: une seule classe dans tout le dataset\n",
    "unique_y = np.unique(y.dropna())\n",
    "if len(unique_y) < 2:\n",
    "    const = int(unique_y[0]) if len(unique_y) == 1 else 0\n",
    "    print(\"\\n==================== SECTION 4 — Policy ML ====================\")\n",
    "    print(\"Impossible d'entraîner π_ML : y ne contient qu'une seule classe (A uniquement ou B uniquement).\")\n",
    "    print(f\"On sort une politique constante: P(A)= {1.0 if const==1 else 0.0}\")\n",
    "\n",
    "    df[\"piML_P(A)\"] = 1.0 if const == 1 else 0.0\n",
    "    df[\"piML_P_A\"] = df[\"piML_P(A)\"]  # alias pratique\n",
    "    df[\"piML_Action_hat\"] = A if const == 1 else B\n",
    "\n",
    "    pred_cols = [\"Pretest_i\", \"HoursTotal\", \"DaysTotal\", \"Action_obs\", \"piML_P(A)\", \"piML_Action_hat\"]\n",
    "    for c in [\"Age\", \"Genre\", \"Zone\", \"LevelTag\"]:\n",
    "        if c in df.columns:\n",
    "            pred_cols.insert(1, c)\n",
    "\n",
    "    df[pred_cols].to_csv(out_path(\"SECTION4_policyML_predictions.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    df_hyp_ml = summarize_action_by_level(df, label_col=\"piML_Action_hat\")\n",
    "    df_hyp_ml.to_csv(out_path(\"SECTION4_hypothesis_piML_by_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # export meta minimal\n",
    "    with open(out_path(\"SECTION4_best_policyML_model_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"best_model_name\": \"ConstantPolicy\",\n",
    "            \"constant_class_A_is_1\": bool(const == 1),\n",
    "            \"threshold_days_for_A\": THRESHOLD_DAYS_FOR_A,\n",
    "            \"hours_per_day\": HOURS_PER_DAY,\n",
    "            \"standard_days_A\": STANDARD_DAYS_A\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "else:\n",
    "    # preprocessing\n",
    "    num_cols = []\n",
    "    cat_cols = []\n",
    "    for c in X.columns:\n",
    "        if c in [\"Genre\", \"Zone\", \"LevelTag\"]:\n",
    "            cat_cols.append(c)\n",
    "        else:\n",
    "            num_cols.append(c)\n",
    "\n",
    "    preproc = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline([\n",
    "                (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"sc\", StandardScaler())\n",
    "            ]), num_cols),\n",
    "            (\"cat\", Pipeline([\n",
    "                (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "            ]), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    candidates = {\n",
    "        \"RandomForestClassifier\": Pipeline([\n",
    "            (\"prep\", preproc),\n",
    "            (\"m\", RandomForestClassifier(\n",
    "                n_estimators=250, random_state=RANDOM_STATE,\n",
    "                max_depth=12, n_jobs=1\n",
    "            ))\n",
    "        ]),\n",
    "        \"GradientBoostingClassifier\": Pipeline([\n",
    "            (\"prep\", preproc),\n",
    "            (\"m\", GradientBoostingClassifier(\n",
    "                random_state=RANDOM_STATE, n_estimators=250\n",
    "            ))\n",
    "        ]),\n",
    "        \"LogisticRegression\": Pipeline([\n",
    "            (\"prep\", preproc),\n",
    "            (\"m\", LogisticRegression(max_iter=5000))\n",
    "        ]),\n",
    "        \"SVC\": Pipeline([\n",
    "            (\"prep\", preproc),\n",
    "            (\"m\", SVC(probability=True, random_state=RANDOM_STATE))\n",
    "        ]),\n",
    "        \"XGBoostClassifier\": Pipeline([\n",
    "            (\"prep\", preproc),\n",
    "            (\"m\", xgb.XGBClassifier(\n",
    "                n_estimators=300, learning_rate=0.07, max_depth=5,\n",
    "                subsample=0.9, colsample_bytree=0.9,\n",
    "                random_state=RANDOM_STATE, n_jobs=1,\n",
    "                eval_metric=\"logloss\"\n",
    "            ) if xgb_ok else HistGradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "        ])\n",
    "    }\n",
    "    if not xgb_ok:\n",
    "        del candidates[\"XGBoostClassifier\"]\n",
    "        candidates[\"HistGradientBoostingClassifier\"] = Pipeline([\n",
    "             (\"prep\", preproc),\n",
    "             (\"m\", HistGradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "        ])\n",
    "\n",
    "    def safe_proba_of_class1(model, Xte):\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(Xte)\n",
    "            proba = np.asarray(proba)\n",
    "            if proba.ndim == 2 and proba.shape[1] >= 2:\n",
    "                return proba[:, 1]\n",
    "            if hasattr(model, \"classes_\") and len(getattr(model, \"classes_\", [])) == 1:\n",
    "                only = int(model.classes_[0])\n",
    "                return np.ones(len(Xte)) * float(only)\n",
    "            return np.zeros(len(Xte), dtype=float)\n",
    "        pred = model.predict(Xte)\n",
    "        return pred.astype(float)\n",
    "\n",
    "    def eval_policy_ml_cv(X, y, pipe, n_splits=5):\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "        probs_all, preds_all, y_all = [], [], []\n",
    "        for tr, te in skf.split(X, y):\n",
    "            m = clone(pipe)\n",
    "            m.fit(X.iloc[tr], y.iloc[tr])\n",
    "\n",
    "            pr = safe_proba_of_class1(m, X.iloc[te])\n",
    "            pr = np.asarray(pr).reshape(-1)\n",
    "\n",
    "            pd_ = (pr >= 0.5).astype(int)\n",
    "            probs_all.extend(pr.tolist())\n",
    "            preds_all.extend(pd_.tolist())\n",
    "            y_all.extend(y.iloc[te].tolist())\n",
    "\n",
    "        probs_all = np.array(probs_all)\n",
    "        preds_all = np.array(preds_all)\n",
    "        y_all = np.array(y_all)\n",
    "\n",
    "        auc = np.nan\n",
    "        if len(np.unique(y_all)) > 1:\n",
    "            try:\n",
    "                auc = float(roc_auc_score(y_all, probs_all))\n",
    "            except Exception:\n",
    "                auc = np.nan\n",
    "\n",
    "        return {\n",
    "            \"AUC\": auc,\n",
    "            \"F1\": float(f1_score(y_all, preds_all, zero_division=0)),\n",
    "            \"ACC\": float(accuracy_score(y_all, preds_all))\n",
    "        }\n",
    "\n",
    "    print(\"\\n==================== SECTION 4 — Model selection for π_ML(a|s,features) ====================\")\n",
    "    rows = []\n",
    "    for name, pipe in candidates.items():\n",
    "        sc = eval_policy_ml_cv(X, y, pipe, n_splits=5)\n",
    "        rows.append({\"Model\": name,\n",
    "                     \"AUC\": (round(sc[\"AUC\"], 4) if isinstance(sc[\"AUC\"], float) and not np.isnan(sc[\"AUC\"]) else sc[\"AUC\"]),\n",
    "                     \"F1\": round(sc[\"F1\"], 4),\n",
    "                     \"ACC\": round(sc[\"ACC\"], 4)})\n",
    "\n",
    "    df_sel = pd.DataFrame(rows)\n",
    "\n",
    "    def _auc_key(v):\n",
    "        try:\n",
    "            return float(v)\n",
    "        except Exception:\n",
    "            return -np.inf\n",
    "\n",
    "    df_sel[\"_AUC_key\"] = df_sel[\"AUC\"].apply(lambda v: _auc_key(v) if v is not None and not (isinstance(v, float) and np.isnan(v)) else -np.inf)\n",
    "    df_sel = df_sel.sort_values([\"_AUC_key\", \"F1\", \"ACC\"], ascending=False).drop(columns=[\"_AUC_key\"])\n",
    "\n",
    "    print(df_sel.to_string(index=False))\n",
    "    df_sel.to_csv(out_path(\"SECTION4_policyML_model_selection.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    best_model_name = df_sel.iloc[0][\"Model\"]\n",
    "    best_pipe = candidates[best_model_name]\n",
    "    best_pipe.fit(X, y)\n",
    "\n",
    "    print(f\"\\nBest π_ML model = {best_model_name}\")\n",
    "\n",
    "    if joblib_ok:\n",
    "        joblib.dump(best_pipe, out_path(\"SECTION4_best_policyML_model.joblib\"))\n",
    "        with open(out_path(\"SECTION4_best_policyML_model_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"best_model_name\": best_model_name,\n",
    "                \"threshold_days_for_A\": THRESHOLD_DAYS_FOR_A,\n",
    "                \"hours_per_day\": HOURS_PER_DAY,\n",
    "                \"standard_days_A\": STANDARD_DAYS_A\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        print(\"[WARN] joblib indisponible: le modèle n'est pas sérialisé.\")\n",
    "\n",
    "    # ============================================================\n",
    "    # 5) PRODUITS: π_ML prédictions + analyse hypothèse sur π_ML\n",
    "    # ============================================================\n",
    "    pA = safe_proba_of_class1(best_pipe, X)\n",
    "    pA = np.asarray(pA).reshape(-1)\n",
    "\n",
    "    df[\"piML_P(A)\"] = pA\n",
    "    df[\"piML_P_A\"] = df[\"piML_P(A)\"]\n",
    "    df[\"piML_Action_hat\"] = np.where(df[\"piML_P(A)\"] >= 0.5, A, B)\n",
    "\n",
    "    pred_cols = [\"Pretest_i\", \"HoursTotal\", \"DaysTotal\", \"Action_obs\", \"piML_P(A)\", \"piML_Action_hat\"]\n",
    "    for c in [\"Age\", \"Genre\", \"Zone\", \"LevelTag\"]:\n",
    "        if c in df.columns:\n",
    "            pred_cols.insert(1, c)\n",
    "\n",
    "    df[pred_cols].to_csv(out_path(\"SECTION4_policyML_predictions.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    df_hyp_ml = summarize_action_by_level(df, label_col=\"piML_Action_hat\")\n",
    "    print(\"\\n[Hypothèse Elysa] Décision π_ML (Action_hat) par niveau Pretest:\")\n",
    "    print(df_hyp_ml.to_string(index=False))\n",
    "    df_hyp_ml.to_csv(out_path(\"SECTION4_hypothesis_piML_by_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    support_rows = []\n",
    "    for group_name, levels, expect_A in [(\"Low (1-2)\", [1, 2], True), (\"High (3-5)\", [3, 4, 5], False)]:\n",
    "        g = df[df[\"Pretest_i\"].isin(levels)]\n",
    "        if len(g) == 0:\n",
    "            continue\n",
    "        pA_obs = float((g[\"Action_obs\"] == A).mean())\n",
    "        pA_ml = float((g[\"piML_Action_hat\"] == A).mean())\n",
    "        support_rows.append({\n",
    "            \"Group\": group_name,\n",
    "            \"N\": int(len(g)),\n",
    "            \"Obs P(A)\": round(pA_obs, 4),\n",
    "            \"π_ML P(A_hat)\": round(pA_ml, 4),\n",
    "            \"Expected\": (\"A\" if expect_A else \"B\")\n",
    "        })\n",
    "\n",
    "    df_support = pd.DataFrame(support_rows)\n",
    "    print(\"\\n[Hypothèse Elysa] Synthèse groupée:\")\n",
    "    print(df_support.to_string(index=False))\n",
    "    df_support.to_csv(out_path(\"SECTION4_hypothesis_support_summary.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\nFichiers produits (Section 4):\")\n",
    "print(\" -\", out_path(\"SECTION4_L3_with_action_labels.csv\"))\n",
    "print(\" -\", out_path(\"SECTION4_policyML_model_selection.csv\"))\n",
    "print(\" -\", out_path(\"SECTION4_policyML_predictions.csv\"))\n",
    "print(\" -\", out_path(\"SECTION4_hypothesis_obs_by_level.csv\"))\n",
    "print(\" -\", out_path(\"SECTION4_hypothesis_piML_by_level.csv\"))\n",
    "print(\" -\", out_path(\"SECTION4_hypothesis_support_summary.csv\"))\n",
    "if joblib_ok and os.path.exists(out_path(\"SECTION4_best_policyML_model.joblib\")):\n",
    "    print(\" -\", out_path(\"SECTION4_best_policyML_model.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f7ab18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] CSV L4 introuvables, reconstruction de T_A/T_B depuis df (L3).\n",
      "\n",
      "==================== SECTION 5 — Expected time-to-absorption (hours/days) (CORRIGÉ) ====================\n",
      "HOURS_PER_DAY = 2.0h (séquence journalière)\n",
      "\n",
      "--- Coûts c(s) en heures (par étape) ---\n",
      "Option FIXE ( 2h): {1: 2.0, 2: 2.0, 3: 2.0, 4: 2.0, 5: 0.0}\n",
      "Option CALIBRÉE (data): {1: 10.0, 2: 10.0, 3: 10.0, 4: 10.0, 5: 0.0}\n",
      "\n",
      "Heures observées (HoursTotal) par niveau initial (L3):\n",
      " Level   N  HoursMean  HoursMedian\n",
      "     1   4  55.000000         60.0\n",
      "     2  66  50.909091         60.0\n",
      "     3 166  51.445783         60.0\n",
      "     4 230  35.565217         20.0\n",
      "     5 347  47.377522         60.0\n",
      "\n",
      "[π_A (A partout) + coût FIXE 2h] Temps attendu jusqu’à maîtrise (état 5) — par niveau initial\n",
      " StartLevel  ExpectedHours  ExpectedDays  CostHoursPerStep\n",
      "          1           7.47          3.73               2.0\n",
      "          2           6.94          3.47               2.0\n",
      "          3           4.84          2.42               2.0\n",
      "          4           4.88          2.44               2.0\n",
      "\n",
      "[π_A (A partout) + coût CALIBRÉ] Temps attendu jusqu’à maîtrise (état 5) — par niveau initial\n",
      " StartLevel  ExpectedHours  ExpectedDays  CostHoursPerStep\n",
      "          1          37.35         18.67              10.0\n",
      "          2          34.70         17.35              10.0\n",
      "          3          24.18         12.09              10.0\n",
      "          4          24.41         12.20              10.0\n",
      "\n",
      "--- Politique optimale π*_hours (Bellman en heures) ---\n",
      "π*_hours (state->A/B): {1: 'B', 2: 'B', 3: 'B', 4: 'B'}\n",
      "V*_hours (états 1..5): [28.9, 26.525, 19.135, 18.793, 0.0]\n",
      "\n",
      "[π*_hours (optimal temps) + coût CALIBRÉ] Temps attendu jusqu’à maîtrise (état 5) — par niveau initial\n",
      " StartLevel  ExpectedHours  ExpectedDays  CostHoursPerStep\n",
      "          1          28.90         14.45              10.0\n",
      "          2          26.53         13.26              10.0\n",
      "          3          19.13          9.57              10.0\n",
      "          4          18.79          9.40              10.0\n",
      "\n",
      "[INFO] df ne contient pas piML_P(A). Exécute Section 4 avant si tu veux π_ML_state.\n",
      "\n",
      " Exports (Section 5):\n",
      " - ./out/202602/out_pomdp\\L5-expected_time_to_absorption_hours_days.json\n",
      " - ./out/202602/out_pomdp\\L5-expected_piA_fixed.csv\n",
      " - ./out/202602/out_pomdp\\L5-expected_piA_calib.csv\n",
      " - ./out/202602/out_pomdp\\L5-expected_pi_star_hours.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "EPS = 1e-12\n",
    "A, B = \"A\", \"B\"\n",
    "LEVELS = [1, 2, 3, 4, 5]\n",
    "TRANSIENT = [1, 2, 3, 4]\n",
    "ABSORBING = 5\n",
    "N_LEVELS = 5\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIG TEMPS (CORRIGÉE)\n",
    "# ------------------------------------------------------------\n",
    "# Séquence journalière = 4 activités = 2h TOTAL\n",
    "HOURS_PER_DAY = 2.0\n",
    "ACTIVITIES_PER_DAY = 4.0\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PATHS\n",
    "# ------------------------------------------------------------\n",
    "OUT_DIR = \"./out/202602/out_pomdp\"   # adapte si besoin\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def out_path(fname: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"L5-{fname}\")\n",
    "\n",
    "L3_PATH = \"./data/2025-08/L3.features_by_student.byPretest.csv\"  # adapte si besoin\n",
    "assert os.path.exists(L3_PATH), f\"Introuvable : {L3_PATH}\"\n",
    "\n",
    "# Si tu as déjà exporté T_A/T_B dans ton code principal\n",
    "TA_CSV = os.path.join(OUT_DIR, \"L4-AMC_TA_standard.csv\")   # from principal\n",
    "TB_CSV = os.path.join(OUT_DIR, \"L4-AMC_TB_intensive.csv\")  # from principal\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# UTILITAIRES\n",
    "# ------------------------------------------------------------\n",
    "def cap_level(x):\n",
    "    try:\n",
    "        v = int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return min(5, max(1, v))\n",
    "\n",
    "def canonical_QR(T: np.ndarray):\n",
    "    Q = T[:len(TRANSIENT), :len(TRANSIENT)]\n",
    "    R = T[:len(TRANSIENT), len(TRANSIENT):]\n",
    "    return Q, R\n",
    "\n",
    "def fundamental_matrix_N(Q: np.ndarray):\n",
    "    I = np.eye(Q.shape[0], dtype=float)\n",
    "    M = I - Q\n",
    "    try:\n",
    "        return np.linalg.inv(M)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.linalg.pinv(M)\n",
    "\n",
    "def expected_time_with_costs(T: np.ndarray, cost_vec_transient: np.ndarray):\n",
    "    \"\"\"\n",
    "    Expected cumulative cost until absorption:\n",
    "      m_cost = N * c\n",
    "    cost_vec_transient: c_i (heures) pour i=1..4\n",
    "    \"\"\"\n",
    "    Q, _ = canonical_QR(T)\n",
    "    N = fundamental_matrix_N(Q)\n",
    "    m = N.dot(cost_vec_transient.reshape(-1, 1)).flatten()\n",
    "    return {\"Q\": Q, \"N\": N, \"m\": m, \"J\": float(m.sum())}\n",
    "\n",
    "def build_T_pi(policy: dict, TA: np.ndarray, TB: np.ndarray) -> np.ndarray:\n",
    "    T = np.zeros((N_LEVELS, N_LEVELS), dtype=float)\n",
    "    for s in LEVELS:\n",
    "        if s == ABSORBING:\n",
    "            T[s-1, :] = 0.0\n",
    "            T[s-1, ABSORBING-1] = 1.0\n",
    "        else:\n",
    "            act = policy.get(s, A)\n",
    "            T[s-1, :] = TA[s-1, :] if act == A else TB[s-1, :]\n",
    "    return T\n",
    "\n",
    "def policy_all_A():\n",
    "    return {s: A for s in TRANSIENT}\n",
    "\n",
    "def bellman_value_iteration_time_cost(TA: np.ndarray, TB: np.ndarray, cost_hours: np.ndarray,\n",
    "                                      max_iter=20000, tol=1e-12):\n",
    "    \"\"\"\n",
    "    Bellman en HEURES :\n",
    "      V(5)=0\n",
    "      V(s)=c(s) + min_a sum_{s'} P(s'|s,a) V(s')\n",
    "    cost_hours: length 5, cost_hours[4]=0\n",
    "    \"\"\"\n",
    "    V = np.zeros(N_LEVELS, dtype=float)\n",
    "    # Initialisation pessimiste mais pas infinie\n",
    "    V[:4] = float(np.max(cost_hours[:4])) * 10.0\n",
    "    pol = {s: A for s in TRANSIENT}\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        V_old = V.copy()\n",
    "        for s in TRANSIENT:\n",
    "            c = float(cost_hours[s-1])\n",
    "            qA = c + float(np.dot(TA[s-1, :], V_old))\n",
    "            qB = c + float(np.dot(TB[s-1, :], V_old))\n",
    "            if qB < qA:\n",
    "                V[s-1] = qB\n",
    "                pol[s] = B\n",
    "            else:\n",
    "                V[s-1] = qA\n",
    "                pol[s] = A\n",
    "        V[ABSORBING-1] = 0.0\n",
    "        if np.max(np.abs(V - V_old)) < tol:\n",
    "            break\n",
    "    return V, pol\n",
    "\n",
    "def policy_from_piML_predictions(df_with_piML: pd.DataFrame,\n",
    "                                level_col=\"Pretest_i\",\n",
    "                                probaA_col=\"piML_P(A)\",\n",
    "                                threshold=0.5):\n",
    "    \"\"\"\n",
    "    π_ML_state(s) = A si mean(P(A|x)) >= threshold sur les élèves du niveau s, sinon B.\n",
    "    \"\"\"\n",
    "    pol = {}\n",
    "    for s in TRANSIENT:\n",
    "        g = df_with_piML[df_with_piML[level_col] == s]\n",
    "        if len(g) == 0:\n",
    "            pol[s] = A\n",
    "            continue\n",
    "        pA = float(g[probaA_col].mean())\n",
    "        pol[s] = A if pA >= threshold else B\n",
    "    return pol\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (0) LOAD DF\n",
    "# ------------------------------------------------------------\n",
    "df = pd.read_csv(L3_PATH)\n",
    "\n",
    "if \"Pretest_i\" not in df.columns:\n",
    "    if \"Pretest\" in df.columns:\n",
    "        df[\"Pretest_i\"] = df[\"Pretest\"].apply(cap_level)\n",
    "    else:\n",
    "        raise ValueError(\"L3 doit contenir Pretest ou Pretest_i\")\n",
    "\n",
    "if \"HoursTotal\" not in df.columns:\n",
    "    raise ValueError(\"L3 doit contenir HoursTotal\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (1) DEFINIR / CHARGER T_A, T_B\n",
    "# ------------------------------------------------------------\n",
    "def empirical_transition_matrix_from_pretest_final(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimation empirique de P(Final=j | Pretest=i) (Action A — Standard).\n",
    "    \"\"\"\n",
    "    if \"Final_i\" not in df.columns:\n",
    "        if \"Final\" in df.columns:\n",
    "            df = df.copy()\n",
    "            df[\"Final_i\"] = df[\"Final\"].apply(cap_level)\n",
    "        else:\n",
    "            raise ValueError(\"L3 doit contenir Final ou Final_i pour estimer T_A empirique.\")\n",
    "\n",
    "    mat_counts = np.zeros((N_LEVELS, N_LEVELS), dtype=float)\n",
    "    dfx = df.dropna(subset=[\"Pretest_i\", \"Final_i\"]).copy()\n",
    "\n",
    "    for _, r in dfx.iterrows():\n",
    "        i = int(r[\"Pretest_i\"]) - 1\n",
    "        j = int(r[\"Final_i\"]) - 1\n",
    "        mat_counts[i, j] += 1.0\n",
    "\n",
    "    T = np.zeros_like(mat_counts)\n",
    "    for i in range(N_LEVELS):\n",
    "        s = mat_counts[i, :].sum()\n",
    "        if s > 0:\n",
    "            T[i, :] = mat_counts[i, :] / s\n",
    "        else:\n",
    "            T[i, :] = 1.0 / N_LEVELS\n",
    "\n",
    "    # force absorbing state 5\n",
    "    T[ABSORBING-1, :] = 0.0\n",
    "    T[ABSORBING-1, ABSORBING-1] = 1.0\n",
    "    return T\n",
    "\n",
    "def make_intensive_matrix_from_standard(TA: np.ndarray,\n",
    "                                     diag_shrink: float = 0.80,\n",
    "                                     regress_shrink: float = 0.70,\n",
    "                                     boost_to_absorb: float = 1.25) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Action B — Intensive : contre-factuel normatif (non causal)\n",
    "    \"\"\"\n",
    "    TB = TA.copy().astype(float)\n",
    "    for i in range(N_LEVELS):\n",
    "        if i == ABSORBING-1:\n",
    "            continue\n",
    "        for j in range(N_LEVELS):\n",
    "            if j == i:\n",
    "                TB[i, j] *= diag_shrink\n",
    "            elif j < i:\n",
    "                TB[i, j] *= regress_shrink\n",
    "        TB[i, ABSORBING-1] *= boost_to_absorb\n",
    "        s = TB[i, :].sum()\n",
    "        TB[i, :] = (TB[i, :] / s) if s > 0 else (1.0 / N_LEVELS)\n",
    "\n",
    "    TB[ABSORBING-1, :] = 0.0\n",
    "    TB[ABSORBING-1, ABSORBING-1] = 1.0\n",
    "    return TB\n",
    "\n",
    "def load_T_from_csv(path: str) -> np.ndarray:\n",
    "    T = pd.read_csv(path, index_col=0)\n",
    "    arr = T.values.astype(float)\n",
    "    if arr.shape != (5, 5):\n",
    "        raise ValueError(f\"Matrice {path} doit être 5x5, reçu {arr.shape}\")\n",
    "    return arr\n",
    "\n",
    "# Priorité: charger depuis CSV L4 si dispo, sinon reconstruire depuis df\n",
    "if os.path.exists(TA_CSV) and os.path.exists(TB_CSV):\n",
    "    T_A = load_T_from_csv(TA_CSV)\n",
    "    T_B = load_T_from_csv(TB_CSV)\n",
    "    print(\"\\n[INFO] T_A/T_B chargées depuis CSV L4:\", TA_CSV, TB_CSV)\n",
    "else:\n",
    "    print(\"\\n[INFO] CSV L4 introuvables, reconstruction de T_A/T_B depuis df (L3).\")\n",
    "    T_A = empirical_transition_matrix_from_pretest_final(df)\n",
    "    T_B = make_intensive_matrix_from_standard(T_A)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (2) CALIBRER COÛTS EN HEURES PAR ÉTAT (option 1 fixe, option 2 data)\n",
    "# ------------------------------------------------------------\n",
    "# Correction : Coût fixe = 2h (un jour)\n",
    "cost_hours_fixed = np.array([HOURS_PER_DAY, HOURS_PER_DAY, HOURS_PER_DAY, HOURS_PER_DAY, 0.0], dtype=float)\n",
    "\n",
    "def calibrate_hours_total_by_level(df: pd.DataFrame):\n",
    "    stats = (\n",
    "        df.dropna(subset=[\"Pretest_i\", \"HoursTotal\"])\n",
    "          .groupby(\"Pretest_i\")[\"HoursTotal\"]\n",
    "          .agg([\"count\", \"mean\", \"median\"])\n",
    "          .reset_index()\n",
    "          .rename(columns={\"Pretest_i\": \"Level\", \"count\": \"N\", \"mean\": \"HoursMean\", \"median\": \"HoursMedian\"})\n",
    "    )\n",
    "    return stats.sort_values(\"Level\")\n",
    "\n",
    "def derive_cost_hours_by_state_from_data(df: pd.DataFrame, T_under_policyA: np.ndarray):\n",
    "    \"\"\"\n",
    "    c_i ≈ mean(HoursTotal | Pretest=i) / t_i(π_A)  (heures par 'étape')\n",
    "    où t_i(π_A) vient de N=(I-Q)^-1 sous π_A, en unités “étapes”.\n",
    "    \"\"\"\n",
    "    Q, _ = canonical_QR(T_under_policyA)\n",
    "    N = fundamental_matrix_N(Q)\n",
    "    t_steps = N.sum(axis=1)  # length 4\n",
    "\n",
    "    stats = calibrate_hours_total_by_level(df)\n",
    "    c = np.array([HOURS_PER_DAY]*4, dtype=float)\n",
    "\n",
    "    for idx, s in enumerate(TRANSIENT):\n",
    "        row = stats[stats[\"Level\"] == s]\n",
    "        if len(row) == 0:\n",
    "            continue\n",
    "        mean_hours = float(row.iloc[0][\"HoursMean\"])\n",
    "        denom = float(t_steps[idx])\n",
    "        if denom > EPS and mean_hours > EPS:\n",
    "            c[idx] = mean_hours / denom\n",
    "\n",
    "    # Correction clip : avec un jour de 2h, on permet des variations larges\n",
    "    # ex: 0.5h (très rapide) à 10h (5 jours bloqué sur l'étape)\n",
    "    c = np.clip(c, 0.5, 10.0)\n",
    "    return np.array([c[0], c[1], c[2], c[3], 0.0], dtype=float), stats, t_steps\n",
    "\n",
    "# Policy π_A\n",
    "piA = policy_all_A()\n",
    "T_piA = build_T_pi(piA, T_A, T_B)\n",
    "\n",
    "cost_hours_calib, stats_hours, t_steps_piA = derive_cost_hours_by_state_from_data(df, T_piA)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (3) CALCULS : π_A, π*_hours, π_ML_state\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n==================== SECTION 5 — Expected time-to-absorption (hours/days) (CORRIGÉ) ====================\")\n",
    "print(f\"HOURS_PER_DAY = {HOURS_PER_DAY:.1f}h (séquence journalière)\")\n",
    "print(\"\\n--- Coûts c(s) en heures (par étape) ---\")\n",
    "print(\"Option FIXE ( 2h):\", {s: float(cost_hours_fixed[s-1]) for s in LEVELS})\n",
    "print(\"Option CALIBRÉE (data):\", {s: float(cost_hours_calib[s-1]) for s in LEVELS})\n",
    "\n",
    "print(\"\\nHeures observées (HoursTotal) par niveau initial (L3):\")\n",
    "print(stats_hours.to_string(index=False))\n",
    "\n",
    "res_piA_fixed = expected_time_with_costs(T_piA, cost_hours_fixed[:4])\n",
    "res_piA_calib = expected_time_with_costs(T_piA, cost_hours_calib[:4])\n",
    "\n",
    "def _print_expected(res, label, cost_hours_vec):\n",
    "    print(f\"\\n[{label}] Temps attendu jusqu’à maîtrise (état 5) — par niveau initial\")\n",
    "    rows = []\n",
    "    for i, s in enumerate(TRANSIENT):\n",
    "        hours = float(res[\"m\"][i])\n",
    "        # Correction : on divise par 2.0 pour avoir les jours\n",
    "        days = hours / HOURS_PER_DAY\n",
    "        rows.append({\n",
    "            \"StartLevel\": s,\n",
    "            \"ExpectedHours\": round(hours, 2),\n",
    "            \"ExpectedDays\": round(days, 2),\n",
    "            \"CostHoursPerStep\": round(float(cost_hours_vec[s-1]), 2)\n",
    "        })\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    print(df_out.to_string(index=False))\n",
    "    return df_out\n",
    "\n",
    "df_piA_fixed = _print_expected(res_piA_fixed, \"π_A (A partout) + coût FIXE 2h\", cost_hours_fixed)\n",
    "df_piA_calib = _print_expected(res_piA_calib, \"π_A (A partout) + coût CALIBRÉ\", cost_hours_calib)\n",
    "\n",
    "# Bellman optimal en HEURES => π*_hours\n",
    "V_hours_star, pi_hours_star = bellman_value_iteration_time_cost(T_A, T_B, cost_hours_calib)\n",
    "T_pi_hours_star = build_T_pi(pi_hours_star, T_A, T_B)\n",
    "res_pi_star_hours = expected_time_with_costs(T_pi_hours_star, cost_hours_calib[:4])\n",
    "\n",
    "print(\"\\n--- Politique optimale π*_hours (Bellman en heures) ---\")\n",
    "print(\"π*_hours (state->A/B):\", pi_hours_star)\n",
    "print(\"V*_hours (états 1..5):\", [round(float(x), 3) for x in V_hours_star])\n",
    "\n",
    "df_pi_star_hours = _print_expected(res_pi_star_hours, \"π*_hours (optimal temps) + coût CALIBRÉ\", cost_hours_calib)\n",
    "\n",
    "# π_ML_state : agrégée par niveau si Section 4 a été exécutée et a produit piML_P(A)\n",
    "pi_ml_state, df_pi_ml = None, None\n",
    "if \"piML_P(A)\" in df.columns:\n",
    "    pi_ml_state = policy_from_piML_predictions(df, level_col=\"Pretest_i\", probaA_col=\"piML_P(A)\", threshold=0.5)\n",
    "    T_pi_ml_state = build_T_pi(pi_ml_state, T_A, T_B)\n",
    "    res_pi_ml = expected_time_with_costs(T_pi_ml_state, cost_hours_calib[:4])\n",
    "\n",
    "    print(\"\\n--- Politique π_ML_state (agrégée par niveau) ---\")\n",
    "    print(\"π_ML_state (state->A/B):\", pi_ml_state)\n",
    "    df_pi_ml = _print_expected(res_pi_ml, \"π_ML_state + coût CALIBRÉ\", cost_hours_calib)\n",
    "else:\n",
    "    print(\"\\n[INFO] df ne contient pas piML_P(A). Exécute Section 4 avant si tu veux π_ML_state.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (4) EXPORTS\n",
    "# ------------------------------------------------------------\n",
    "export = {\n",
    "    \"HOURS_PER_DAY\": HOURS_PER_DAY,\n",
    "    \"cost_hours_fixed\": cost_hours_fixed.tolist(),\n",
    "    \"cost_hours_calibrated\": cost_hours_calib.tolist(),\n",
    "    \"piA\": piA,\n",
    "    \"pi_hours_star\": pi_hours_star,\n",
    "    \"V_hours_star\": V_hours_star.tolist(),\n",
    "    \"pi_ml_state\": (pi_ml_state if pi_ml_state is not None else None),\n",
    "    \"expected_piA_fixed_hours\": df_piA_fixed.to_dict(orient=\"records\"),\n",
    "    \"expected_piA_calib_hours\": df_piA_calib.to_dict(orient=\"records\"),\n",
    "    \"expected_pi_star_hours\": df_pi_star_hours.to_dict(orient=\"records\"),\n",
    "    \"expected_pi_ml_hours\": (df_pi_ml.to_dict(orient=\"records\") if df_pi_ml is not None else None)\n",
    "}\n",
    "\n",
    "with open(out_path(\"expected_time_to_absorption_hours_days.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "pd.DataFrame(export[\"expected_piA_fixed_hours\"]).to_csv(out_path(\"expected_piA_fixed.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "pd.DataFrame(export[\"expected_piA_calib_hours\"]).to_csv(out_path(\"expected_piA_calib.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "pd.DataFrame(export[\"expected_pi_star_hours\"]).to_csv(out_path(\"expected_pi_star_hours.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "if export[\"expected_pi_ml_hours\"] is not None:\n",
    "    pd.DataFrame(export[\"expected_pi_ml_hours\"]).to_csv(out_path(\"expected_pi_ml_hours.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n Exports (Section 5):\")\n",
    "print(\" -\", out_path(\"expected_time_to_absorption_hours_days.json\"))\n",
    "print(\" -\", out_path(\"expected_piA_fixed.csv\"))\n",
    "print(\" -\", out_path(\"expected_piA_calib.csv\"))\n",
    "print(\" -\", out_path(\"expected_pi_star_hours.csv\"))\n",
    "if export[\"expected_pi_ml_hours\"] is not None:\n",
    "    print(\" -\", out_path(\"expected_pi_ml_hours.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95e1fcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Lecture L1 échouée (sheet Activités). Err=name '_normalize_colname' is not defined\n",
      "\n",
      "==================== SECTION 6 — AUTO-STOP PLANNING (L1=10 activités FIXES) ====================\n",
      "OUT_DIR = ./out/202602/out_pomdp\n",
      "HOURS_PER_DAY = 2.0h\n",
      "START_LEVEL=1, TARGET_LEVEL=5, MAX_DAYS_CAP=60\n",
      "L1_XLSX_PATH=./data/2025-08/L1.20250818-DataMathsElysa.xlsx | loaded=False\n",
      "[INFO] L4 CSV introuvables -> reconstruction de T_A/T_B depuis L3.\n",
      "[INFO] pi_hours_star chargée depuis SECTION5 JSON.\n",
      "\n",
      "--- Synthèse des Politiques Décisionnelles ---\n",
      "π*_hours (Optimal Temps) : {1: 'B', 2: 'B', 3: 'B', 4: 'B'}\n",
      "π_ML_state (ML Observé)  : None\n",
      "\n",
      "[SCÉNARIO] Baseline MDP — POLICY=π*_hours (Most Likely) :\n",
      "\n",
      "\n",
      "[SCÉNARIO] Baseline MDP — POLICY=π*_hours (Most Likely) :\n",
      "\n",
      "Best sequences of activities selected for 60 days (auto-stop) (Start 1, Target 5) :  Baseline MDP (fully observed) | byModel=PolicyTimeOptimal | mode=most_likely\n",
      "Day 1: [1, 0, 0, 2, 0, 0, 3, 0, 0, 4]\n",
      "Day 2: [2, 4, 0, 0, 1, 0, 0, 3, 0, 0]\n",
      "Day 3: [0, 3, 2, 0, 0, 4, 0, 0, 1, 0]\n",
      "Day 4: [0, 0, 2, 3, 0, 0, 1, 0, 0, 4]\n",
      "Day 5: [4, 0, 0, 3, 2, 0, 0, 1, 0, 0]\n",
      "Day 6: [0, 2, 0, 0, 1, 4, 0, 0, 3, 0]\n",
      "Day 7: [0, 0, 2, 0, 0, 4, 1, 0, 0, 3]\n",
      "Day 8: [4, 0, 0, 1, 0, 0, 2, 3, 0, 0]\n",
      "Day 9: [0, 4, 0, 0, 2, 0, 0, 3, 1, 0]\n",
      "Day 10: [0, 0, 3, 0, 0, 4, 0, 0, 1, 2]\n",
      "Day 11: [1, 0, 0, 2, 0, 0, 3, 0, 0, 4]\n",
      "Day 12: [2, 4, 0, 0, 1, 0, 0, 3, 0, 0]\n",
      "Day 13: [0, 3, 2, 0, 0, 4, 0, 0, 1, 0]\n",
      "Day 14: [0, 0, 2, 3, 0, 0, 1, 0, 0, 4]\n",
      "Day 15: [4, 0, 0, 3, 2, 0, 0, 1, 0, 0]\n",
      "Day 16: [0, 2, 0, 0, 1, 4, 0, 0, 3, 0]\n",
      "Day 17: [0, 0, 2, 0, 0, 4, 1, 0, 0, 3]\n",
      "Day 18: [4, 0, 0, 1, 0, 0, 2, 3, 0, 0]\n",
      "Day 19: [0, 4, 0, 0, 2, 0, 0, 3, 1, 0]\n",
      "Day 20: [0, 0, 3, 0, 0, 4, 0, 0, 1, 2]\n",
      "Day 21: [1, 0, 0, 2, 0, 0, 3, 0, 0, 4]\n",
      "Day 22: [2, 4, 0, 0, 1, 0, 0, 3, 0, 0]\n",
      "Day 23: [0, 3, 2, 0, 0, 4, 0, 0, 1, 0]\n",
      "Day 24: [0, 0, 2, 3, 0, 0, 1, 0, 0, 4]\n",
      "Day 25: [4, 0, 0, 3, 2, 0, 0, 1, 0, 0]\n",
      "Day 26: [0, 2, 0, 0, 1, 4, 0, 0, 3, 0]\n",
      "Day 27: [0, 0, 2, 0, 0, 4, 1, 0, 0, 3]\n",
      "Day 28: [4, 0, 0, 1, 0, 0, 2, 3, 0, 0]\n",
      "Day 29: [0, 4, 0, 0, 2, 0, 0, 3, 1, 0]\n",
      "Day 30: [0, 0, 3, 0, 0, 4, 0, 0, 1, 2]\n",
      "Day 31: [1, 0, 0, 2, 0, 0, 3, 0, 0, 4]\n",
      "Day 32: [2, 4, 0, 0, 1, 0, 0, 3, 0, 0]\n",
      "Day 33: [0, 3, 2, 0, 0, 4, 0, 0, 1, 0]\n",
      "Day 34: [0, 0, 2, 3, 0, 0, 1, 0, 0, 4]\n",
      "Day 35: [4, 0, 0, 3, 2, 0, 0, 1, 0, 0]\n",
      "Day 36: [0, 2, 0, 0, 1, 4, 0, 0, 3, 0]\n",
      "Day 37: [0, 0, 2, 0, 0, 4, 1, 0, 0, 3]\n",
      "Day 38: [4, 0, 0, 1, 0, 0, 2, 3, 0, 0]\n",
      "Day 39: [0, 4, 0, 0, 2, 0, 0, 3, 1, 0]\n",
      "Day 40: [0, 0, 3, 0, 0, 4, 0, 0, 1, 2]\n",
      "Day 41: [1, 0, 0, 2, 0, 0, 3, 0, 0, 4]\n",
      "Day 42: [2, 4, 0, 0, 1, 0, 0, 3, 0, 0]\n",
      "Day 43: [0, 3, 2, 0, 0, 4, 0, 0, 1, 0]\n",
      "Day 44: [0, 0, 2, 3, 0, 0, 1, 0, 0, 4]\n",
      "Day 45: [4, 0, 0, 3, 2, 0, 0, 1, 0, 0]\n",
      "Day 46: [0, 2, 0, 0, 1, 4, 0, 0, 3, 0]\n",
      "Day 47: [0, 0, 2, 0, 0, 4, 1, 0, 0, 3]\n",
      "Day 48: [4, 0, 0, 1, 0, 0, 2, 3, 0, 0]\n",
      "Day 49: [0, 4, 0, 0, 2, 0, 0, 3, 1, 0]\n",
      "Day 50: [0, 0, 3, 0, 0, 4, 0, 0, 1, 2]\n",
      "Day 51: [1, 0, 0, 2, 0, 0, 3, 0, 0, 4]\n",
      "Day 52: [2, 4, 0, 0, 1, 0, 0, 3, 0, 0]\n",
      "Day 53: [0, 3, 2, 0, 0, 4, 0, 0, 1, 0]\n",
      "Day 54: [0, 0, 2, 3, 0, 0, 1, 0, 0, 4]\n",
      "Day 55: [4, 0, 0, 3, 2, 0, 0, 1, 0, 0]\n",
      "Day 56: [0, 2, 0, 0, 1, 4, 0, 0, 3, 0]\n",
      "Day 57: [0, 0, 2, 0, 0, 4, 1, 0, 0, 3]\n",
      "Day 58: [4, 0, 0, 1, 0, 0, 2, 3, 0, 0]\n",
      "Day 59: [0, 4, 0, 0, 2, 0, 0, 3, 1, 0]\n",
      "Day 60: [0, 0, 3, 0, 0, 4, 0, 0, 1, 2]\n",
      "\n",
      "[INFO] Absorption atteinte ? False | État final : 1 | X_days : 60 | Heures totales : 120.0h\n",
      "\n",
      "[SCÉNARIO] Baseline MDP — POLICY=π*_hours (Sample) :\n",
      "\n",
      "Best sequences of activities selected for 1 days (auto-stop) (Start 1, Target 5) :  Baseline MDP (fully observed) | byModel=PolicyTimeOptimal | mode=sample_seed42\n",
      "Day 1: [1, 0, 0, 2, 0, 0, 3, 0, 0, 4]\n",
      "\n",
      "[INFO] (Stochastique) Absorption atteinte ? True | X_days : 1 | Heures totales : 2.0h\n",
      "\n",
      "[INFO] Politique π_ML_state non disponible (Section 4/5).\n",
      "\n",
      "[INFO] POMDP ignoré : matrices latentes manquantes.\n",
      "\n",
      "================================================================================\n",
      " SECTION 6 TERMINÉE — Synthèse exportée : ./out/202602/out_pomdp\\L6-SECTION6_autostop_summary.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONSTANTES & CONFIGURATION\n",
    "# ------------------------------------------------------------\n",
    "EPS = 1e-12\n",
    "A, B = \"A\", \"B\"\n",
    "\n",
    "LEVELS = [1, 2, 3, 4, 5]\n",
    "TRANSIENT = [1, 2, 3, 4]\n",
    "ABSORBING = 5\n",
    "N_LEVELS = 5\n",
    "\n",
    "MAX_DAYS_CAP = 60  # sécurité anti-boucle infinie\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIG TEMPS\n",
    "# ------------------------------------------------------------\n",
    "HOURS_PER_DAY = 2.0\n",
    "ACTIVITIES_PER_DAY = 4.0\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PATHS OUTPUT\n",
    "# ------------------------------------------------------------\n",
    "OUT_DIR = \"./out/202602/out_pomdp\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def out_path(fname: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"L6-{fname}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# INPUTS (L1/L3 + artefacts L4/L5)\n",
    "# ------------------------------------------------------------\n",
    "L1_XLSX_PATH = \"./data/2025-08/L1.20250818-DataMathsElysa.xlsx\"   # <<< IMPORTANT: chemin réel dans ton environnement\n",
    "L3_PATH = \"./data/2025-08/L3.features_by_student.byPretest.csv\"\n",
    "\n",
    "TA_CSV = os.path.join(OUT_DIR, \"L4-AMC_TA_standard.csv\")\n",
    "TB_CSV = os.path.join(OUT_DIR, \"L4-AMC_TB_intensive.csv\")\n",
    "\n",
    "SECTION5_JSON = os.path.join(OUT_DIR, \"L5-expected_time_to_absorption_hours_days.json\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DEFAULTS si variables non définies\n",
    "# ------------------------------------------------------------\n",
    "START_LEVEL = int(globals().get(\"START_LEVEL\", 1))\n",
    "TARGET_LEVEL = int(globals().get(\"TARGET_LEVEL\", 5))\n",
    "\n",
    "BY_MODEL_NAME = globals().get(\"BY_MODEL_NAME\", \"PolicyTimeOptimal\")\n",
    "TECHNIQUE_MDP = globals().get(\"TECHNIQUE_MDP\", \"Baseline MDP (fully observed)\")\n",
    "TECHNIQUE_POMDP = globals().get(\"TECHNIQUE_POMDP\", \"POMDP approx (HMM belief + projection)\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# UTILITAIRES GÉNÉRAUX\n",
    "# ------------------------------------------------------------\n",
    "def cap_level(x):\n",
    "    try:\n",
    "        v = int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return min(5, max(1, v))\n",
    "\n",
    "def _safe_tag(s: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"-_.\" else \"_\" for ch in str(s))\n",
    "\n",
    "def load_T_from_csv(path: str) -> np.ndarray:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    T = pd.read_csv(path, index_col=0)\n",
    "    arr = T.values.astype(float)\n",
    "    if arr.shape != (5, 5):\n",
    "        return None\n",
    "    return arr\n",
    "\n",
    "def load_T_from_csv_soft(path: str) -> np.ndarray:\n",
    "    \"\"\"Retourne None si absent ou mauvaise taille (fallback safe).\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    try:\n",
    "        T = pd.read_csv(path, index_col=0)\n",
    "        arr = T.values.astype(float)\n",
    "        if arr.shape != (5, 5):\n",
    "            return None\n",
    "        return arr\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def empirical_transition_matrix_from_pretest_final(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Estimation empirique de P(Final=j | Pretest=i) (Action A — Standard).\"\"\"\n",
    "    if \"Final_i\" not in df.columns:\n",
    "        if \"Final\" in df.columns:\n",
    "            df = df.copy()\n",
    "            df[\"Final_i\"] = df[\"Final\"].apply(cap_level)\n",
    "        else:\n",
    "            print(\"[WARN] Pas de colonne Final/Final_i. Matrice identité.\")\n",
    "            return np.eye(N_LEVELS)\n",
    "\n",
    "    if \"Pretest_i\" not in df.columns:\n",
    "        if \"Pretest\" in df.columns:\n",
    "            df = df.copy()\n",
    "            df[\"Pretest_i\"] = df[\"Pretest\"].apply(cap_level)\n",
    "        else:\n",
    "            print(\"[WARN] Pas de colonne Pretest/Pretest_i. Matrice identité.\")\n",
    "            return np.eye(N_LEVELS)\n",
    "\n",
    "    mat_counts = np.zeros((N_LEVELS, N_LEVELS), dtype=float)\n",
    "    dfx = df.dropna(subset=[\"Pretest_i\", \"Final_i\"]).copy()\n",
    "\n",
    "    for _, r in dfx.iterrows():\n",
    "        i = int(r[\"Pretest_i\"]) - 1\n",
    "        j = int(r[\"Final_i\"]) - 1\n",
    "        if 0 <= i < N_LEVELS and 0 <= j < N_LEVELS:\n",
    "            mat_counts[i, j] += 1.0\n",
    "\n",
    "    T = np.zeros_like(mat_counts)\n",
    "    for i in range(N_LEVELS):\n",
    "        s = mat_counts[i, :].sum()\n",
    "        if s > 0:\n",
    "            T[i, :] = mat_counts[i, :] / s\n",
    "        else:\n",
    "            T[i, :] = 1.0 / N_LEVELS\n",
    "\n",
    "    # état 5 absorbant\n",
    "    T[ABSORBING-1, :] = 0.0\n",
    "    T[ABSORBING-1, ABSORBING-1] = 1.0\n",
    "    return T\n",
    "\n",
    "def make_intensive_matrix_from_standard(TA: np.ndarray,\n",
    "                                        diag_shrink: float = 0.80,\n",
    "                                        regress_shrink: float = 0.70,\n",
    "                                        boost_to_absorb: float = 1.25) -> np.ndarray:\n",
    "    \"\"\"Action B — Intensive : contre-factuel normatif (non causal).\"\"\"\n",
    "    TB = TA.copy().astype(float)\n",
    "    for i in range(N_LEVELS):\n",
    "        if i == ABSORBING-1:\n",
    "            continue\n",
    "        for j in range(N_LEVELS):\n",
    "            if j == i:\n",
    "                TB[i, j] *= diag_shrink\n",
    "            elif j < i:\n",
    "                TB[i, j] *= regress_shrink\n",
    "        TB[i, ABSORBING-1] *= boost_to_absorb\n",
    "        s = TB[i, :].sum()\n",
    "        TB[i, :] = (TB[i, :] / s) if s > 0 else (1.0 / N_LEVELS)\n",
    "\n",
    "    TB[ABSORBING-1, :] = 0.0\n",
    "    TB[ABSORBING-1, ABSORBING-1] = 1.0\n",
    "    return TB\n",
    "\n",
    "def bellman_value_iteration_time_cost(TA: np.ndarray, TB: np.ndarray, cost_hours: np.ndarray,\n",
    "                                      max_iter=20000, tol=1e-12):\n",
    "    \"\"\"\n",
    "    Bellman en HEURES :\n",
    "      V(5)=0\n",
    "      V(s)=c(s) + min_a sum_{s'} P(s'|s,a) V(s')\n",
    "    \"\"\"\n",
    "    V = np.zeros(N_LEVELS, dtype=float)\n",
    "    V[:4] = float(np.max(cost_hours[:4])) * 10.0\n",
    "    pol = {s: A for s in TRANSIENT}\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        V_old = V.copy()\n",
    "        for s in TRANSIENT:\n",
    "            c = float(cost_hours[s-1])\n",
    "            qA = c + float(np.dot(TA[s-1, :], V_old))\n",
    "            qB = c + float(np.dot(TB[s-1, :], V_old))\n",
    "            if qB < qA:\n",
    "                V[s-1] = qB\n",
    "                pol[s] = B\n",
    "            else:\n",
    "                V[s-1] = qA\n",
    "                pol[s] = A\n",
    "        V[ABSORBING-1] = 0.0\n",
    "        if np.max(np.abs(V - V_old)) < tol:\n",
    "            break\n",
    "    return V, pol\n",
    "\n",
    "def choose_action(level: int, policy: dict) -> str:\n",
    "    return policy.get(int(level), A)\n",
    "\n",
    "def next_state_sample(level: int, action: str, TA: np.ndarray, TB: np.ndarray, rng: np.random.Generator) -> int:\n",
    "    row = TA[level-1, :] if action == A else TB[level-1, :]\n",
    "    row = np.clip(row, 0.0, 1.0)\n",
    "    s = float(row.sum())\n",
    "    row = (row / s) if s > EPS else (np.ones_like(row) / len(row))\n",
    "    return int(rng.choice(np.arange(1, N_LEVELS+1), p=row))\n",
    "\n",
    "def next_state_most_likely(level: int, action: str, TA: np.ndarray, TB: np.ndarray) -> int:\n",
    "    row = TA[level-1, :] if action == A else TB[level-1, :]\n",
    "    return int(np.argmax(row) + 1)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# UTILITAIRES GÉNÉRAUX\n",
    "# ------------------------------------------------------------\n",
    "def cap_level(x):\n",
    "    try:\n",
    "        v = int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return min(5, max(1, v))\n",
    "\n",
    "def _safe_tag(s: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"-_.\" else \"_\" for ch in str(s))\n",
    "\n",
    "def load_T_from_csv_soft(path: str) -> np.ndarray:\n",
    "    \"\"\"Retourne None si absent ou mauvaise taille (fallback safe).\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    try:\n",
    "        T = pd.read_csv(path, index_col=0)\n",
    "        arr = T.values.astype(float)\n",
    "        if arr.shape != (5, 5):\n",
    "            return None\n",
    "        return arr\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def empirical_transition_matrix_from_pretest_final(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Estimation empirique de P(Final=j | Pretest=i) (Action A — Standard).\"\"\"\n",
    "    if \"Final_i\" not in df.columns:\n",
    "        if \"Final\" in df.columns:\n",
    "            df = df.copy()\n",
    "            df[\"Final_i\"] = df[\"Final\"].apply(cap_level)\n",
    "        else:\n",
    "            print(\"[WARN] Pas de colonne Final/Final_i. Matrice identité.\")\n",
    "            return np.eye(N_LEVELS)\n",
    "\n",
    "    if \"Pretest_i\" not in df.columns:\n",
    "        if \"Pretest\" in df.columns:\n",
    "            df = df.copy()\n",
    "            df[\"Pretest_i\"] = df[\"Pretest\"].apply(cap_level)\n",
    "        else:\n",
    "            print(\"[WARN] Pas de colonne Pretest/Pretest_i. Matrice identité.\")\n",
    "            return np.eye(N_LEVELS)\n",
    "\n",
    "    mat_counts = np.zeros((N_LEVELS, N_LEVELS), dtype=float)\n",
    "    dfx = df.dropna(subset=[\"Pretest_i\", \"Final_i\"]).copy()\n",
    "\n",
    "    for _, r in dfx.iterrows():\n",
    "        i = int(r[\"Pretest_i\"]) - 1\n",
    "        j = int(r[\"Final_i\"]) - 1\n",
    "        if 0 <= i < N_LEVELS and 0 <= j < N_LEVELS:\n",
    "            mat_counts[i, j] += 1.0\n",
    "\n",
    "    T = np.zeros_like(mat_counts)\n",
    "    for i in range(N_LEVELS):\n",
    "        s = mat_counts[i, :].sum()\n",
    "        if s > 0:\n",
    "            T[i, :] = mat_counts[i, :] / s\n",
    "        else:\n",
    "            T[i, :] = 1.0 / N_LEVELS\n",
    "\n",
    "    # état 5 absorbant\n",
    "    T[ABSORBING-1, :] = 0.0\n",
    "    T[ABSORBING-1, ABSORBING-1] = 1.0\n",
    "    return T\n",
    "\n",
    "def make_intensive_matrix_from_standard(TA: np.ndarray,\n",
    "                                        diag_shrink: float = 0.80,\n",
    "                                        regress_shrink: float = 0.70,\n",
    "                                        boost_to_absorb: float = 1.25) -> np.ndarray:\n",
    "    \"\"\"Action B — Intensive : contre-factuel normatif (non causal).\"\"\"\n",
    "    TB = TA.copy().astype(float)\n",
    "    for i in range(N_LEVELS):\n",
    "        if i == ABSORBING-1:\n",
    "            continue\n",
    "        for j in range(N_LEVELS):\n",
    "            if j == i:\n",
    "                TB[i, j] *= diag_shrink\n",
    "            elif j < i:\n",
    "                TB[i, j] *= regress_shrink\n",
    "        TB[i, ABSORBING-1] *= boost_to_absorb\n",
    "        s = TB[i, :].sum()\n",
    "        TB[i, :] = (TB[i, :] / s) if s > 0 else (1.0 / N_LEVELS)\n",
    "\n",
    "    TB[ABSORBING-1, :] = 0.0\n",
    "    TB[ABSORBING-1, ABSORBING-1] = 1.0\n",
    "    return TB\n",
    "\n",
    "def bellman_value_iteration_time_cost(TA: np.ndarray, TB: np.ndarray, cost_hours: np.ndarray,\n",
    "                                      max_iter=20000, tol=1e-12):\n",
    "    \"\"\"\n",
    "    Bellman en HEURES :\n",
    "      V(5)=0\n",
    "      V(s)=c(s) + min_a sum_{s'} P(s'|s,a) V(s')\n",
    "    \"\"\"\n",
    "    V = np.zeros(N_LEVELS, dtype=float)\n",
    "    V[:4] = float(np.max(cost_hours[:4])) * 10.0\n",
    "    pol = {s: A for s in TRANSIENT}\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        V_old = V.copy()\n",
    "        for s in TRANSIENT:\n",
    "            c = float(cost_hours[s-1])\n",
    "            qA = c + float(np.dot(TA[s-1, :], V_old))\n",
    "            qB = c + float(np.dot(TB[s-1, :], V_old))\n",
    "            if qB < qA:\n",
    "                V[s-1] = qB\n",
    "                pol[s] = B\n",
    "            else:\n",
    "                V[s-1] = qA\n",
    "                pol[s] = A\n",
    "        V[ABSORBING-1] = 0.0\n",
    "        if np.max(np.abs(V - V_old)) < tol:\n",
    "            break\n",
    "    return V, pol\n",
    "\n",
    "def choose_action(level: int, policy: dict) -> str:\n",
    "    return policy.get(int(level), A)\n",
    "\n",
    "def next_state_sample(level: int, action: str, TA: np.ndarray, TB: np.ndarray, rng: np.random.Generator) -> int:\n",
    "    row = TA[level-1, :] if action == A else TB[level-1, :]\n",
    "    row = np.clip(row, 0.0, 1.0)\n",
    "    s = float(row.sum())\n",
    "    row = (row / s) if s > EPS else (np.ones_like(row) / len(row))\n",
    "    return int(rng.choice(np.arange(1, N_LEVELS+1), p=row))\n",
    "\n",
    "def next_state_most_likely(level: int, action: str, TA: np.ndarray, TB: np.ndarray) -> int:\n",
    "    row = TA[level-1, :] if action == A else TB[level-1, :]\n",
    "    return int(np.argmax(row) + 1)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# L1 — ACTIVITÉS : vecteur FIXE de 10 activités par niveau + vecteur par jour\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "LEVEL_PREFIX = {\n",
    "    1: \"Deb\",\n",
    "    2: \"Unchiffe\",\n",
    "    3: \"Deuxchiffres\",\n",
    "    4: \"TroisA\",\n",
    "    5: \"TroisB\",\n",
    "}\n",
    "\n",
    "DF_L1_ACT = None              # dataframe onglet \"Activités\"\n",
    "LEVEL_ACT_COLS_10 = {}        # level -> liste 10 colonnes activités\n",
    "\n",
    "def _pick_10_activity_cols_for_level(df_act: pd.DataFrame, level: int):\n",
    "    \"\"\"\n",
    "    Retourne 10 colonnes d'activités pour un niveau, dans l'ordre du fichier.\n",
    "    Si <10 => padding avec \"Unused\".\n",
    "    \"\"\"\n",
    "    pref = LEVEL_PREFIX[int(level)]\n",
    "    cols = [c for c in df_act.columns if c not in [\"Semaine\", \"Jour\"]]\n",
    "    cols = [_normalize_colname(c) for c in cols]\n",
    "\n",
    "    # IMPORTANT: pandas garde les noms originaux; on va mapper normalisé -> original\n",
    "    norm_to_orig = {_normalize_colname(c): c for c in df_act.columns}\n",
    "\n",
    "    # récupérer colonnes commençant par le prefix\n",
    "    all_norm = [c for c in cols if c.startswith(pref)]\n",
    "    chosen_norm = all_norm[:10]\n",
    "\n",
    "    chosen = [norm_to_orig[c] for c in chosen_norm if c in norm_to_orig]\n",
    "    # padding si nécessaire\n",
    "    if len(chosen) < 10:\n",
    "        for k in range(len(chosen)+1, 11):\n",
    "            chosen.append(f\"{pref}_Unused{k}\")  # colonne fictive (affichage)\n",
    "    return chosen\n",
    "\n",
    "def load_L1_activities_or_fallback():\n",
    "    \"\"\"\n",
    "    Charge l'onglet 'Activités' du fichier L1 et prépare 10 colonnes par niveau.\n",
    "    \"\"\"\n",
    "    global DF_L1_ACT, LEVEL_ACT_COLS_10\n",
    "    DF_L1_ACT = None\n",
    "    LEVEL_ACT_COLS_10 = {}\n",
    "\n",
    "    loaded = False\n",
    "    if os.path.exists(L1_XLSX_PATH):\n",
    "        try:\n",
    "            DF_L1_ACT = pd.read_excel(L1_XLSX_PATH, sheet_name=\"Activités\")\n",
    "            # normalise colonnes\n",
    "            DF_L1_ACT.columns = [_normalize_colname(c) for c in DF_L1_ACT.columns]\n",
    "\n",
    "            # convert Jour en int\n",
    "            if \"Jour\" in DF_L1_ACT.columns:\n",
    "                DF_L1_ACT[\"Jour\"] = DF_L1_ACT[\"Jour\"].apply(lambda x: int(x) if pd.notna(x) else x)\n",
    "\n",
    "            for lvl in LEVELS:\n",
    "                LEVEL_ACT_COLS_10[int(lvl)] = _pick_10_activity_cols_for_level(DF_L1_ACT, int(lvl))\n",
    "\n",
    "            loaded = True\n",
    "        except Exception as e:\n",
    "            DF_L1_ACT = None\n",
    "            LEVEL_ACT_COLS_10 = {}\n",
    "            print(f\"[WARN] Lecture L1 échouée (sheet Activités). Err={e}\")\n",
    "\n",
    "    return loaded\n",
    "\n",
    "def activities10_for_level(level: int):\n",
    "    \"\"\"\n",
    "    Retourne la liste FIXE de 10 activités pour ce niveau.\n",
    "    \"\"\"\n",
    "    level = int(level)\n",
    "    if DF_L1_ACT is None:\n",
    "        return [f\"GenAct{j}_Lv{level}\" for j in range(1, 11)]\n",
    "    return list(LEVEL_ACT_COLS_10[level])\n",
    "\n",
    "def _day_to_L1_day(day: int) -> int:\n",
    "    \"\"\"\n",
    "    L1 ne contient que 10 jours.\n",
    "    Pour 60 jours: on boucle => 1..10,1..10,...\n",
    "    \"\"\"\n",
    "    day = int(day)\n",
    "    return ((day - 1) % 10) + 1\n",
    "\n",
    "def vector10_for_level_day(level: int, day: int):\n",
    "    \"\"\"\n",
    "    Construit le vecteur taille 10 pour (niveau, day),\n",
    "    en utilisant le jour équivalent dans L1 (cycle 10 jours).\n",
    "    \"\"\"\n",
    "    level = int(level)\n",
    "    day_eff = _day_to_L1_day(day)\n",
    "\n",
    "    acts10 = activities10_for_level(level)\n",
    "    v = [0] * 10\n",
    "\n",
    "    if DF_L1_ACT is None:\n",
    "        DISPERSED_PATTERNS = [\n",
    "            [0, 3, 6, 9],\n",
    "            [1, 4, 7, 0],\n",
    "            [2, 5, 8, 1],\n",
    "            [3, 6, 9, 2],\n",
    "            [4, 7, 0, 3],\n",
    "            [5, 8, 1, 4],\n",
    "            [6, 9, 2, 5],\n",
    "            [7, 0, 3, 6],\n",
    "            [8, 1, 4, 7],\n",
    "            [9, 2, 5, 8],\n",
    "        ]\n",
    "        ORDER_PATTERNS = [\n",
    "            [1, 2, 3, 4],\n",
    "            [4, 1, 3, 2],\n",
    "            [2, 4, 1, 3],\n",
    "            [3, 1, 4, 2],\n",
    "            [2, 1, 4, 3],\n",
    "            [4, 3, 2, 1],\n",
    "            [1, 3, 2, 4],\n",
    "            [3, 4, 1, 2],\n",
    "            [1, 4, 2, 3],\n",
    "            [2, 3, 4, 1],\n",
    "        ]\n",
    "\n",
    "\n",
    "        # fallback : change quand même avec day (petite variation)\n",
    "        # (ça évite 60 jours identiques même en fallback)\n",
    "        # index pattern selon day_eff (1..10)\n",
    "        idxs = DISPERSED_PATTERNS[(day_eff - 1) % len(DISPERSED_PATTERNS)]\n",
    "        order = ORDER_PATTERNS[(day_eff - 1) % len(ORDER_PATTERNS)]\n",
    "\n",
    "        # appliquer positions selon l'ordre choisi\n",
    "        for col_idx, pos in zip(idxs, order):\n",
    "            v[col_idx] = pos\n",
    "\n",
    "        return v, acts10, day_eff\n",
    "\n",
    "    # ligne du jour (jour 1..10 dans DF)\n",
    "    row = DF_L1_ACT.loc[DF_L1_ACT[\"Jour\"] == day_eff]\n",
    "    if len(row) == 0:\n",
    "        return v, acts10, day_eff\n",
    "    row = row.iloc[0]\n",
    "\n",
    "    # remplir v via les 10 colonnes\n",
    "    for i, col in enumerate(acts10):\n",
    "        if col not in DF_L1_ACT.columns:\n",
    "            continue\n",
    "        val = row.get(col, np.nan)\n",
    "        if pd.isna(val):\n",
    "            continue\n",
    "        try:\n",
    "            iv = int(val)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if 1 <= iv <= 4:\n",
    "            v[i] = iv\n",
    "\n",
    "    return v, acts10, day_eff\n",
    "\n",
    "def action_to_sequence(level: int, day: int, action: str):\n",
    "    \"\"\"\n",
    "    Renvoie pos1..pos4 (noms) du planning L1 pour (niveau, day).\n",
    "    NB: L1 ne dépend pas de A/B, mais on garde action dans la signature (cohérence).\n",
    "    \"\"\"\n",
    "    v, acts10, day_eff = vector10_for_level_day(level, day)\n",
    "    pos_to_act = {}\n",
    "    for idx, p in enumerate(v):\n",
    "        if p in [1, 2, 3, 4]:\n",
    "            pos_to_act[int(p)] = acts10[idx]\n",
    "\n",
    "    return {\n",
    "        \"pos1\": pos_to_act.get(1, \"\"),\n",
    "        \"pos2\": pos_to_act.get(2, \"\"),\n",
    "        \"pos3\": pos_to_act.get(3, \"\"),\n",
    "        \"pos4\": pos_to_act.get(4, \"\"),\n",
    "        \"_day_eff_L1\": day_eff,  # debug utile\n",
    "    }\n",
    "\n",
    "# Charger L1 maintenant\n",
    "loaded = load_L1_activities_or_fallback()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (3) POMDP auto-stop — Utilitaires et Moteur\n",
    "# ------------------------------------------------------------\n",
    "def normalize(p: np.ndarray) -> np.ndarray:\n",
    "    s = float(p.sum())\n",
    "    if s <= 0:\n",
    "        return np.ones_like(p) / len(p)\n",
    "    return p / s\n",
    "\n",
    "def belief_init_from_observation(obs_level: int, O_emit: np.ndarray, prior=None) -> np.ndarray:\n",
    "    if prior is None:\n",
    "        prior = np.ones(O_emit.shape[1], dtype=float) / O_emit.shape[1]\n",
    "    b = prior * O_emit[obs_level-1, :]\n",
    "    return normalize(b)\n",
    "\n",
    "def belief_predict(b: np.ndarray, Th: np.ndarray) -> np.ndarray:\n",
    "    return normalize(b.dot(Th))\n",
    "\n",
    "def belief_update(b_pred: np.ndarray, obs_level: int, O_emit: np.ndarray) -> np.ndarray:\n",
    "    b_new = b_pred * O_emit[obs_level-1, :]\n",
    "    return normalize(b_new)\n",
    "\n",
    "def most_likely_observation_from_belief(b: np.ndarray, O_emit: np.ndarray) -> int:\n",
    "    p_obs = O_emit.dot(b)\n",
    "    return int(np.argmax(p_obs) + 1)\n",
    "\n",
    "def build_plan_pomdp_auto_stop(start_obs_level: int,\n",
    "                               O_emit: np.ndarray,\n",
    "                               T_hidden_A: np.ndarray,\n",
    "                               T_hidden_B: np.ndarray,\n",
    "                               pi_hidden_star: dict,\n",
    "                               hidden_names=(\"Faible\",\"Maitrise\"),\n",
    "                               mode_obs: str = \"most_likely\",\n",
    "                               seed: int = 42,\n",
    "                               max_days: int = MAX_DAYS_CAP):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    H0, H1 = 0, 1\n",
    "\n",
    "    b = belief_init_from_observation(int(start_obs_level), O_emit)\n",
    "    plan = []\n",
    "    obs_level = int(start_obs_level)\n",
    "\n",
    "    for day in range(1, max_days+1):\n",
    "        z_hat = int(np.argmax(b))\n",
    "        if z_hat == H1:\n",
    "            break\n",
    "\n",
    "        action = pi_hidden_star.get(z_hat, A)\n",
    "\n",
    "        #  FIX: signature correct (level, day, action)\n",
    "        seq = action_to_sequence(obs_level, day, action)\n",
    "\n",
    "        plan.append({\n",
    "            \"Day\": day,\n",
    "            \"ObsLevel\": obs_level,\n",
    "            \"z_hat\": hidden_names[z_hat],\n",
    "            \"belief_Faible\": float(b[H0]),\n",
    "            \"belief_Maitrise\": float(b[H1]),\n",
    "            \"Action\": action,\n",
    "            \"pos1\": seq.get(\"pos1\"),\n",
    "            \"pos2\": seq.get(\"pos2\"),\n",
    "            \"pos3\": seq.get(\"pos3\"),\n",
    "            \"pos4\": seq.get(\"pos4\"),\n",
    "        })\n",
    "\n",
    "        Th = T_hidden_A if action == A else T_hidden_B\n",
    "        b_pred = belief_predict(b, Th)\n",
    "\n",
    "        if mode_obs == \"sample\":\n",
    "            p_obs = O_emit.dot(b_pred)\n",
    "            p_obs = p_obs / max(EPS, float(p_obs.sum()))\n",
    "            obs_level = int(rng.choice(np.arange(1, N_LEVELS+1), p=p_obs))\n",
    "        else:\n",
    "            obs_level = most_likely_observation_from_belief(b_pred, O_emit)\n",
    "\n",
    "        b = belief_update(b_pred, obs_level, O_emit)\n",
    "\n",
    "    reached = (int(np.argmax(b)) == H1)\n",
    "    return plan, reached\n",
    "\n",
    "\n",
    "def print_and_save_plan_pomdp_variable(plan_rows,\n",
    "                                       technique: str,\n",
    "                                       by_model: str,\n",
    "                                       start_obs_level: int,\n",
    "                                       target_level: int,\n",
    "                                       filename_prefix: str,\n",
    "                                       mode_label: str):\n",
    "    X = len(plan_rows)\n",
    "    title = (f\"Best sequences of activities selected for {X} days (auto-stop) \"\n",
    "             f\"(Start {start_obs_level}, Target {target_level}) :  {technique} |  byModel={by_model} |  mode={mode_label}\")\n",
    "    print(title)\n",
    "\n",
    "    acts10_header = activities10_for_level(start_obs_level)\n",
    "    print(\"Activities(10) fixed order:\")\n",
    "    for k, a in enumerate(acts10_header, start=1):\n",
    "        print(f\"  {k:02d}. {a}\")\n",
    "\n",
    "    lines = [title, \"Activities(10) fixed order:\"]\n",
    "    lines += [f\"  {k:02d}. {a}\" for k, a in enumerate(acts10_header, start=1)]\n",
    "\n",
    "    out_rows = []\n",
    "    for r in plan_rows:\n",
    "        day = int(r.get(\"Day\", 0))\n",
    "        obs_level = int(r.get(\"ObsLevel\", start_obs_level))\n",
    "\n",
    "        v10, _, l1_day = vector10_for_level_day(obs_level, day)\n",
    "\n",
    "        print(f\"Day {day} (L1-Day {l1_day}) [ObsLevel={obs_level} z_hat={r.get('z_hat','')} Action={r.get('Action','')}] : {v10}\")\n",
    "        lines.append(f\"Day {day} (L1-Day {l1_day}) [ObsLevel={obs_level} z_hat={r.get('z_hat','')} Action={r.get('Action','')}] : {v10}\")\n",
    "\n",
    "        out_rows.append({\n",
    "            \"Day\": day,\n",
    "            \"L1_DayUsed\": l1_day,\n",
    "            \"StartObsLevel\": start_obs_level,\n",
    "            \"TargetLevel\": target_level,\n",
    "            \"Technique\": technique,\n",
    "            \"ByModel\": by_model,\n",
    "            \"Mode\": mode_label,\n",
    "            \"ObsLevel\": obs_level,\n",
    "            \"z_hat\": r.get(\"z_hat\",\"\"),\n",
    "            \"belief_Faible\": r.get(\"belief_Faible\",\"\"),\n",
    "            \"belief_Maitrise\": r.get(\"belief_Maitrise\",\"\"),\n",
    "            \"Action\": r.get(\"Action\",\"\"),\n",
    "            \"pos1\": r.get(\"pos1\",\"\"),\n",
    "            \"pos2\": r.get(\"pos2\",\"\"),\n",
    "            \"pos3\": r.get(\"pos3\",\"\"),\n",
    "            \"pos4\": r.get(\"pos4\",\"\"),\n",
    "            \"Vector10\": json.dumps(v10, ensure_ascii=False),\n",
    "            \"HoursPerDay\": HOURS_PER_DAY,\n",
    "            \"CumHoursIfCompleted\": round(day * HOURS_PER_DAY, 2),\n",
    "        })\n",
    "\n",
    "    safe_model = _safe_tag(by_model)\n",
    "    out_txt = out_path(f\"{filename_prefix}_autoStop_Start{start_obs_level}_Target{target_level}__byModel-{safe_model}__{mode_label}.txt\")\n",
    "    out_csv = out_path(f\"{filename_prefix}_autoStop_Start{start_obs_level}_Target{target_level}__byModel-{safe_model}__{mode_label}.csv\")\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "        f.write(\"\\n(0 = non sélectionnée ce jour ; 1..4 = position de l’activité dans la journée)\\n\")\n",
    "\n",
    "    pd.DataFrame(out_rows).to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    return out_txt, out_csv, X\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# (4) CHARGEMENTS / POLITIQUES — Exécution\n",
    "# ============================================================\n",
    "print(\"\\n==================== SECTION 6 — AUTO-STOP PLANNING (L1=10 activités FIXES) ====================\")\n",
    "print(f\"OUT_DIR = {OUT_DIR}\")\n",
    "print(f\"HOURS_PER_DAY = {HOURS_PER_DAY}h\")\n",
    "print(f\"START_LEVEL={START_LEVEL}, TARGET_LEVEL={TARGET_LEVEL}, MAX_DAYS_CAP={MAX_DAYS_CAP}\")\n",
    "print(f\"L1_XLSX_PATH={L1_XLSX_PATH} | loaded={DF_L1_ACT is not None}\")\n",
    "\n",
    "# (A) Matrices T_A / T_B (L4 sinon reconstruction L3)\n",
    "T_A = load_T_from_csv_soft(TA_CSV)\n",
    "T_B = load_T_from_csv_soft(TB_CSV)\n",
    "\n",
    "if (T_A is None) or (T_B is None):\n",
    "    if os.path.exists(L3_PATH):\n",
    "        df = pd.read_csv(L3_PATH)\n",
    "        if \"Pretest_i\" not in df.columns and \"Pretest\" in df.columns:\n",
    "            df[\"Pretest_i\"] = df[\"Pretest\"].apply(cap_level)\n",
    "        print(\"[INFO] L4 CSV introuvables -> reconstruction de T_A/T_B depuis L3.\")\n",
    "        T_A = empirical_transition_matrix_from_pretest_final(df)\n",
    "        T_B = make_intensive_matrix_from_standard(T_A)\n",
    "    else:\n",
    "        print(\"[WARN] L3 introuvable et L4 absent -> matrices identité fictives.\")\n",
    "        T_A = np.eye(N_LEVELS)\n",
    "        T_B = np.eye(N_LEVELS)\n",
    "else:\n",
    "    print(\"[INFO] T_A/T_B chargées depuis les artefacts L4 CSV.\")\n",
    "\n",
    "# (B) Politiques (SECTION5 JSON sinon Bellman local) : Charger policy Section 5\n",
    "pi_hours_star = None\n",
    "pi_ml_state = None\n",
    "SECTION5_data = {}\n",
    "\n",
    "if os.path.exists(SECTION5_JSON):\n",
    "    try:\n",
    "        with open(SECTION5_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "            SECTION5_data = json.load(f)\n",
    "\n",
    "        if isinstance(SECTION5_data.get(\"pi_hours_star\", None), dict):\n",
    "            pi_hours_star = {int(k): v for k, v in SECTION5_data[\"pi_hours_star\"].items()}\n",
    "            print(\"[INFO] pi_hours_star chargée depuis SECTION5 JSON.\")\n",
    "\n",
    "        if isinstance(SECTION5_data.get(\"pi_ml_state\", None), dict):\n",
    "            pi_ml_state = {int(k): v for k, v in SECTION5_data[\"pi_ml_state\"].items()}\n",
    "            print(\"[INFO] pi_ml_state chargée depuis SECTION5 JSON.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Impossible de lire SECTION5 JSON : {e}\")\n",
    "\n",
    "if pi_hours_star is None:\n",
    "    print(\"[INFO] pi_hours_star absente -> Bellman local (coût=2h/étape).\")\n",
    "    cost_hours_vec = np.array([HOURS_PER_DAY]*4 + [0.0], dtype=float)\n",
    "    _, pi_hours_star = bellman_value_iteration_time_cost(T_A, T_B, cost_hours_vec)\n",
    "\n",
    "print(\"\\n--- Synthèse des Politiques Décisionnelles ---\")\n",
    "print(f\"π*_hours (Optimal Temps) : {pi_hours_star}\")\n",
    "print(f\"π_ML_state (ML Observé)  : {pi_ml_state}\")\n",
    "\n",
    "# ============================================================\n",
    "# (5) RUN MDP auto-stop\n",
    "# ============================================================\n",
    "policy_opt = pi_hours_star\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# IMPORTANT : redéfinir build_plan_mdp_auto_stop ICI\n",
    "# pour éviter NameError si cellules non exécutées\n",
    "# ------------------------------------------------------------\n",
    "def build_plan_mdp_auto_stop(start_level: int,\n",
    "                            policy: dict,\n",
    "                            TA: np.ndarray, TB: np.ndarray,\n",
    "                            mode: str = \"most_likely\",\n",
    "                            seed: int = 42,\n",
    "                            stop_at: int = 5,\n",
    "                            max_days: int = MAX_DAYS_CAP):\n",
    "    \"\"\"\n",
    "    Simule une trajectoire MDP jour par jour.\n",
    "    La séquence affichée provient de L1 (10 activités fixes) par (niveau, jour).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    current = int(start_level)\n",
    "    plan = []\n",
    "\n",
    "    for day in range(1, max_days+1):\n",
    "        if current >= stop_at:\n",
    "            break\n",
    "\n",
    "        action = choose_action(current, policy)\n",
    "\n",
    "        # IMPORTANT: on fournit day aussi\n",
    "        seq = action_to_sequence(current, day, action)\n",
    "\n",
    "        plan.append({\n",
    "            \"Day\": day,\n",
    "            \"Level\": current,\n",
    "            \"Action\": action,\n",
    "            \"L1_day_used\": seq.get(\"_day_eff_L1\", None),\n",
    "            \"pos1\": seq.get(\"pos1\"),\n",
    "            \"pos2\": seq.get(\"pos2\"),\n",
    "            \"pos3\": seq.get(\"pos3\"),\n",
    "            \"pos4\": seq.get(\"pos4\"),\n",
    "            \"vector10\": vector10_for_level_day(current, day)[0],\n",
    "            \"acts10\": activities10_for_level(current),\n",
    "        })\n",
    "\n",
    "        if mode == \"sample\":\n",
    "            current = next_state_sample(current, action, TA, TB, rng)\n",
    "        else:\n",
    "            current = next_state_most_likely(current, action, TA, TB)\n",
    "\n",
    "    reached = (current >= stop_at)\n",
    "    return plan, reached, current\n",
    "\n",
    "# Scénario 1 : most_likely\n",
    "print(\"\\n[SCÉNARIO] Baseline MDP — POLICY=π*_hours (Most Likely) :\\n\")\n",
    "plan_opt_ml, reached_opt, final_state_opt = build_plan_mdp_auto_stop(\n",
    "    start_level=START_LEVEL,\n",
    "    policy=policy_opt,\n",
    "    TA=T_A, TB=T_B,\n",
    "    mode=\"most_likely\",\n",
    "    seed=42,\n",
    "    stop_at=ABSORBING,\n",
    "    max_days=MAX_DAYS_CAP\n",
    ")\n",
    "\n",
    "def print_and_save_plan_variable_days(plan_rows,\n",
    "                                      technique: str,\n",
    "                                      by_model: str,\n",
    "                                      start_level: int,\n",
    "                                      target_level: int,\n",
    "                                      filename_prefix: str,\n",
    "                                      mode_label: str):\n",
    "    X = len(plan_rows)\n",
    "    title = (f\"Best sequences of activities selected for {X} days (auto-stop) \"\n",
    "             f\"(Start {start_level}, Target {target_level}) :  {technique} | byModel={by_model} | mode={mode_label}\")\n",
    "    print(title)\n",
    "\n",
    "    lines = [title]\n",
    "    out_rows = []\n",
    "\n",
    "    for i, r in enumerate(plan_rows, start=1):\n",
    "        lvl = int(r.get(\"Level\", start_level))  # <= IMPORTANT: niveau du jour\n",
    "        base_acts10 = activities10_for_level(lvl)\n",
    "\n",
    "        seq = {\"pos1\": r.get(\"pos1\"), \"pos2\": r.get(\"pos2\"), \"pos3\": r.get(\"pos3\"), \"pos4\": r.get(\"pos4\")}\n",
    "        v = [0] * 10\n",
    "\n",
    "        for pos in [1, 2, 3, 4]:\n",
    "            a = seq.get(f\"pos{pos}\")\n",
    "            if a in base_acts10:\n",
    "                v[base_acts10.index(a)] = pos\n",
    "\n",
    "        print(f\"Day {i}: {v}\")\n",
    "        lines.append(f\"Day {i}: {v}\")\n",
    "\n",
    "        out_rows.append({\n",
    "            \"Day\": i,\n",
    "            \"StartLevel\": start_level,\n",
    "            \"TargetLevel\": target_level,\n",
    "            \"Technique\": technique,\n",
    "            \"ByModel\": by_model,\n",
    "            \"Mode\": mode_label,\n",
    "            \"LevelUsedForDecision\": lvl,\n",
    "            \"Action\": r.get(\"Action\", \"\"),\n",
    "            \"L1_day_used\": r.get(\"L1_day_used\", \"\"),\n",
    "            \"ActivityVector10\": json.dumps(v, ensure_ascii=False),\n",
    "            \"Activities10_Order\": json.dumps(base_acts10, ensure_ascii=False),\n",
    "            \"pos1\": seq.get(\"pos1\", \"\"),\n",
    "            \"pos2\": seq.get(\"pos2\", \"\"),\n",
    "            \"pos3\": seq.get(\"pos3\", \"\"),\n",
    "            \"pos4\": seq.get(\"pos4\", \"\"),\n",
    "            \"HoursPerDay\": HOURS_PER_DAY,\n",
    "            \"CumHoursIfCompleted\": round(i * HOURS_PER_DAY, 2),\n",
    "        })\n",
    "\n",
    "    safe_model = _safe_tag(by_model)\n",
    "    out_txt = out_path(f\"{filename_prefix}_autoStop_Start{start_level}_Target{target_level}__byModel-{safe_model}__{mode_label}.txt\")\n",
    "    out_csv = out_path(f\"{filename_prefix}_autoStop_Start{start_level}_Target{target_level}__byModel-{safe_model}__{mode_label}.csv\")\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "        f.write(\"\\n(0 = non sélectionnée ce jour ; 1..4 = position de l’activité)\\n\")\n",
    "\n",
    "    pd.DataFrame(out_rows).to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    return out_txt, out_csv, X\n",
    "\n",
    "print(\"\\n[SCÉNARIO] Baseline MDP — POLICY=π*_hours (Most Likely) :\\n\")\n",
    "opt_txt, opt_csv, opt_X = print_and_save_plan_variable_days(\n",
    "    plan_opt_ml,\n",
    "    technique=TECHNIQUE_MDP,\n",
    "    by_model=BY_MODEL_NAME,\n",
    "    start_level=START_LEVEL,\n",
    "    target_level=ABSORBING,\n",
    "    filename_prefix=\"best_sequences_MDP_OPT\",\n",
    "    mode_label=\"most_likely\"\n",
    ")\n",
    "\n",
    "print(f\"\\n[INFO] Absorption atteinte ? {reached_opt} | État final : {final_state_opt} | \"\n",
    "      f\"X_days : {opt_X} | Heures totales : {opt_X*HOURS_PER_DAY:.1f}h\")\n",
    "\n",
    "# Scénario 2 : sample\n",
    "plan_opt_sample, reached_opt_s, final_state_opt_s = build_plan_mdp_auto_stop(\n",
    "    start_level=START_LEVEL,\n",
    "    policy=policy_opt,\n",
    "    TA=T_A, TB=T_B,\n",
    "    mode=\"sample\",\n",
    "    seed=42,\n",
    "    stop_at=ABSORBING,\n",
    "    max_days=MAX_DAYS_CAP\n",
    ")\n",
    "\n",
    "print(\"\\n[SCÉNARIO] Baseline MDP — POLICY=π*_hours (Sample) :\\n\")\n",
    "optS_txt, optS_csv, optS_X = print_and_save_plan_variable_days(\n",
    "    plan_opt_sample,\n",
    "    technique=TECHNIQUE_MDP,\n",
    "    by_model=BY_MODEL_NAME,\n",
    "    start_level=START_LEVEL,\n",
    "    target_level=ABSORBING,\n",
    "    filename_prefix=\"best_sequences_MDP_OPT\",\n",
    "    mode_label=\"sample_seed42\"\n",
    ")\n",
    "\n",
    "print(f\"\\n[INFO] (Stochastique) Absorption atteinte ? {reached_opt_s} | \"\n",
    "      f\"X_days : {optS_X} | Heures totales : {optS_X*HOURS_PER_DAY:.1f}h\")\n",
    "\n",
    "# Scénario 3 : politique ML si disponible\n",
    "ml_txt = ml_csv = None\n",
    "ml_X = None\n",
    "reached_ml = None\n",
    "final_state_ml = None\n",
    "\n",
    "if isinstance(pi_ml_state, dict) and len(pi_ml_state) > 0:\n",
    "    plan_ml, reached_ml, final_state_ml = build_plan_mdp_auto_stop(\n",
    "        start_level=START_LEVEL,\n",
    "        policy=pi_ml_state,\n",
    "        TA=T_A, TB=T_B,\n",
    "        mode=\"most_likely\",\n",
    "        seed=42,\n",
    "        stop_at=ABSORBING,\n",
    "        max_days=MAX_DAYS_CAP\n",
    "    )\n",
    "\n",
    "    print(\"\\n[SCÉNARIO] Baseline MDP — POLICY=π_ML_state (Most Likely) :\\n\")\n",
    "    ml_txt, ml_csv, ml_X = print_and_save_plan_variable_days(\n",
    "        plan_ml,\n",
    "        technique=TECHNIQUE_MDP,\n",
    "        by_model=BY_MODEL_NAME,\n",
    "        start_level=START_LEVEL,\n",
    "        target_level=ABSORBING,\n",
    "        filename_prefix=\"best_sequences_MDP_piML\",\n",
    "        mode_label=\"most_likely\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[INFO] π_ML_state atteint maîtrise ? {reached_ml} | \"\n",
    "          f\"X_days : {ml_X} | Heures totales : {ml_X*HOURS_PER_DAY:.1f}h\")\n",
    "else:\n",
    "    print(\"\\n[INFO] Politique π_ML_state non disponible (Section 4/5).\")\n",
    "\n",
    "# ============================================================\n",
    "# (6) RUN POMDP auto-stop (si matrices latentes disponibles)\n",
    "# ============================================================\n",
    "p_txt = p_csv = None\n",
    "p_X = None\n",
    "reached_p = None\n",
    "\n",
    "if all(k in globals() for k in [\"O_emit\", \"T_hidden_A\", \"T_hidden_B\", \"pi_hidden_star\"]):\n",
    "    if isinstance(pi_hidden_star, dict) and len(pi_hidden_star) > 0:\n",
    "        any_key = list(pi_hidden_star.keys())[0]\n",
    "        if not isinstance(any_key, int):\n",
    "            pi_hidden_star_int = {0: pi_hidden_star.get(\"Faible\", A), 1: pi_hidden_star.get(\"Maitrise\", A)}\n",
    "        else:\n",
    "            pi_hidden_star_int = pi_hidden_star\n",
    "    else:\n",
    "        pi_hidden_star_int = {0: A, 1: A}\n",
    "\n",
    "    plan_pomdp_ml, reached_p = build_plan_pomdp_auto_stop(\n",
    "        start_obs_level=START_LEVEL,\n",
    "        O_emit=O_emit,\n",
    "        T_hidden_A=T_hidden_A,\n",
    "        T_hidden_B=T_hidden_B,\n",
    "        pi_hidden_star=pi_hidden_star_int,\n",
    "        hidden_names=(\"Faible\", \"Maitrise\"),\n",
    "        mode_obs=\"most_likely\",\n",
    "        seed=42,\n",
    "        max_days=MAX_DAYS_CAP\n",
    "    )\n",
    "\n",
    "    print(\"\\n[SCÉNARIO] POMDP approx (HMM belief + projection) :\\n\")\n",
    "    p_txt, p_csv, p_X = print_and_save_plan_pomdp_variable(\n",
    "        plan_pomdp_ml,\n",
    "        technique=TECHNIQUE_POMDP,\n",
    "        by_model=BY_MODEL_NAME,\n",
    "        start_obs_level=START_LEVEL,\n",
    "        target_level=ABSORBING,\n",
    "        filename_prefix=\"best_sequences_POMDP_OPT\",\n",
    "        mode_label=\"most_likely\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[INFO] POMDP maîtrise latente atteinte ? {reached_p} | \"\n",
    "          f\"X_days : {p_X} | Heures totales : {p_X*HOURS_PER_DAY:.1f}h\")\n",
    "else:\n",
    "    print(\"\\n[INFO] POMDP ignoré : matrices latentes manquantes.\")\n",
    "\n",
    "# ============================================================\n",
    "# (7) EXPORT SUMMARY JSON\n",
    "# ============================================================\n",
    "summary6 = {\n",
    "    \"OUT_DIR\": OUT_DIR,\n",
    "    \"MAX_DAYS_CAP\": MAX_DAYS_CAP,\n",
    "    \"HOURS_PER_DAY\": HOURS_PER_DAY,\n",
    "    \"START_LEVEL\": START_LEVEL,\n",
    "    \"TARGET_LEVEL\": TARGET_LEVEL,\n",
    "    \"pi_hours_star\": pi_hours_star,\n",
    "    \"pi_ml_state\": (pi_ml_state if isinstance(pi_ml_state, dict) else None),\n",
    "\n",
    "    \"MDP_OPT_most_likely\": {\n",
    "        \"X_days\": opt_X, \"total_hours\": float(opt_X * HOURS_PER_DAY) if opt_X else 0,\n",
    "        \"reached\": bool(reached_opt), \"final_state\": int(final_state_opt),\n",
    "        \"txt\": opt_txt, \"csv\": opt_csv\n",
    "    },\n",
    "    \"MDP_OPT_sample_seed42\": {\n",
    "        \"X_days\": optS_X, \"total_hours\": float(optS_X * HOURS_PER_DAY) if optS_X else 0,\n",
    "        \"reached\": bool(reached_opt_s), \"final_state\": int(final_state_opt_s),\n",
    "        \"txt\": optS_txt, \"csv\": optS_csv\n",
    "    },\n",
    "    \"MDP_piML_most_likely\": (\n",
    "        {\n",
    "            \"X_days\": ml_X, \"total_hours\": float(ml_X * HOURS_PER_DAY) if ml_X else 0,\n",
    "            \"reached\": bool(reached_ml), \"final_state\": int(final_state_ml),\n",
    "            \"txt\": ml_txt, \"csv\": ml_csv\n",
    "        } if ml_X is not None else None\n",
    "    ),\n",
    "    \"POMDP_OPT_most_likely\": (\n",
    "        {\n",
    "            \"X_days\": p_X, \"total_hours\": float(p_X * HOURS_PER_DAY) if p_X else 0,\n",
    "            \"reached\": bool(reached_p),\n",
    "            \"txt\": p_txt, \"csv\": p_csv\n",
    "        } if p_X is not None else None\n",
    "    )\n",
    "}\n",
    "\n",
    "with open(out_path(\"SECTION6_autostop_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary6, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\" SECTION 6 TERMINÉE — Synthèse exportée : {out_path('SECTION6_autostop_summary.json')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c822d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== SECTION 7 — Hypothesis (Bellman/Q in hours) ====================\n",
      "[INFO] Value used: V_hours_star\n",
      " Level BestAction_by_Qhours  Q_A_hours  Q_B_hours  Delta_B_minus_A_hours\n",
      "     1                    B     29.081     26.900                 -2.181\n",
      "     2                    B     26.962     24.525                 -2.436\n",
      "     3                    B     19.136     17.135                 -2.001\n",
      "     4                    B     19.275     16.793                 -2.482\n",
      "\n",
      "Lecture  rapide :\n",
      "- Delta_B_minus_A_hours < 0  => B réduit l’espérance de temps (meilleur).\n",
      "- Delta_B_minus_A_hours > 0  => A est meilleur.\n",
      "=> Compare niveaux 1–2 vs 3–4 pour confirmer/infirmer l’hypothèse.\n",
      "\n",
      "\n",
      "==================== SECTION 7 — Hypothesis (Monte-Carlo absorption time) ====================\n",
      " Level  MC_N  A_mean_h  B_mean_h  A_median_h  B_median_h  B_minus_A_mean_h  B_minus_A_median_h  A_p10_h  A_p90_h  B_p10_h  B_p90_h\n",
      "     1  2000     29.82     23.45        24.0        16.0             -6.37                -8.0      8.0     64.0      8.0     48.0\n",
      "     2  2000     27.38     20.54        24.0        16.0             -6.83                -8.0      8.0     56.0      8.0     40.0\n",
      "     3  2000     19.17     15.27        16.0         8.0             -3.90                -8.0      8.0     40.0      8.0     32.0\n",
      "     4  2000     19.39     15.55        16.0         8.0             -3.84                -8.0      8.0     40.0      8.0     32.0\n",
      "\n",
      "Lecture  rapide :\n",
      "- B_minus_A_mean_h < 0  => en moyenne, B mène plus vite à la maîtrise (niveau 5).\n",
      "- Les quantiles p10/p90 donnent l’incertitude (variabilité des trajectoires).\n",
      "\n",
      "\n",
      "==================== SECTION 7 — Réel (HoursTotal) par niveau ====================\n",
      " Level  n_students  HoursTotal_mean  HoursTotal_median  HoursTotal_p10  HoursTotal_p90  HoursTotal_min  HoursTotal_max\n",
      "     1           4            55.00               60.0            46.0            60.0            40.0            60.0\n",
      "     2          66            50.91               60.0            40.0            60.0            40.0            60.0\n",
      "     3         166            51.45               60.0            40.0            60.0            20.0            60.0\n",
      "     4         230            35.57               20.0            20.0            60.0            20.0            60.0\n",
      "\n",
      "Lecture  :\n",
      "- Ceci décrit le temps de remediation OBSERVÉ (heures) selon le niveau initial.\n",
      "- Ça ne prouve pas A vs B si on n’a pas de label 'action réelle', mais ça contextualise la difficulté par niveau.\n",
      "\n",
      "\n",
      "==================== SECTION 7 — Verdict hypothèse ====================\n",
      "{\n",
      "  \"hypothesis_statement\": \"Levels 1-2 => A, Levels 3-4 => B\",\n",
      "  \"check_Qhours_pass\": false,\n",
      "  \"check_MC_pass\": false,\n",
      "  \"notes\": {\n",
      "    \"Qhours\": \"best action by minimizing one-step Bellman Q in HOURS\",\n",
      "    \"MC\": \"mean absorption time (hours) via Monte-Carlo with N=2000\"\n",
      "  }\n",
      "}\n",
      "\n",
      " Exports Section 7 :\n",
      "- ./out/202602/out_pomdp\\L6-SECTION7_hypothesis_level_summary.csv\n",
      "- ./out/202602/out_pomdp\\L6-SECTION7_mc_summary.csv\n",
      "- ./out/202602/out_pomdp\\L6-SECTION7_real_hours_by_level.csv\n",
      "- ./out/202602/out_pomdp\\L6-SECTION7_hypothesis_summary.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "EPS = 1e-12\n",
    "A, B = \"A\", \"B\"\n",
    "LEVELS = [1,2,3,4,5]\n",
    "TRANSIENT = [1,2,3,4]\n",
    "ABSORBING = 5\n",
    "N_LEVELS = 5\n",
    "\n",
    "#  : 4 activités/jour, 2h/activité\n",
    "HOURS_PER_ACTIVITY = 2.0\n",
    "ACTIVITIES_PER_DAY = 4.0\n",
    "HOURS_PER_DAY = HOURS_PER_ACTIVITY * ACTIVITIES_PER_DAY  # 8h\n",
    "\n",
    "# Monte-Carlo\n",
    "MC_N = 2000\n",
    "MC_MAX_DAYS = 120\n",
    "MC_SEED = 42\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (0) Helpers\n",
    "# ------------------------------------------------------------\n",
    "def _ensure_prob_row(row: np.ndarray) -> np.ndarray:\n",
    "    row = np.array(row, dtype=float)\n",
    "    row = np.clip(row, 0.0, 1.0)\n",
    "    s = float(row.sum())\n",
    "    if s <= EPS:\n",
    "        return np.ones_like(row) / len(row)\n",
    "    return row / s\n",
    "\n",
    "def q_hours(level: int, action: str, V_hours: np.ndarray, TA: np.ndarray, TB: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Q_hours(s,a) = cost(hours_per_day) + sum_{s'} P(s'|s,a) * V_hours(s')\n",
    "    V_hours(5)=0\n",
    "    \"\"\"\n",
    "    row = TA[level-1, :] if action == A else TB[level-1, :]\n",
    "    row = _ensure_prob_row(row)\n",
    "    return float(HOURS_PER_DAY + np.dot(row, V_hours))\n",
    "\n",
    "def pick_best_action_by_q(level: int, V_hours: np.ndarray, TA: np.ndarray, TB: np.ndarray) -> dict:\n",
    "    qA = q_hours(level, A, V_hours, TA, TB)\n",
    "    qB = q_hours(level, B, V_hours, TA, TB)\n",
    "    best = A if qA <= qB else B\n",
    "    return {\"qA_hours\": qA, \"qB_hours\": qB, \"best_action\": best}\n",
    "\n",
    "def mc_absorption_time_hours(start_level: int, action_fixed: str, TA: np.ndarray, TB: np.ndarray,\n",
    "                             n: int = 1000, seed: int = 42, max_days: int = 120) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simule n trajectoires jusqu’à absorption (5), en appliquant action_fixed à chaque étape.\n",
    "    Retourne un vecteur des temps (heures).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    times = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        s = int(start_level)\n",
    "        days = 0\n",
    "        while s != ABSORBING and days < max_days:\n",
    "            row = TA[s-1, :] if action_fixed == A else TB[s-1, :]\n",
    "            row = _ensure_prob_row(row)\n",
    "            s = int(rng.choice(np.arange(1, N_LEVELS+1), p=row))\n",
    "            days += 1\n",
    "\n",
    "        times.append(float(days * HOURS_PER_DAY))\n",
    "\n",
    "    return np.array(times, dtype=float)\n",
    "\n",
    "def summarize_mc(x: np.ndarray) -> dict:\n",
    "    x = np.array(x, dtype=float)\n",
    "    return {\n",
    "        \"mean\": float(np.mean(x)),\n",
    "        \"median\": float(np.median(x)),\n",
    "        \"p10\": float(np.quantile(x, 0.10)),\n",
    "        \"p90\": float(np.quantile(x, 0.90)),\n",
    "        \"min\": float(np.min(x)),\n",
    "        \"max\": float(np.max(x)),\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (1) Choisir V_hours à utiliser\n",
    "# ------------------------------------------------------------\n",
    "if \"V_hours_star\" in globals():\n",
    "    Vh = np.array(V_hours_star, dtype=float)\n",
    "    used_V_name = \"V_hours_star\"\n",
    "elif \"V_star\" in globals():\n",
    "    # fallback : V_star est en “pas” (jours). On approx en heures : pas * 8h\n",
    "    Vh = np.array(V_star, dtype=float) * HOURS_PER_DAY\n",
    "    used_V_name = \"V_star_scaled_to_hours\"\n",
    "else:\n",
    "    # dernier recours\n",
    "    Vh = np.zeros(N_LEVELS, dtype=float)\n",
    "    used_V_name = \"none\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (2) Hypothesis check — via Q_hours (Bellman local)\n",
    "# ------------------------------------------------------------\n",
    "rows = []\n",
    "for s in TRANSIENT:\n",
    "    d = pick_best_action_by_q(s, Vh, T_A, T_B)\n",
    "    rows.append({\n",
    "        \"Level\": s,\n",
    "        \"BestAction_by_Qhours\": d[\"best_action\"],\n",
    "        \"Q_A_hours\": round(d[\"qA_hours\"], 3),\n",
    "        \"Q_B_hours\": round(d[\"qB_hours\"], 3),\n",
    "        \"Delta_B_minus_A_hours\": round(d[\"qB_hours\"] - d[\"qA_hours\"], 3),  # <0 => B better\n",
    "    })\n",
    "\n",
    "df_q = pd.DataFrame(rows).sort_values(\"Level\")\n",
    "df_q.to_csv(out_path(\"SECTION7_hypothesis_level_summary.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n==================== SECTION 7 — Hypothesis (Bellman/Q in hours) ====================\")\n",
    "print(f\"[INFO] Value used: {used_V_name}\")\n",
    "print(df_q.to_string(index=False))\n",
    "print(\"\\nLecture  rapide :\")\n",
    "print(\"- Delta_B_minus_A_hours < 0  => B réduit l’espérance de temps (meilleur).\")\n",
    "print(\"- Delta_B_minus_A_hours > 0  => A est meilleur.\")\n",
    "print(\"=> Compare niveaux 1–2 vs 3–4 pour confirmer/infirmer l’hypothèse.\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (3) Hypothesis check — Monte-Carlo (A fixe vs B fixe)\n",
    "# ------------------------------------------------------------\n",
    "mc_rows = []\n",
    "for s in TRANSIENT:\n",
    "    tA = mc_absorption_time_hours(s, A, T_A, T_B, n=MC_N, seed=MC_SEED+s, max_days=MC_MAX_DAYS)\n",
    "    tB = mc_absorption_time_hours(s, B, T_A, T_B, n=MC_N, seed=MC_SEED+100+s, max_days=MC_MAX_DAYS)\n",
    "\n",
    "    sA = summarize_mc(tA)\n",
    "    sB = summarize_mc(tB)\n",
    "\n",
    "    mc_rows.append({\n",
    "        \"Level\": s,\n",
    "        \"MC_N\": MC_N,\n",
    "        \"A_mean_h\": round(sA[\"mean\"], 2),\n",
    "        \"B_mean_h\": round(sB[\"mean\"], 2),\n",
    "        \"A_median_h\": round(sA[\"median\"], 2),\n",
    "        \"B_median_h\": round(sB[\"median\"], 2),\n",
    "        \"B_minus_A_mean_h\": round(sB[\"mean\"] - sA[\"mean\"], 2),   # <0 => B better\n",
    "        \"B_minus_A_median_h\": round(sB[\"median\"] - sA[\"median\"], 2),\n",
    "        \"A_p10_h\": round(sA[\"p10\"], 2),\n",
    "        \"A_p90_h\": round(sA[\"p90\"], 2),\n",
    "        \"B_p10_h\": round(sB[\"p10\"], 2),\n",
    "        \"B_p90_h\": round(sB[\"p90\"], 2),\n",
    "    })\n",
    "\n",
    "df_mc = pd.DataFrame(mc_rows).sort_values(\"Level\")\n",
    "df_mc.to_csv(out_path(\"SECTION7_mc_summary.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n==================== SECTION 7 — Hypothesis (Monte-Carlo absorption time) ====================\")\n",
    "print(df_mc.to_string(index=False))\n",
    "print(\"\\nLecture  rapide :\")\n",
    "print(\"- B_minus_A_mean_h < 0  => en moyenne, B mène plus vite à la maîtrise (niveau 5).\")\n",
    "print(\"- Les quantiles p10/p90 donnent l’incertitude (variabilité des trajectoires).\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (4) Données réelles — HoursTotal par niveau (descriptif)\n",
    "# ------------------------------------------------------------\n",
    "df_real = df.dropna(subset=[\"Pretest_i\", \"HoursTotal\"]).copy()\n",
    "df_real[\"Pretest_i\"] = df_real[\"Pretest_i\"].astype(int)\n",
    "df_real[\"HoursTotal\"] = pd.to_numeric(df_real[\"HoursTotal\"], errors=\"coerce\")\n",
    "\n",
    "real_rows = []\n",
    "for s in TRANSIENT:\n",
    "    dfi = df_real[df_real[\"Pretest_i\"] == s]\n",
    "    if len(dfi) == 0:\n",
    "        continue\n",
    "    x = dfi[\"HoursTotal\"].dropna().values.astype(float)\n",
    "    if len(x) == 0:\n",
    "        continue\n",
    "    real_rows.append({\n",
    "        \"Level\": s,\n",
    "        \"n_students\": int(len(x)),\n",
    "        \"HoursTotal_mean\": round(float(np.mean(x)), 2),\n",
    "        \"HoursTotal_median\": round(float(np.median(x)), 2),\n",
    "        \"HoursTotal_p10\": round(float(np.quantile(x, 0.10)), 2),\n",
    "        \"HoursTotal_p90\": round(float(np.quantile(x, 0.90)), 2),\n",
    "        \"HoursTotal_min\": round(float(np.min(x)), 2),\n",
    "        \"HoursTotal_max\": round(float(np.max(x)), 2),\n",
    "    })\n",
    "\n",
    "df_real_sum = pd.DataFrame(real_rows).sort_values(\"Level\")\n",
    "df_real_sum.to_csv(out_path(\"SECTION7_real_hours_by_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n==================== SECTION 7 — Réel (HoursTotal) par niveau ====================\")\n",
    "if len(df_real_sum) > 0:\n",
    "    print(df_real_sum.to_string(index=False))\n",
    "    print(\"\\nLecture  :\")\n",
    "    print(\"- Ceci décrit le temps de remediation OBSERVÉ (heures) selon le niveau initial.\")\n",
    "    print(\"- Ça ne prouve pas A vs B si on n’a pas de label 'action réelle', mais ça contextualise la difficulté par niveau.\\n\")\n",
    "else:\n",
    "    print(\"[WARN] Pas de données HoursTotal exploitables.\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (5) π_ML : distribution des actions prédites par niveau (si dispo)\n",
    "# ------------------------------------------------------------\n",
    "ml_exported = False\n",
    "if \"pi_ml_state\" in globals() and isinstance(pi_ml_state, dict):\n",
    "    # pi_ml_state est une “politique stationnaire” (par niveau) => simple tableau\n",
    "    ml_rows = []\n",
    "    for s in TRANSIENT:\n",
    "        ml_rows.append({\n",
    "            \"Level\": s,\n",
    "            \"pi_ml_state_action\": pi_ml_state.get(s, A)\n",
    "        })\n",
    "    df_ml = pd.DataFrame(ml_rows)\n",
    "    df_ml.to_csv(out_path(\"SECTION7_ml_policy_by_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    ml_exported = True\n",
    "\n",
    "    print(\"\\n==================== SECTION 7 — π_ML_state (stationnaire) ====================\")\n",
    "    print(df_ml.to_string(index=False))\n",
    "    print(\"\\nLecture  :\")\n",
    "    print(\"- π_ML_state dit : 'si un élève est au niveau s, l’action la plus probable observée/prévue est A ou B'.\\n\")\n",
    "\n",
    "# Si tu as une fonction de proba par ligne (Section 5), on peut aussi faire : moyenne P(B|features) par niveau\n",
    "if \"predict_action_proba_row\" in globals() and callable(predict_action_proba_row):\n",
    "    dfx = df.dropna(subset=[\"Pretest_i\"]).copy()\n",
    "    dfx[\"Pretest_i\"] = dfx[\"Pretest_i\"].astype(int)\n",
    "\n",
    "    prows = []\n",
    "    for s in TRANSIENT:\n",
    "        dfi = dfx[dfx[\"Pretest_i\"] == s].head(500).copy()  # cap sécurité\n",
    "        if len(dfi) == 0:\n",
    "            continue\n",
    "        pB_list = []\n",
    "        for _, r in dfi.iterrows():\n",
    "            pr = predict_action_proba_row(r)  # expected dict or tuple\n",
    "            # on accepte plusieurs formats robustes :\n",
    "            if isinstance(pr, dict) and \"pB\" in pr:\n",
    "                pB_list.append(float(pr[\"pB\"]))\n",
    "            elif isinstance(pr, (list, tuple)) and len(pr) >= 1:\n",
    "                pB_list.append(float(pr[0]))\n",
    "        if len(pB_list) == 0:\n",
    "            continue\n",
    "\n",
    "        prows.append({\n",
    "            \"Level\": s,\n",
    "            \"n_rows_used\": int(len(pB_list)),\n",
    "            \"mean_pB\": round(float(np.mean(pB_list)), 4),\n",
    "            \"median_pB\": round(float(np.median(pB_list)), 4),\n",
    "        })\n",
    "\n",
    "    if len(prows) > 0:\n",
    "        df_pB = pd.DataFrame(prows).sort_values(\"Level\")\n",
    "        df_pB.to_csv(out_path(\"SECTION7_ml_probB_by_level.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(\"\\n==================== SECTION 7 — π_ML(features): proba(B) par niveau ====================\")\n",
    "        print(df_pB.to_string(index=False))\n",
    "        print(\"\\nLecture  :\")\n",
    "        print(\"- mean_pB proche de 1 => le ML pense que B est souvent adapté pour ce niveau/ces profils.\")\n",
    "        print(\"- mean_pB proche de 0 => plutôt A.\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (6) Verdict automatique sur l’hypothèse (résumé)\n",
    "# ------------------------------------------------------------\n",
    "# Hypothèse : 1-2 => A ; 3-4 => B\n",
    "hyp_ok_q = True\n",
    "for s in [1,2]:\n",
    "    act = df_q[df_q[\"Level\"]==s][\"BestAction_by_Qhours\"].values\n",
    "    if len(act) and act[0] != A:\n",
    "        hyp_ok_q = False\n",
    "for s in [3,4]:\n",
    "    act = df_q[df_q[\"Level\"]==s][\"BestAction_by_Qhours\"].values\n",
    "    if len(act) and act[0] != B:\n",
    "        hyp_ok_q = False\n",
    "\n",
    "hyp_ok_mc = True\n",
    "for s in [1,2]:\n",
    "    v = df_mc[df_mc[\"Level\"]==s][\"B_minus_A_mean_h\"].values\n",
    "    if len(v) and float(v[0]) < 0:  # B faster for level 1/2\n",
    "        hyp_ok_mc = False\n",
    "for s in [3,4]:\n",
    "    v = df_mc[df_mc[\"Level\"]==s][\"B_minus_A_mean_h\"].values\n",
    "    if len(v) and float(v[0]) > 0:  # A faster for level 3/4\n",
    "        hyp_ok_mc = False\n",
    "\n",
    "verdict = {\n",
    "    \"hypothesis_statement\": \"Levels 1-2 => A, Levels 3-4 => B\",\n",
    "    \"check_Qhours_pass\": bool(hyp_ok_q),\n",
    "    \"check_MC_pass\": bool(hyp_ok_mc),\n",
    "    \"notes\": {\n",
    "        \"Qhours\": \"best action by minimizing one-step Bellman Q in HOURS\",\n",
    "        \"MC\": f\"mean absorption time (hours) via Monte-Carlo with N={MC_N}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(out_path(\"SECTION7_hypothesis_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(verdict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n==================== SECTION 7 — Verdict hypothèse ====================\")\n",
    "print(json.dumps(verdict, ensure_ascii=False, indent=2))\n",
    "print(\"\\n Exports Section 7 :\")\n",
    "print(\"-\", out_path(\"SECTION7_hypothesis_level_summary.csv\"))\n",
    "print(\"-\", out_path(\"SECTION7_mc_summary.csv\"))\n",
    "print(\"-\", out_path(\"SECTION7_real_hours_by_level.csv\"))\n",
    "if ml_exported:\n",
    "    print(\"-\", out_path(\"SECTION7_ml_policy_by_level.csv\"))\n",
    "print(\"-\", out_path(\"SECTION7_hypothesis_summary.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316f28c",
   "metadata": {},
   "source": [
    "# SECTION 8 — Analyse absorbante (Perdikaris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1f9bcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== SECTION 8 ====================\n",
      "OUT_DIR = ./out/202602/out_pomdp\n",
      "HOURS_PER_DAY = 8.0h\n",
      "MODE_P = TA | P = T_A (Standard)\n",
      "\n",
      "--- (P) Matrice de transition ---\n",
      "          1         2         3         4         5\n",
      "1  0.500000  0.250000  0.000000  0.000000  0.250000\n",
      "2  0.075758  0.545455  0.075758  0.045455  0.257576\n",
      "3  0.000000  0.030120  0.433735  0.108434  0.427711\n",
      "4  0.004348  0.000000  0.347826  0.239130  0.408696\n",
      "5  0.000000  0.000000  0.000000  0.000000  1.000000\n",
      "\n",
      "--- (Q) Transitoire → Transitoire ---\n",
      "          1         2         3         4\n",
      "1  0.500000  0.250000  0.000000  0.000000\n",
      "2  0.075758  0.545455  0.075758  0.045455\n",
      "3  0.000000  0.030120  0.433735  0.108434\n",
      "4  0.004348  0.000000  0.347826  0.239130\n",
      "\n",
      "--- (R) Transitoire → Absorbant (L5) ---\n",
      "          5\n",
      "1  0.250000\n",
      "2  0.257576\n",
      "3  0.427711\n",
      "4  0.408696\n",
      "\n",
      "--- (I) Matrice identité ---\n",
      "     1    2    3    4\n",
      "1  1.0  0.0  0.0  0.0\n",
      "2  0.0  1.0  0.0  0.0\n",
      "3  0.0  0.0  1.0  0.0\n",
      "4  0.0  0.0  0.0  1.0\n",
      "\n",
      "--- (N) Matrice fondamentale : N = (I - Q)^(-1) ---\n",
      "          1         2         3         4\n",
      "1  2.185306  1.216985  0.227375  0.105107\n",
      "2  0.370611  2.433970  0.454751  0.210214\n",
      "3  0.024225  0.143346  1.962159  0.288196\n",
      "4  0.023562  0.072484  0.898286  1.446633\n",
      "\n",
      "[INFO] inversion utilisée = inv\n",
      "\n",
      "--- (C) Comptes attendus (C = N) ---\n",
      "          1         2         3         4\n",
      "1  2.185306  1.216985  0.227375  0.105107\n",
      "2  0.370611  2.433970  0.454751  0.210214\n",
      "3  0.024225  0.143346  1.962159  0.288196\n",
      "4  0.023562  0.072484  0.898286  1.446633\n",
      "\n",
      "--- (H) Horizon moyen avant maîtrise (jours) ---\n",
      "   H (days)\n",
      "1  3.734773\n",
      "2  3.469547\n",
      "3  2.417927\n",
      "4  2.440965\n",
      "\n",
      "--- (B) Probabilité d'atteindre L5 à terme ---\n",
      "   P(absorb L5)\n",
      "1           1.0\n",
      "2           1.0\n",
      "3           1.0\n",
      "4           1.0\n",
      "\n",
      "--- (t) Temps moyen jusqu'à maîtrise (jours) ---\n",
      "   t (days)\n",
      "1  3.734773\n",
      "2  3.469547\n",
      "3  2.417927\n",
      "4  2.440965\n",
      "\n",
      "--- Temps moyen jusqu'à maîtrise (heures) ---\n",
      "   t (hours)\n",
      "1  29.878186\n",
      "2  27.756372\n",
      "3  19.343413\n",
      "4  19.527721\n",
      "\n",
      "--- Visites totales moyennes avant maîtrise (par niveau initial) ---\n",
      "   Total expected visits\n",
      "1               3.734773\n",
      "2               3.469547\n",
      "3               2.417927\n",
      "4               2.440965\n",
      "\n",
      "--- Visites moyennes par état (où on passe le plus de temps) ---\n",
      "   Avg visits of state\n",
      "1             0.650926\n",
      "2             0.966696\n",
      "3             0.885643\n",
      "4             0.512538\n",
      "\n",
      "--- (A) Retour/visite relative avant maîtrise ---\n",
      "          1         2         3         4\n",
      "1  1.000000  0.500000  0.115880  0.072656\n",
      "2  0.169592  1.000000  0.231761  0.145313\n",
      "3  0.011086  0.058894  1.000000  0.199219\n",
      "4  0.010782  0.029780  0.457805  1.000000\n",
      "\n",
      "--- Indicateur de retours en arrière (max A vers niveaux < j) ---\n",
      " FromAroundLevel  MaxReturnToLower\n",
      "               1            0.0000\n",
      "               2            0.5000\n",
      "               3            0.2318\n",
      "               4            0.1992\n",
      "\n",
      "[OK] Exports Section 8 :\n",
      " - ./out/202602/out_pomdp\\L0-SECTION8_perdikaris_components_inline_examples.json\n",
      " - ./out/202602/out_pomdp\\L0-SECTION8_P.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION8_Q.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION8_R.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION8_I.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION8_N.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION8_C_counts.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION8_H_days.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION8_B.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION8_t_days.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION8_t_hours.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION8_A_return.csv\n",
      " - ./out/202602/out_pomdp\\L0-SECTION8_backward_return_summary.csv\n",
      "\n",
      "==================== FIN SECTION 8 ====================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# 0) PARAMS / SAFE DEFAULTS\n",
    "# ============================================================\n",
    "EPS = 1e-12\n",
    "\n",
    "OUT_DIR = globals().get(\"OUT_DIR\", \"./out/202602/out_pomdp\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# prend la valeur existante dans ton notebook ; fallback = 2h\n",
    "HOURS_PER_DAY = float(globals().get(\"HOURS_PER_DAY\", 2.0))\n",
    "\n",
    "states = [1, 2, 3, 4, 5]\n",
    "TRANSIENT = [1, 2, 3, 4]\n",
    "ABSORBING = 5\n",
    "\n",
    "def out_path(fname: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"L0-{fname}\")\n",
    "\n",
    "def _as_df(mat, idx, cols):\n",
    "    return pd.DataFrame(mat, index=idx, columns=cols)\n",
    "\n",
    "def _clip_and_renorm_rows(P):\n",
    "    \"\"\"Sécurise P : clip [0,1], renormalise lignes, force absorbant état 5.\"\"\"\n",
    "    P = np.asarray(P, dtype=float).copy()\n",
    "    P = np.clip(P, 0.0, 1.0)\n",
    "    for i in range(P.shape[0]):\n",
    "        s = float(P[i, :].sum())\n",
    "        if s <= EPS:\n",
    "            P[i, :] = 1.0 / P.shape[1]\n",
    "        else:\n",
    "            P[i, :] /= s\n",
    "    P[ABSORBING-1, :] = 0.0\n",
    "    P[ABSORBING-1, ABSORBING-1] = 1.0\n",
    "    return P\n",
    "\n",
    "def _fmt(x, nd=4):\n",
    "    try:\n",
    "        return float(np.round(float(x), nd))\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "def _pct(x, nd=1):\n",
    "    return f\"{_fmt(100.0*float(x), nd)}%\"\n",
    "\n",
    "# ============================================================\n",
    "# 1) CHOIX DE LA MATRICE P (TA / TB / POLICY)\n",
    "# ============================================================\n",
    "MODE_P = globals().get(\"SECTION8_MODE_P\", \"TA\")  # \"TA\" | \"TB\" | \"POLICY\"\n",
    "\n",
    "T_A = globals().get(\"T_A\", None)\n",
    "T_B = globals().get(\"T_B\", None)\n",
    "pi_hours_star = globals().get(\"pi_hours_star\", None)  # dict {level: 'A'/'B'}\n",
    "\n",
    "if MODE_P == \"TB\":\n",
    "    if T_B is None:\n",
    "        raise NameError(\"MODE_P='TB' mais T_B n'est pas défini. Exécute Section 4/5/6 avant.\")\n",
    "    P = T_B\n",
    "    P_label = \"P = T_B (Intensive)\"\n",
    "elif MODE_P == \"POLICY\":\n",
    "    if T_A is None or T_B is None:\n",
    "        raise NameError(\"MODE_P='POLICY' mais T_A/T_B n'est pas défini.\")\n",
    "    if not isinstance(pi_hours_star, dict):\n",
    "        raise NameError(\"MODE_P='POLICY' mais pi_hours_star (dict) n'est pas défini.\")\n",
    "    P = np.zeros((5, 5), dtype=float)\n",
    "    for s in states:\n",
    "        if s == ABSORBING:\n",
    "            P[s-1, :] = 0.0\n",
    "            P[s-1, ABSORBING-1] = 1.0\n",
    "        else:\n",
    "            a = pi_hours_star.get(int(s), \"A\")\n",
    "            P[s-1, :] = (T_A[s-1, :] if a == \"A\" else T_B[s-1, :])\n",
    "    P_label = \"P = Pπ (Policy-induced from π*_hours)\"\n",
    "else:\n",
    "    if T_A is None:\n",
    "        raise NameError(\"MODE_P='TA' mais T_A n'est pas défini. Exécute Section 4/5/6 avant.\")\n",
    "    P = T_A\n",
    "    P_label = \"P = T_A (Standard)\"\n",
    "\n",
    "P = _clip_and_renorm_rows(P)\n",
    "\n",
    "print(\"\\n==================== SECTION 8 ====================\")\n",
    "print(f\"OUT_DIR = {OUT_DIR}\")\n",
    "print(f\"HOURS_PER_DAY = {HOURS_PER_DAY:.1f}h\")\n",
    "print(f\"MODE_P = {MODE_P} | {P_label}\")\n",
    "\n",
    "# ============================================================\n",
    "# (P) Matrice de transition\n",
    "# ============================================================\n",
    "print(\"\\n--- (P) Matrice de transition ---\")\n",
    "dfP = _as_df(P, states, states)\n",
    "print(dfP.to_string())\n",
    "\n",
    "# ============================================================\n",
    "# (Q, R, I) Forme canonique\n",
    "# ============================================================\n",
    "Q = P[np.ix_([s-1 for s in TRANSIENT], [s-1 for s in TRANSIENT])]   # 4x4\n",
    "R = P[np.ix_([s-1 for s in TRANSIENT], [ABSORBING-1])]              # 4x1\n",
    "I = np.eye(len(TRANSIENT), dtype=float)                             # 4x4\n",
    "\n",
    "print(\"\\n--- (Q) Transitoire → Transitoire ---\")\n",
    "dfQ = _as_df(Q, TRANSIENT, TRANSIENT)\n",
    "print(dfQ.to_string())\n",
    "\n",
    "print(\"\\n--- (R) Transitoire → Absorbant (L5) ---\")\n",
    "dfR = _as_df(R.reshape(-1, 1), TRANSIENT, [ABSORBING])\n",
    "print(dfR.to_string())\n",
    "\n",
    "print(\"\\n--- (I) Matrice identité ---\")\n",
    "dfI = _as_df(I, TRANSIENT, TRANSIENT)\n",
    "print(dfI.to_string())\n",
    "\n",
    "# ============================================================\n",
    "# (N) Matrice fondamentale\n",
    "# ============================================================\n",
    "IQ = I - Q\n",
    "try:\n",
    "    N = np.linalg.inv(IQ)\n",
    "    inv_used = \"inv\"\n",
    "except np.linalg.LinAlgError:\n",
    "    N = np.linalg.pinv(IQ)\n",
    "    inv_used = \"pinv\"\n",
    "\n",
    "print(\"\\n--- (N) Matrice fondamentale : N = (I - Q)^(-1) ---\")\n",
    "dfN = _as_df(N, TRANSIENT, TRANSIENT)\n",
    "print(dfN.to_string())\n",
    "print(f\"\\n[INFO] inversion utilisée = {inv_used}\")\n",
    "\n",
    "# ============================================================\n",
    "# (C) Comptes attendus : C = N\n",
    "# ============================================================\n",
    "C_counts = N.copy()\n",
    "\n",
    "print(\"\\n--- (C) Comptes attendus (C = N) ---\")\n",
    "print(_as_df(C_counts, TRANSIENT, TRANSIENT).to_string())\n",
    "\n",
    "# ============================================================\n",
    "# (H) Horizon moyen avant maîtrise : H = N * 1\n",
    "# ============================================================\n",
    "H_horizon_days = (N @ np.ones((len(TRANSIENT), 1))).reshape(-1, 1)\n",
    "dfH = _as_df(H_horizon_days, TRANSIENT, [\"H (days)\"])\n",
    "\n",
    "print(\"\\n--- (H) Horizon moyen avant maîtrise (jours) ---\")\n",
    "print(dfH.to_string())\n",
    "\n",
    "# ============================================================\n",
    "# (B) Probabilité d'absorption en L5 : B = N * R\n",
    "# ============================================================\n",
    "B = (N @ R.reshape(-1, 1)).reshape(-1, 1)\n",
    "dfB = _as_df(B, TRANSIENT, [\"P(absorb L5)\"])\n",
    "\n",
    "print(\"\\n--- (B) Probabilité d'atteindre L5 à terme ---\")\n",
    "print(dfB.to_string())\n",
    "\n",
    "# ============================================================\n",
    "# (t) Temps moyen jusqu’à absorption : t = N * 1\n",
    "# ============================================================\n",
    "t_days = (N @ np.ones((len(TRANSIENT), 1))).reshape(-1, 1)\n",
    "t_hours = t_days * HOURS_PER_DAY\n",
    "df_t_days = _as_df(t_days, TRANSIENT, [\"t (days)\"])\n",
    "df_t_hours = _as_df(t_hours, TRANSIENT, [\"t (hours)\"])\n",
    "\n",
    "print(\"\\n--- (t) Temps moyen jusqu'à maîtrise (jours) ---\")\n",
    "print(df_t_days.to_string())\n",
    "\n",
    "print(\"\\n--- Temps moyen jusqu'à maîtrise (heures) ---\")\n",
    "print(df_t_hours.to_string())\n",
    "\n",
    "# ============================================================\n",
    "# Visites moyennes : bottleneck\n",
    "# ============================================================\n",
    "visit_total_by_start = N.sum(axis=1).reshape(-1, 1)\n",
    "visit_avg_by_state = N.mean(axis=0).reshape(-1, 1)\n",
    "\n",
    "df_visit_total = _as_df(visit_total_by_start, TRANSIENT, [\"Total expected visits\"])\n",
    "df_visit_avg = _as_df(visit_avg_by_state, TRANSIENT, [\"Avg visits of state\"])\n",
    "\n",
    "print(\"\\n--- Visites totales moyennes avant maîtrise (par niveau initial) ---\")\n",
    "print(df_visit_total.to_string())\n",
    "\n",
    "print(\"\\n--- Visites moyennes par état (où on passe le plus de temps) ---\")\n",
    "print(df_visit_avg.to_string())\n",
    "\n",
    "bottleneck_idx = int(np.argmax(visit_avg_by_state.reshape(-1)))\n",
    "bottleneck_level = TRANSIENT[bottleneck_idx]\n",
    "bottleneck_val = float(visit_avg_by_state[bottleneck_idx, 0])\n",
    "\n",
    "# ============================================================\n",
    "# (A) Retour/visite relative avant absorption : A_return[i,j] = N[i,j] / N[j,j]\n",
    "# ============================================================\n",
    "A_return = np.zeros_like(N)\n",
    "diagN = np.diag(N)\n",
    "\n",
    "for j in range(len(TRANSIENT)):\n",
    "    denom = float(diagN[j])\n",
    "    if denom <= EPS:\n",
    "        A_return[:, j] = 0.0\n",
    "    else:\n",
    "        A_return[:, j] = N[:, j] / denom\n",
    "\n",
    "dfA = _as_df(A_return, TRANSIENT, TRANSIENT)\n",
    "\n",
    "print(\"\\n--- (A) Retour/visite relative avant maîtrise ---\")\n",
    "print(dfA.to_string())\n",
    "\n",
    "# ============================================================\n",
    "# Indicateur de retours en arrière (max A vers niveaux < j)\n",
    "# ============================================================\n",
    "rows_back = []\n",
    "for j, lvl_j in enumerate(TRANSIENT):\n",
    "    back_vals = []\n",
    "    for i, lvl_i in enumerate(TRANSIENT):\n",
    "        if lvl_i < lvl_j:\n",
    "            back_vals.append(float(A_return[i, j]))\n",
    "    back_max = float(np.max(back_vals)) if back_vals else 0.0\n",
    "    rows_back.append({\"FromAroundLevel\": lvl_j, \"MaxReturnToLower\": _fmt(back_max, 4)})\n",
    "\n",
    "df_back = pd.DataFrame(rows_back)\n",
    "\n",
    "print(\"\\n--- Indicateur de retours en arrière (max A vers niveaux < j) ---\")\n",
    "print(df_back.to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# EXPORTS (JSON + CSV)\n",
    "# ============================================================\n",
    "out_data = {\n",
    "    \"meta\": {\n",
    "        \"MODE_P\": MODE_P,\n",
    "        \"P_label\": P_label,\n",
    "        \"hours_per_day\": HOURS_PER_DAY,\n",
    "        \"states\": states,\n",
    "        \"transient\": TRANSIENT,\n",
    "        \"absorbing\": ABSORBING,\n",
    "        \"inv_used\": inv_used\n",
    "    },\n",
    "    \"P\": P.tolist(),\n",
    "    \"Q\": Q.tolist(),\n",
    "    \"R\": R.reshape(-1, 1).tolist(),\n",
    "    \"I\": I.tolist(),\n",
    "    \"N\": N.tolist(),\n",
    "    \"C_counts\": C_counts.tolist(),\n",
    "    \"H_horizon_days\": H_horizon_days.tolist(),\n",
    "    \"B\": B.tolist(),\n",
    "    \"t_days\": t_days.tolist(),\n",
    "    \"t_hours\": t_hours.tolist(),\n",
    "    \"A_return\": A_return.tolist(),\n",
    "    \"visit_total_by_start\": visit_total_by_start.tolist(),\n",
    "    \"visit_avg_by_state\": visit_avg_by_state.tolist(),\n",
    "    \"backward_return_summary\": df_back.to_dict(orient=\"records\")\n",
    "}\n",
    "\n",
    "json_path = os.path.join(OUT_DIR, \"L0-SECTION8_perdikaris_components_inline_examples.json\")\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "dfP.to_csv(os.path.join(OUT_DIR, \"L0-SECTION8_P.csv\"), encoding=\"utf-8-sig\")\n",
    "dfQ.to_csv(os.path.join(OUT_DIR, \"L0-SECTION8_Q.csv\"), encoding=\"utf-8-sig\")\n",
    "dfR.to_csv(os.path.join(OUT_DIR, \"L0-SECTION8_R.csv\"), encoding=\"utf-8-sig\")\n",
    "dfI.to_csv(os.path.join(OUT_DIR, \"L0-SECTION8_I.csv\"), encoding=\"utf-8-sig\")\n",
    "dfN.to_csv(os.path.join(OUT_DIR, \"L0-SECTION8_N.csv\"), encoding=\"utf-8-sig\")\n",
    "_as_df(C_counts, TRANSIENT, TRANSIENT).to_csv(os.path.join(OUT_DIR, \"L0-SECTION8_C_counts.csv\"), encoding=\"utf-8-sig\")\n",
    "dfH.to_csv(os.path.join(OUT_DIR, \"L0-SECTION8_H_days.csv\"), encoding=\"utf-8-sig\")\n",
    "dfB.to_csv(os.path.join(OUT_DIR, \"L0-SECTION8_B.csv\"), encoding=\"utf-8-sig\")\n",
    "df_t_days.to_csv(os.path.join(OUT_DIR, \"L0-SECTION8_t_days.csv\"), encoding=\"utf-8-sig\")\n",
    "df_t_hours.to_csv(os.path.join(OUT_DIR, \"L0-SECTION8_t_hours.csv\"), encoding=\"utf-8-sig\")\n",
    "dfA.to_csv(os.path.join(OUT_DIR, \"L0-SECTION8_A_return.csv\"), encoding=\"utf-8-sig\")\n",
    "df_back.to_csv(os.path.join(OUT_DIR, \"L0-SECTION8_backward_return_summary.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[OK] Exports Section 8 :\")\n",
    "print(\" -\", json_path)\n",
    "print(\" -\", os.path.join(OUT_DIR, \"L0-SECTION8_P.csv\"))\n",
    "print(\" -\", os.path.join(OUT_DIR, \"L0-SECTION8_Q.csv\"))\n",
    "print(\" -\", os.path.join(OUT_DIR, \"L0-SECTION8_R.csv\"))\n",
    "print(\" -\", os.path.join(OUT_DIR, \"L0-SECTION8_I.csv\"))\n",
    "print(\" -\", os.path.join(OUT_DIR, \"L0-SECTION8_N.csv\"))\n",
    "print(\" -\", os.path.join(OUT_DIR, \"L0-SECTION8_C_counts.csv\"))\n",
    "print(\" -\", os.path.join(OUT_DIR, \"L0-SECTION8_H_days.csv\"))\n",
    "print(\" -\", os.path.join(OUT_DIR, \"L0-SECTION8_B.csv\"))\n",
    "print(\" -\", os.path.join(OUT_DIR, \"L0-SECTION8_t_days.csv\"))\n",
    "print(\" -\", os.path.join(OUT_DIR, \"L0-SECTION8_t_hours.csv\"))\n",
    "print(\" -\", os.path.join(OUT_DIR, \"L0-SECTION8_A_return.csv\"))\n",
    "print(\" -\", os.path.join(OUT_DIR, \"L0-SECTION8_backward_return_summary.csv\"))\n",
    "\n",
    "print(\"\\n==================== FIN SECTION 8 ====================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
